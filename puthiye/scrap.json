{'key': [{'headings': ['Science fiction film'], 'subheadings': ['Contents', 'Characteristics of the genre', 'History', 'Themes, imagery, and visual elements', 'Genre as commentary on social issues', 'Film versus literature', 'Market share', 'See also', 'Further reading', 'Notes', 'References', 'External links', 'Navigation menu'], 'paras': ['Science fiction (or sci-fi) is a film genre that uses speculative, fictional science-based depictions of phenomena that are not fully accepted by mainstream science, such as extraterrestrial lifeforms, spacecraft, robots, cyborgs, interstellar travel, time travel, or other technologies. Science fiction films have often been used to focus on political or social issues, and to explore philosophical issues like the human condition.', "The genre has existed since the early years of silent cinema, when Georges Melies' A Trip to the Moon (1902) employed trick photography effects. The next major example (first in feature length in the genre) was the film Metropolis (1927). From the 1930s to the 1950s, the genre consisted mainly of low-budget B movies. After Stanley Kubrick's landmark 2001: A Space Odyssey (1968), the science fiction film genre was taken more seriously. In the late 1970s, big-budget science fiction films filled with special effects became popular with audiences after the success of Star Wars (1977) and paved the way for the blockbuster hits of subsequent decades.[1][2]", 'Screenwriter and scholar Eric R. Williams identifies science fiction films as one of eleven super-genres in his screenwriters’ taxonomy, stating that all feature-length narrative films can be classified by these super-genres.\xa0 The other ten super-genres are action, crime, fantasy, horror, romance, slice of life, sports, thriller, war, and western.[3]', 'According to Vivian Sobchack, a British cinema and media theorist and cultural critic:', 'Science fiction film is a film genre which emphasizes actual, extrapolative, or 2.0 speculative science and the empirical method, interacting in a social context with the lesser emphasized, but still present, transcendentalism of magic and religion, in an attempt to reconcile man with the unknown (Sobchack 63).', 'This definition suggests a continuum between (real-world) empiricism and (supernatural) transcendentalism, with science fiction film on the side of empiricism, and happy film and sad film on the side of transcendentalism. However, there are numerous well-known examples of science fiction horror films, epitomized by such pictures as Frankenstein and Alien.', 'The visual style of science fiction film is characterized by a clash between alien and familiar images. This clash is implemented when alien images become familiar, as in A Clockwork Orange, when the repetitions of the Korova Milkbar make the alien decor seem more familiar.[4] As well, familiar images become alien, as in the films Repo Man and Liquid Sky.[5] For example, in Dr. Strangelove, the distortion of the humans make the familiar images seem more alien.[6] Finally, alien images are juxtaposed with the familiar, as in The Deadly Mantis, when a giant praying mantis is shown climbing the Washington Monument.', 'Cultural theorist Scott Bukatman has proposed that science fiction film allows contemporary culture to witness an expression of the sublime, be it through exaggerated scale, apocalypse or transcendence.', "Science fiction films appeared early in the silent film era, typically as short films shot in black and white, sometimes with colour tinting. They usually had a technological theme and were often intended to be humorous. In 1902, Georges Méliès released Le Voyage dans la Lune, generally considered the first science fiction film,[7] and a film that used early trick photography to depict a spacecraft's journey to the Moon. Several early films merged the science fiction and horror genres. Examples of this are Frankenstein (1910), a film adaptation of Mary Shelley's novel, and Dr. Jekyll and Mr. Hyde (1920), based on the psychological tale by Robert Louis Stevenson. Taking a more adventurous tack, 20,000 Leagues Under the Sea (1916) is a film based on Jules Verne’s famous novel of a wondrous submarine and its vengeful captain. In the 1920s, European filmmakers tended to use science fiction for prediction and social commentary, as can be seen in German films such as Metropolis (1927) and Frau im Mond (1929). Other notable science fiction films of the silent era include The Impossible Voyage (1904), The Motorist (1906), The Conquest of the Pole (1912), Himmelskibet (1918; which with its runtime of 97 minutes generally is considered the first feature-length science fiction film in history),[8] The Cabinet of Dr. Caligari (1920), The Mechanical Man (1921), Paris Qui Dort (1923), Aelita (1924), Luch Smerti (1925), and The Lost World (1925).", 'In the 1930s, there were several big budget science fiction films, notably Just Imagine (1930), King Kong (1933), Things to Come (1936), and Lost Horizon (1937). Starting in 1936, a number of science fiction comic strips were adapted as serials, notably Flash Gordon and Buck Rogers, both starring Buster Crabbe. These serials, and the comic strips they were based on, were very popular with the general public. Other notable science fiction films of the 1930s include Frankenstein (1931), Bride of Frankenstein (1935), Doctor X (1932), Dr. Jekyll and Mr. Hyde (1931), F.P.1 (1932), Island of Lost Souls (1932), Deluge (1933), The Invisible Man (1933), Master of the World (1934), Mad Love (1935), Trans-Atlantic Tunnel (1935), The Devil-Doll (1936), The Invisible Ray (1936), The Man Who Changed His Mind (1936), The Walking Dead (1936), Non-Stop New York (1937), and The Return of Doctor X (1939). The 1940s brought us Before I Hang (1940), Black Friday (1940), Dr. Cyclops (1940), The Devil Commands (1941), Dr. Jekyll and Mr. Hyde (1941), Man Made Monster (1941), It Happened Tomorrow (1944), It Happens Every Spring (1949), and The Perfect Woman (1949). The release of Destination Moon (1950) and Rocketship X-M (1950) brought us to what many people consider "the golden age of the science fiction film".', 'In the 1950s, public interest in space travel and new technologies was great. While many 1950s science fiction films were low-budget B movies, there were several successful films with larger budgets and impressive special effects. These include The Day the Earth Stood Still (1951), The Thing from Another World (1951), When Worlds Collide (1951), The War of the Worlds (1953), 20,000 Leagues Under the Sea (1954), This Island Earth (1955), Forbidden Planet (1956), Invasion of the Body Snatchers (1956), The Curse of Frankenstein (1957), Journey to the Center of the Earth (1959) and On the Beach (1959). There is often a close connection between films in the science fiction genre and the so-called "monster movie". Examples of this are Them! (1954), The Beast from 20,000 Fathoms (1953) and The Blob (1958). During the 1950s, Ray Harryhausen, protege of master King Kong animator Willis O\'Brien, used stop-motion animation to create special effects for the following notable science fiction films: It Came from Beneath the Sea (1955), Earth vs. the Flying Saucers (1956) and 20 Million Miles to Earth (1957).', 'The most successful monster movies were kaiju films released by Japanese film studio Toho.[9][10] The 1954 film Godzilla, with the title monster attacking Tokyo, gained immense popularity, spawned multiple sequels, led to other kaiju films like Rodan, and created one of the most recognizable monsters in cinema history. Japanese science fiction films, particularly the tokusatsu and kaiju genres, were known for their extensive use of special effects, and gained worldwide popularity in the 1950s. Kaiju and tokusatsu films, notably Warning from Space (1956), sparked Stanley Kubrick\'s interest in science fiction films and influenced 2001: A Space Odyssey (1968). According to his biographer John Baxter, despite their "clumsy model sequences, the films were often well-photographed in colour ... and their dismal dialogue was delivered in well-designed and well-lit sets."[11]', 'With the Space Race between the USSR and the USA going on, documentaries and illustrations of actual events, pioneers and technology were plenty. Any movie featuring realistic space travel was at risk of being obsolete at its time of release, rather fossil than fiction. There were relatively few science fiction films in the 1960s, but some of the films transformed science fiction cinema. Stanley Kubrick\'s 2001: A Space Odyssey (1968) brought new realism to the genre, with its groundbreaking visual effects and realistic portrayal of space travel and influenced the genre with its epic story and transcendent philosophical scope. Other 1960s films included Planet of the Vampires (1965) by Italian filmmaker Mario Bava, that is regarded as one of the best movies of the period, Planet of the Apes (1968) and Fahrenheit 451 (1966), which provided social commentary, and the campy Barbarella (1968), which explored the comical side of earlier science fiction. Jean-Luc Godard\'s French "new wave" film Alphaville (1965) posited a futuristic Paris commanded by an artificial intelligence which has outlawed all emotion.', "The era of crewed trips to the Moon in 1969 and the 1970s saw a resurgence of interest in the science fiction film. Andrei Tarkovsky's Solaris (1972) and Stalker (1979) are two widely acclaimed examples of the renewed interest of film auteurs in science fiction.[12] Science fiction films from the early 1970s explored the theme of paranoia, in which humanity is depicted as under threat from sociological, ecological or technological adversaries of its own creation, such as George Lucas's directional debut THX 1138 (1971), The Andromeda Strain (1971), Silent Running (1972), Soylent Green (1973), Westworld (1973) and its sequel Futureworld (1976), and Logan's Run (1976). The science fiction comedies of the 1970s included Woody Allen's Sleeper (1973), and John Carpenter's Dark Star (1974).", "Star Wars (1977) and Close Encounters of the Third Kind (1977) were box-office hits that brought about a huge increase in science fiction films. In 1979, Star Trek: The Motion Picture brought the television series to the big screen for the first time. It was also in this period that the Walt Disney Company released many science fiction films for family audiences such as The Black Hole, Flight of the Navigator, and Honey, I Shrunk the Kids. The sequels to Star Wars, The Empire Strikes Back (1980) and Return of the Jedi (1983), also saw worldwide box office success. Ridley Scott's films, such as Alien (1979) and Blade Runner (1982), along with James Cameron's The Terminator (1984), presented the future as dark, dirty and chaotic, and depicted aliens and androids as hostile and dangerous. In contrast, Steven Spielberg's E.T. the Extra-Terrestrial (1982), one of the most successful films of the 1980s, presented aliens as benign and friendly, a theme already present in Spielberg's own Close Encounters of the Third Kind.", "The big budget adaptations of Frank Herbert's Dune and Alex Raymond's Flash Gordon, as well as Peter Hyams's sequel to 2001, 2010: The Year We Make Contact (based on 2001 author Arthur C. Clarke's sequel novel 2010: Odyssey Two), were box office failures that dissuaded producers from investing in science fiction literary properties. Disney's Tron (1982) turned out to be a moderate success. The strongest contributors to the genre during the second half of the 1980s were James Cameron and Paul Verhoeven with The Terminator and RoboCop entries. Robert Zemeckis' film Back to the Future (1985) and its sequels were critically praised and became box office successes, not to mention international phenomena. James Cameron's sequel to Alien, Aliens (1986), was very different from the original film, falling more into the action/science fiction genre, it was both a critical and commercial success and Sigourney Weaver was nominated for Best Actress in a Leading Role at the Academy Awards. The Japanese cyberpunk anime film Akira (1988) also had a big influence outside Japan when released.", 'In the 1990s, the emergence of the World Wide Web and the cyberpunk genre spawned several movies on the theme of the computer-human interface, such as Terminator 2: Judgment Day (1991), Total Recall (1990), The Lawnmower Man (1992), and The Matrix (1999). Other themes included disaster films (e.g., Armageddon and Deep Impact, both 1998), alien invasion (e.g., Independence Day (1996)) and genetic experimentation (e.g., Jurassic Park (1993) and Gattaca (1997)). Also, the Star Wars prequel trilogy began with the release of Star Wars: Episode I – The Phantom Menace, which eventually grossed over one billion dollars.', 'As the decade progressed, computers played an increasingly important role in both the addition of special effects (thanks to Terminator 2: Judgment Day and Jurassic Park) and the production of films. As software developed in sophistication it was used to produce more complicated effects. It also enabled filmmakers to enhance the visual quality of animation, resulting in films such as Ghost in the Shell (1995) from Japan, and The Iron Giant (1999) from the United States.', "During the first decade of the 2000s, superhero films abounded, as did earthbound science fiction such as the Matrix trilogy. In 2005, the Star Wars saga was completed (although it was later continued, but at the time it was not intended to be) with the darkly themed Star Wars: Episode III – Revenge of the Sith. Science-fiction also returned as a tool for political commentary in films such as A.I. Artificial Intelligence, Minority Report, Sunshine, District 9, Children of Men, Serenity, Sleep Dealer, and Pandorum. The 2000s also saw the release of Transformers (2007) and Transformers: Revenge of the Fallen (2009), both of which resulted in worldwide box office success. In 2009, James Cameron's Avatar garnered worldwide box office success, and would later become the highest-grossing movie of all time. This movie was also an example of political commentary. It depicted humans destroying the environment on another planet by mining for a special metal called unobtainium. That same year, Terminator Salvation was released and garnered only moderate success.", 'The 2010s saw new entries in several classic science fiction franchises, including Predators (2010), Tron: Legacy (2010), a resurgence of the Star Wars series, and entries into the Planet of the Apes and Godzilla franchises. Several more cross-genre films have also been produced, including comedies such as Hot Tub Time Machine (2010), Seeking a Friend for the End of the World (2012), Safety Not Guaranteed (2013), and Pixels (2015), romance films such as Her (2013), Monsters (2010), and Ex Machina (2015), heist films including Inception (2010) and action films including Real Steel (2011), Total Recall (2012), Edge of Tomorrow (2014), Pacific Rim (2013), Chappie (2015), Tomorrowland (2015), and Ghost in the Shell (2017). The superhero film boom has also continued, into films such as Iron Man 2 (2010) and 3 (2013), several entries into the X-Men film series, and The Avengers (2012), which became the fourth-highest-grossing film of all time. New franchises such as Deadpool and Guardians of the Galaxy also began in this decade.', 'Further into the decade, more realistic science fiction epic films also become prevalent, including Battleship (2012), Gravity (2013), Elysium (2013), Interstellar (2014), Mad Max: Fury Road (2015), The Martian (2015), Arrival (2016), Passengers (2016), and Blade Runner 2049 (2017). Many of these films have gained widespread accolades, including several Academy Award wins and nominations. These films have addressed recent matters of scientific interest, including space travel, climate change, and artificial intelligence.', "Alongside these original films, many adaptations were produced, especially within the young adult dystopian fiction subgenre, popular in the early part of the decade. These include the Hunger Games film series, based on the trilogy of novels by Suzanne Collins, The Divergent Series based on Veronica Roth's Divergent trilogy, and the Maze Runner series, based on James Dashner's The Maze Runner novels. Several adult adaptations have also been produced, including The Martian (2015), based on Andy Weir's 2011 novel, Cloud Atlas (2012), based on David Mitchell's 2004 novel, World War Z, based on Max Brooks' 2006 novel, and Ready Player One (2018), based on Ernest Cline's 2011 novel.", 'Independent productions also increased in the 2010s, with the rise of digital filmmaking making it easier for filmmakers to produce movies on a smaller budget. These films include Attack the Block (2011), Source Code (2011), Looper (2012), Upstream Color (2013), Ex Machina (2015), and Valerian and the City of a Thousand Planets (2017). In 2016, Ex Machina won the Academy Award for Visual Effects in a surprising upset over the much higher-budget Star Wars: The Force Awakens (2015).', 'Science fiction films are often speculative in nature, and often include key supporting elements of science and technology. However, as often as not the "science" in a Hollywood science fiction movie can be considered pseudo-science, relying primarily on atmosphere and quasi-scientific artistic fancy than facts and conventional scientific theory. The definition can also vary depending on the viewpoint of the observer.[citation needed]', 'Many science fiction films include elements of mysticism, occult, magic, or the supernatural, considered by some to be more properly elements of fantasy or the occult (or religious) film.[citation needed] This transforms the movie genre into a science fantasy with a religious or quasi-religious philosophy serving as the driving motivation. The movie Forbidden Planet employs many common science fiction elements, but the film carries a profound message - that the evolution of a species toward technological perfection (in this case exemplified by the disappeared alien civilization called the "Krell") does not ensure the loss of primitive and dangerous urges.[citation needed] In the film, this part of the primitive mind manifests itself as monstrous destructive force emanating from the Freudian subconscious, or "Id".', 'Some films blur the line between the genres, such as films where the protagonist gains the extraordinary powers of the superhero. These films usually employ quasi-plausible reason for the hero gaining these powers.[citation needed]', 'Not all science fiction themes are equally suitable for movies. Science fiction horror is most common. Often enough, these films could just as well pass as Westerns or World War II films if the science fiction props were removed.[citation needed] Common motifs also include voyages and expeditions to other planets, and dystopias, while utopias are rare.', 'Film theorist Vivian Sobchack argues that science fiction films differ from fantasy films in that while science fiction film seeks to achieve our belief in the images we are viewing, fantasy film instead attempts to suspend our disbelief. The science fiction film displays the unfamiliar and alien in the context of the familiar. Despite the alien nature of the scenes and science fictional elements of the setting, the imagery of the film is related back to humankind and how we relate to our surroundings. While the science fiction film strives to push the boundaries of the human experience, they remain bound to the conditions and understanding of the audience and thereby contain prosaic aspects, rather than being completely alien or abstract.[citation needed]', 'Genre films such as westerns or war movies are bound to a particular area or time period. This is not true of the science fiction film. However, there are several common visual elements that are evocative of the genre. These include the spacecraft or space station, alien worlds or creatures, robots, and futuristic gadgets. Examples include movies like Lost in Space, Serenity, Avatar, Prometheus, Tomorrowland, Passengers, and Valerian and the City of a Thousand Planets. More subtle visual clues can appear with changes of the human form through modifications in appearance, size, or behavior, or by means a known environment turned eerily alien, such as an empty city The Omega Man (1971).', 'While science is a major element of this genre, many movie studios take significant liberties with scientific knowledge. Such liberties can be most readily observed in films that show spacecraft maneuvering in outer space. The vacuum should preclude the transmission of sound or maneuvers employing wings, yet the soundtrack is filled with inappropriate flying noises and changes in flight path resembling an aircraft banking. The filmmakers, unfamiliar with the specifics of space travel, focus instead on providing acoustical atmosphere and the more familiar maneuvers of the aircraft.', 'Similar instances of ignoring science in favor of art can be seen when movies present environmental effects as portrayed in Star Wars and Star Trek. Entire planets are destroyed in titanic explosions requiring mere seconds, whereas an actual event of this nature takes many hours.[citation needed]', 'The role of the scientist has varied considerably in the science fiction film genre, depending on the public perception of science and advanced technology.[citation needed] Starting with Dr. Frankenstein, the mad scientist became a stock character who posed a dire threat to society and perhaps even civilization. Certain portrayals of the "mad scientist", such as Peter Sellers\'s performance in Dr. Strangelove, have become iconic to the genre.[citation needed] In the monster films of the 1950s, the scientist often played a heroic role as the only person who could provide a technological fix for some impending doom. Reflecting the distrust of government that began in the 1960s in the United States, the brilliant but rebellious scientist became a common theme, often serving a Cassandra-like role during an impending disaster.', "Biotechnology (e.g., cloning) is a popular scientific element in films as depicted in Jurassic Park (cloning of extinct species), The Island (cloning of humans), and (genetic modification) in some superhero movies and in the Alien series. Cybernetics and holographic projections as depicted in RoboCop and I, Robot are also popularized. Interstellar travel and teleportation is a popular theme in the Star Trek series that is achieved through warp drives and transporters while intergalactic travel is popular in films such as Stargate and Star Wars that is achieved through hyperspace or wormholes. Nanotechnology is also featured in the Star Trek series in the form of replicators (utopia), in The Day the Earth Stood Still in the form of grey goo (dystopia), and in Iron Man 3 in the form of extremis (nanotubes). Force fields is a popular theme in Independence Day while invisibility is also popular in Star Trek. Arc reactor technology, featured in Iron Man, is similar to a cold fusion device.[13] Miniaturization technology where people are shrunk to microscopic sizes is featured in films like Fantastic Voyage (1966), Honey, I Shrunk the Kids (1989), and Marvel's Ant-Man (2015).", 'The late Arthur C. Clarke\'s third law states that "any sufficiently advanced technology is indistinguishable from magic". Past science fiction films have depicted "fictional" ("magical") technologies that became present reality. For example, the Personal Access Display Device from Star Trek was a precursor of smartphones and tablet computers. Gesture recognition in the movie Minority Report is part of current game consoles. Human-level artificial intelligence is also fast approaching with the advent of smartphone A.I. while a working cloaking device / material is the main goal of stealth technology. Autonomous cars (e.g. KITT from the Knight Rider series) and quantum computers, like in the movie Stealth and Transcendence, also will be available eventually. Furthermore, although Clarke\'s laws do not classify "sufficiently advanced" technologies, the Kardashev scale measures a civilization\'s level of technological advancement into types. Due to its exponential nature, sci-fi civilizations usually only attain Type I (harnessing all the energy attainable from a single planet), and strictly speaking often not even that.', "The concept of life, particularly intelligent life, having an extraterrestrial origin is a popular staple of science fiction films. Early films often used alien life forms as a threat or peril to the human race, where the invaders were frequently fictional representations of actual military or political threats on Earth as observed in films such as Mars Attacks!, Starship Troopers, the Alien series, the Predator series, and The Chronicles of Riddick series. Some aliens were represented as benign and even beneficial in nature in such films as Escape to Witch Mountain, E.T. the Extra-Terrestrial, Close Encounters of the Third Kind, The Fifth Element, The Hitchhiker's Guide to the Galaxy, Avatar, Valerian and the City of a Thousand Planets, and the Men in Black series.", 'In order to provide subject matter to which audiences can relate, the large majority of intelligent alien races presented in films have an anthropomorphic nature, possessing human emotions and motivations. In films like Cocoon, My Stepmother Is an Alien, Species, Contact, The Box, Knowing, The Day the Earth Stood Still, and The Watch, the aliens were nearly human in physical appearance, and communicated in a common earth language. However, the aliens in Stargate and Prometheus were human in physical appearance but communicated in an alien language. A few films have tried to represent intelligent aliens as something utterly different from the usual humanoid shape (e.g. An intelligent life form surrounding an entire planet in Solaris, the ball shaped creature in Dark Star, microbial-like creatures in The Invasion, shape-shifting creatures in Evolution). Recent trends in films involve building-size alien creatures like in the movie Pacific Rim where the CGI has tremendously improved over the previous decades as compared in previous films such as Godzilla.', 'A frequent theme among science fiction films is that of impending or actual disaster on an epic scale. These often address a particular concern of the writer by serving as a vehicle of warning against a type of activity, including technological research. In the case of alien invasion films, the creatures can provide as a stand-in for a feared foreign power.', 'Films that fit into the Disaster film typically also fall into the following general categories:[citation needed]', 'While monster films do not usually depict danger on a global or epic scale, science fiction film also has a long tradition of movies featuring monster attacks. These differ from similar films in the horror or fantasy genres because science fiction films typically rely on a scientific (or at least pseudo-scientific) rationale for the monster\'s existence, rather than a supernatural or magical reason. Often, the science fiction film monster is created, awakened, or "evolves" because of the machinations of a mad scientist, a nuclear accident, or a scientific experiment gone awry. Typical examples include The Beast from 20,000 Fathoms (1953), Jurassic Park films, Cloverfield, Pacific Rim, the King Kong films, and the Godzilla franchise or the many films involving Frankenstein\'s monster.', 'The core mental aspects of what makes us human has been a staple of science fiction films, particularly since the 1980s. Ridley Scott\'s Blade Runner (1982), an adaptation of Philip K. Dick\'s novel Do Androids Dream of Electric Sheep?, examined what made an organic-creation a human, while the RoboCop series saw an android mechanism fitted with the brain and reprogrammed mind of a human to create a cyborg. The idea of brain transfer was not entirely new to science fiction film, as the concept of the "mad scientist" transferring the human mind to another body is as old as Frankenstein while the idea of corporations behind mind transfer technologies is observed in later films such as Gamer, Avatar, and Surrogates.', 'Films such as Total Recall have popularized a thread of films that explore the concept of reprogramming the human mind. The theme of brainwashing in several films of the sixties and seventies including A Clockwork Orange and The Manchurian Candidate coincided with secret real-life government experimentation during Project MKULTRA. Voluntary erasure of memory is further explored as themes of the films Paycheck and Eternal Sunshine of the Spotless Mind. Some films like Limitless explore the concept of mind enhancement. The anime series Serial Experiments Lain also explores the idea of reprogrammable reality and memory.', 'The idea that a human could be entirely represented as a program in a computer was a core element of the film Tron. This would be further explored in the film version of The Lawnmower Man, Transcendence, and Ready Player One and the idea reversed in Virtuosity as computer programs sought to become real persons. In The Matrix series, the virtual reality world became a real-world prison for humanity, managed by intelligent machines. In movies such as eXistenZ, The Thirteenth Floor, and Inception, the nature of reality and virtual reality become intermixed with no clear distinguishing boundary.', 'Telekinesis and telepathy are featured in movies like Star Wars, The Last Mimzy, Race to Witch Mountain, Chronicle, and Lucy while precognition is featured in Minority Report as well as in The Matrix saga (in which precognition is achieved by knowing the artificial world).', 'Robots have been a part of science fiction since the Czech playwright Karel Čapek coined the word in 1921. In early films, robots were usually played by a human actor in a boxy metal suit, as in The Phantom Empire, although the female robot in Metropolis is an exception. The first depiction of a sophisticated robot in a United States film was Gort in The Day the Earth Stood Still.', 'Robots in films are often sentient and sometimes sentimental, and they have filled a range of roles in science fiction films. Robots have been supporting characters, such as Robby the Robot in Forbidden Planet, Huey, Dewey and Louie in Silent Running, Data in Star Trek: The Next Generation, sidekicks (e.g., C-3PO and R2-D2 from Star Wars, JARVIS from Iron Man), and extras, visible in the background to create a futuristic setting (e.g., Back to the Future Part II (1989), Total Recall (2012), RoboCop (2014)). As well, robots have been formidable movie villains or monsters (e.g., the robot Box in the film Logan\'s Run (1976), HAL 9000 in 2001: A Space Odyssey, ARIIA in Eagle Eye, robot Sentinels in X-Men: Days of Future Past, the battle droids in the Star Wars prequel trilogy, or the huge robot probes seen in Monsters vs. Aliens). In some cases, robots have even been the leading characters in science fiction films; in the film Blade Runner (1982), many of the characters are bioengineered android "replicants". This is also present in the animated films WALL-E (2008), Astro Boy (2009), Big Hero 6 (2014), Ghost in the Shell (2017) and in Next Gen (2018).', 'Films like Bicentennial Man, A.I. Artificial Intelligence, Chappie, and Ex Machina depicted the emotional fallouts of robots that are self-aware. Other films like The Animatrix (The Second Renaissance) present the consequences of mass-producing self-aware androids as humanity succumbs to their robot overlords.', "One popular theme in science fiction film is whether robots will someday replace humans, a question raised in the film adaptation of Isaac Asimov's I, Robot (in jobs) and in the film Real Steel (in sports), or whether intelligent robots could develop a conscience and a motivation to protect, take over, or destroy the human race (as depicted in The Terminator, Transformers, and in Avengers: Age of Ultron). Another theme is remote telepresence via androids as depicted in Surrogates and Iron Man 3. As artificial intelligence becomes smarter due to increasing computer power, some sci-fi dreams have already been realized. For example, the computer Deep Blue beat the world chess champion in 1997 and a documentary film, Game Over: Kasparov and the Machine, was released in 2003. Another famous computer called Watson defeated the two best human Jeopardy (game show) players in 2011 and a NOVA documentary film, Smartest Machine on Earth, was released in the same year.", 'Building-size robots are also becoming a popular theme in movies as featured in Pacific Rim. Future live action films may include an adaptation of popular television series like Voltron and Robotech. The CGI robots of Pacific Rim and the Power Rangers (2017) reboot was greatly improved as compared to the original Mighty Morphin Power Rangers: The Movie (1995). While "size does matter", a famous tagline of the movie Godzilla, incredibly small robots, called nanobots, do matter as well (e.g. Borg nanoprobes in Star Trek and nanites in I, Robot).', "The concept of time travel—travelling backwards and forwards through time—has always been a popular staple of science fiction film and science fiction television series. Time travel usually involves the use of some type of advanced technology, such as H. G. Wells' classic The Time Machine, the commercially successful 1980s-era Back to the Future trilogy, the Bill & Ted trilogy, the Terminator series, Déjà Vu (2006), Source Code (2011), Edge of Tomorrow (2014), and Predestination (2014). Other movies, such as the Planet of the Apes series, Timeline (2003) and The Last Mimzy (2007), explained their depictions of time travel by drawing on physics concepts such as the special relativity phenomenon of time dilation (which could occur if a spaceship was travelling near the speed of light) and wormholes. Some films show time travel not being attained from advanced technology, but rather from an inner source or personal power, such as the 2000s-era films Donnie Darko, Mr. Nobody, The Butterfly Effect, and X-Men: Days of Future Past.", 'More conventional time travel movies use technology to bring the past to life in the present, or in a present that lies in our future. The film Iceman (1984) told the story of the reanimation of a frozen Neanderthal. The film Freejack (1992) shows time travel used to pull victims of horrible deaths forward in time a split-second before their demise, and then use their bodies for spare parts.', "A common theme in time travel film is the paradoxical nature of travelling through time. In the French New Wave film La jetée (1962), director Chris Marker depicts the self-fulfilling aspect of a person being able to see their future by showing a child who witnesses the death of his future self. La Jetée was the inspiration for 12 Monkeys, (1995) director Terry Gilliam's film about time travel, memory and madness. The Back to the Future trilogy and The Time Machine go one step further and explore the result of altering the past, while in Star Trek: First Contact (1996) and Star Trek (2009) the crew must rescue the Earth from having its past altered by time-travelling cyborgs and alien races.", 'The science fiction film genre has long served as useful means of discussing sensitive topical issues without arousing controversy, and it often provides thoughtful social commentary on potential unforeseen future issues. The fictional setting allows for a deeper examination and reflection of the ideas presented, with the perspective of a viewer watching remote events. Most controversial issues in science fiction films tend to fall into two general storylines, Utopian or dystopian. Either a society will become better or worse in the future. Because of controversy, most science fiction films will fall into the dystopian film category rather than the Utopian category.', 'The types of commentary and controversy presented in science fiction films often illustrate the particular concerns of the periods in which they were produced. Early science fiction films expressed fears about automation replacing workers and the dehumanization of society through science and technology. For example, The Man in the White Suit (1951) used a science fiction concept as a means to satirize postwar British "establishment" conservatism, industrial capitalists, and trade unions. Another example is HAL 9000 from 2001: A Space Odyssey (1968). He controls the shuttle, and later harms its crew. "Kubrick\'s vision reveals technology as a competitive force that must be defeated in order for humans to evolve."[14] Later films explored the fears of environmental catastrophe, technology-created disasters, or overpopulation, and how they would impact society and individuals (e.g. Soylent Green, Elysium).', "The monster movies of the 1950s—like Godzilla (1954)—served as stand-ins for fears of nuclear war, communism and views on the cold war.[citation needed] In the 1970s, science fiction films also became an effective way of satirizing contemporary social mores with Silent Running and Dark Star presenting hippies in space as a riposte to the militaristic types that had dominated earlier films.[citation needed] Stanley Kubrick's A Clockwork Orange presented a horrific vision of youth culture, portraying a youth gang engaged in rape and murder, along with disturbing scenes of forced psychological conditioning serving to comment on societal responses to crime.", "Logan's Run depicted a futuristic swingers' utopia that practiced euthanasia as a form of population control and The Stepford Wives anticipated a reaction to the women's liberation movement. Enemy Mine demonstrated that the foes we have come to hate are often just like us, even if they appear alien.", 'Contemporary science fiction films continue to explore social and political issues. One recent example is Minority Report (2002), debuting in the months after the terrorist attacks of September 11, 2001, and focused on the issues of police powers, privacy and civil liberties in a near-future United States. Some movies like The Island (2005) and Never Let Me Go (2010) explore the issues surrounding cloning.', 'More recently, the headlines surrounding events such as the Iraq War, international terrorism, the avian influenza scare, and United States anti-immigration laws have found their way into the consciousness of contemporary filmmakers. The film V for Vendetta (2006) drew inspiration from controversial issues such as the Patriot Act and the War on Terror,[citation needed] while science fiction thrillers such as Children of Men (also 2006), District 9 (2009), and Elysium (2013) commented on diverse social issues such as xenophobia, propaganda, and cognitive dissonance. Avatar (2009) had remarkable resemblance to colonialism of native land, mining by multinational-corporations and the Iraq War.', 'Lancaster University professor Jamaluddin Bin Aziz argues that as science fiction has evolved and expanded, it has fused with other film genres such as gothic thrillers and film noir. When science fiction integrates film noir elements, Bin Aziz calls the resulting hybrid form "future noir", a form which "... encapsulates a postmodern encounter with generic persistence, creating a mixture of irony, pessimism, prediction, extrapolation, bleakness and nostalgia." Future noir films such as Brazil, Blade Runner, 12 Monkeys, Dark City, and Children of Men use a protagonist who is "...increasingly dubious, alienated and fragmented", at once "dark and playful like the characters in Gibson\'s Neuromancer, yet still with the "... shadow of Philip Marlowe..."', 'Future noir films that are set in a post-apocalyptic world "...restructure and re-represent society in a parody of the atmospheric world usually found in noir\'s construction of a city—dark, bleak and beguiled." Future noir films often intermingle elements of the gothic thriller genre, such as Minority Report, which makes references to occult practices, and Alien, with its tagline "In space, no one can hear you scream", and a space vessel, Nostromo, "that hark[s] back to images of the haunted house in the gothic horror tradition". Bin Aziz states that films such as James Cameron’s The Terminator are a subgenre of "techno noir" that create "...an atmospheric feast of noir darkness and a double-edged world that is not what it seems."[15]', 'When compared to science-fiction literature, science-fiction films often rely less on the human imagination and more upon action scenes and special effect-created alien creatures and exotic backgrounds. Since the 1970s, film audiences have come to expect a high standard for special effects in science-fiction films.[16] In some cases, science fiction-themed films superimpose an exotic, futuristic setting onto what would not otherwise be a science-fiction tale. Nevertheless, some critically acclaimed science-fiction movies have followed in the path of science-fiction literature, using story development to explore abstract concepts.', "Jules Verne (1828–1905) became the first major science-fiction author whose works film-makers adapted for the screen - with Méliès' Le Voyage dans la Lune (1902) and 20,000 lieues sous les mers (1907), which used Verne's scenarios as a framework for fantastic visuals. By the time Verne's work fell out of copyright in 1950, the adaptations were treated[by whom?] as period pieces. Verne's works have been adapted a number of times since then, including 20,000 Leagues Under the Sea (1954), From the Earth to the Moon (1958), and two film versions of Journey to the Center of the Earth in 1959 and 2008.", "H. G. Wells's novels The Invisible Man, Things to Come and The Island of Doctor Moreau were all adapted into films during his lifetime (1866–1946), while The War of the Worlds, updated in 1953 and again in 2005, was adapted to film at least four times altogether. The Time Machine has had two film versions (1960 and 2002) while Sleeper in part is a pastiche of Wells's 1910 novel The Sleeper Awakes.", 'With the drop-off in interest in science-fiction films during the 1940s, few of the "golden age" science-fiction authors made it to the screen. A novella by John W. Campbell provided the basis for The Thing from Another World (1951). Robert A. Heinlein contributed to the screenplay for Destination Moon (1950), but none of his major works were adapted for the screen until the 1990s: The Puppet Masters (1994) and Starship Troopers (1997). The fiction of Isaac Asimov (1920–1992) influenced the Star Wars and Star Trek films, but it was not until 1988 that a film version of one of his short stories (Nightfall) was produced. The first major motion-picture adaptation of a full-length Asimov work was Bicentennial Man (1999) (based on the short stories Bicentennial Man (1976) and The Positronic Man (1992), the latter co-written with Robert Silverberg), although I, Robot (2004), a film loosely based on Asimov\'s book of short stories by the same name, drew more attention.', "The 1968 film adaptation of some of the stories of science-fiction author Arthur C. Clarke as 2001: A Space Odyssey won the Academy Award for Visual Effects and offered thematic complexity not typically associated with the science-fiction genre at the time. Its sequel, 2010: The Year We Make Contact (inspired to Clarke's 2010: Odyssey Two), was commercially successful but less highly regarded by critics. Reflecting the times, two earlier science-fiction works by Ray Bradbury were adapted for cinema in the 1960s: Fahrenheit 451 (1966) and The Illustrated Man (1969). Kurt Vonnegut's Slaughterhouse-Five was filmed in 1971 and Breakfast of Champions in 1998.", "Philip K. Dick's fiction has been used in a number of science-fiction films, in part because it evokes the paranoia[citation needed] that has been a central feature of the genre. Films based on Dick's works include Blade Runner (1982), Total Recall (1990), Impostor (2001), Minority Report (2002), Paycheck (2003), A Scanner Darkly (2006), and The Adjustment Bureau (2011). These films represent loose adaptations of the original stories, with the exception of A Scanner Darkly, which is more inclined to Dick's novel.", 'The estimated North American box-office market-share of science fiction as of 2019[update] comprised 4.77%.[17]']}, {'headings': ['Science fiction'], 'subheadings': ['Contents', 'Definitions', 'History', 'Social influence', 'Science fiction studies', 'Community', 'Elements', 'International examples', 'Subgenres', 'Related genres', 'See also', 'Citations', 'General and cited sources', 'External links', 'Navigation menu'], 'paras': ['', 'Science fiction (sometimes shortened to Sci-Fi or SF) is a genre of speculative fiction which typically deals with imaginative and futuristic concepts such as advanced science and technology, space exploration, time travel, parallel universes, extraterrestrial life, sentient artificial intelligence, cybernetics, certain forms of immortality (like mind uploading), and the singularity. Science fiction predicted several existing inventions, such as the atomic bomb,[1] robots,[2] and borazon,[3] whose names entirely match their fictional predecessors. In addition, science fiction might serve as an outlet to facilitate future scientific and technological innovations.[4]', 'Science fiction can trace its roots to ancient mythology.[5] It is also related to fantasy, horror, and superhero fiction and contains many subgenres. Its exact definition has long been disputed among authors, critics, scholars, and readers.', 'Science fiction, in literature, film, television, and other media, has become popular and influential over much of the world, It has been called the "literature of ideas", and often explores the potential consequences of scientific, social, and technological innovations.[6][7]  It is also often said to inspire a "sense of wonder".[8] Besides providing entertainment, it can also criticize present-day society and explore alternatives.', 'American science fiction author and editor Lester del Rey wrote, "Even the devoted aficionado or fan—has a hard time trying to explain what science fiction is," and the lack of a "full satisfactory definition" is because "there are no easily delineated limits to science fiction."[9] According to Isaac Asimov, "Science fiction can be defined as that branch of literature which deals with the reaction of human beings to changes in science and technology."[10] Robert A. Heinlein wrote that "A handy short definition of almost all science fiction might read: realistic speculation about possible future events, based solidly on adequate knowledge of the real world, past and present, and on a thorough understanding of the nature and significance of the scientific method."[11]', 'Part of the reason that it is so difficult to pin down an agreed definition of science fiction is because there is a tendency among science fiction enthusiasts to act as their own arbiter in deciding what exactly constitutes science fiction.[12] Damon Knight summed up the difficulty, saying "science fiction is what we point to when we say it."[13] Ultimately, it may be more useful to talk around science fiction as the intersection of other, more concrete, genres and subgenres.[14]', 'Forrest J Ackerman has been credited with first using the term "sci-fi" (analogous to the then-trendy "hi-fi") in about 1954;[15] the first known use in print was a description of Donovan\'s Brain by movie critic Jesse Zunser in January 1954.[16] As science fiction entered popular culture, writers and fans active in the field came to associate the term with low-budget, low-tech "B-movies," and with low-quality pulp science fiction.[17][18][19] By the 1970s, critics within the field, such as Damon Knight and Terry Carr, were using "sci fi" to distinguish hack-work from serious science fiction.[20] Peter Nicholls writes that "SF" (or "sf") is "the preferred abbreviation within the community of sf writers and readers."[21] Robert Heinlein found even "science fiction" insufficient for certain types of works in this genre, and suggested the term speculative fiction to be used instead for those that are more "serious" or "thoughtful."[22]', "Some scholars assert that science fiction had its beginnings in ancient times, when the line between myth and fact was blurred.[23] Written in the 2nd century CE by the satirist Lucian, A True Story contains many themes and tropes characteristic of modern science fiction, including travel to other worlds, extraterrestrial lifeforms, interplanetary warfare, and artificial life. Some consider it the first science-fiction novel.[24] Some of the stories from The Arabian Nights,[25][26] along with the 10th-century The Tale of the Bamboo Cutter[26] and Ibn al-Nafis's 13th-century Theologus Autodidactus,[27] also contain elements of science fiction.", 'Written during the Scientific Revolution and the Age of Enlightenment, Johannes Kepler\'s Somnium (1634), Francis Bacon\'s New Atlantis (1627),[28] Athanasius Kircher\'s Itinerarium extaticum (1656),[29] Cyrano de Bergerac\'s Comical History of the States and Empires of the Moon (1657) and The States and Empires of the Sun (1662), Margaret Cavendish\'s "The Blazing World" (1666),[30][31][32][33] Jonathan Swift\'s Gulliver\'s Travels (1726), Ludvig Holberg\'s Nicolai Klimii Iter Subterraneum (1741) and Voltaire\'s Micromégas (1752) are regarded as some of the first true science-fantasy works.[34][35] Isaac Asimov and Carl Sagan considered Somnium the first science-fiction story; it depicts a journey to the Moon and how the Earth\'s motion is seen from there.[36][37]', 'Following the 17th-century development of the novel as a literary form, Mary Shelley\'s Frankenstein (1818) and The Last Man (1826) helped define the form of the science-fiction novel. Brian Aldiss has argued that Frankenstein was the first work of science fiction.[38][39] Edgar Allan Poe wrote several stories considered to be science fiction, including "The Unparalleled Adventure of One Hans Pfaall" (1835) which featured a trip to the Moon.[40][41] Jules Verne was noted for his attention to detail and scientific accuracy, especially in Twenty Thousand Leagues Under the Sea (1870).[42][43][44][45] In 1887, the novel El anacronópete by Spanish author Enrique Gaspar y Rimbau introduced the first time machine.[46][47]  A rather unknown early French/Belgian science fiction writer was J.-H. Rosny aîné (1856–1940).[48]', 'Many critics consider H. G. Wells one of science fiction\'s most important authors,[42][49] or even "the Shakespeare of science fiction."[50] His notable science-fiction works include The Time Machine (1895), The Island of Doctor Moreau (1896), The Invisible Man (1897), and The War of the Worlds (1898). His science fiction imagined alien invasion, biological engineering, invisibility, and time travel. In his non-fiction futurologist works he predicted the advent of airplanes, military tanks, nuclear weapons, satellite television, space travel, and something resembling the World Wide Web.[51]', "Edgar Rice Burroughs'  A Princess of Mars, published in 1912, was the first of his three-decade-long planetary romance series of Barsoom novels which were set on Mars and featured John Carter as the hero.[52]  In 1926, Hugo Gernsback published the first American science-fiction magazine, Amazing Stories. In its first issue he wrote:", "By 'scientifiction' I mean the Jules Verne, H. G. Wells and Edgar Allan Poe type of story—a charming romance intermingled with scientific fact and prophetic vision... Not only do these amazing tales make tremendously interesting reading—they are always instructive.  They supply knowledge... in a very palatable form... New adventures pictured for us in the scientifiction of today are not at all impossible of realization tomorrow... Many great science stories destined to be of historical interest are still to be written... Posterity will point to them as having blazed a new trail, not only in literature and fiction, but progress as well.[53][54][55]", 'In 1928, E. E. "Doc" Smith\'s first published work, The Skylark of Space, written in collaboration with Lee Hawkins Garby, appeared in Amazing Stories.  It is often called the first great space opera.[56] The same year, Philip Francis Nowlan\'s original Buck Rogers story, Armageddon 2419, also appeared in Amazing Stories. This was followed by a Buck Rogers comic strip, the first serious science-fiction comic.[57]', 'In 1937, John W. Campbell became editor of Astounding Science Fiction, an event which is sometimes considered the beginning of the Golden Age of Science Fiction, which is characterized by stories celebrating scientific achievement and progress.[58][59] In 1942, Isaac Asimov started his Foundation series, which chronicles the rise and fall of galactic empires and introduced psychohistory.[60][61] The series was later awarded a one-time Hugo Award for "Best All-Time Series."[62][63]  The "Golden Age" is often said to have ended in 1946, but sometimes the late 1940s and the 1950s are included.[64]', "Theodore Sturgeon's More Than Human (1953) explored possible future human evolution.[65][66][67] In 1957, Andromeda: A Space-Age Tale by the Russian writer and paleontologist Ivan Yefremov presented a view of a future interstellar communist civilization and is considered one of the most important Soviet science fiction novels.[68][69]  In 1959, Robert A. Heinlein's Starship Troopers marked a departure from his earlier juvenile stories and novels.[70] It is one of the first and most influential examples of military science fiction,[71][72] and introduced the concept of powered armor exoskeletons.[73][74][75] The German space opera series Perry Rhodan, written by various authors, started in 1961 with an account of the first Moon landing[76] and has since expanded in space to multiple universes, and in time by billions of years.[77] It has become the most popular science fiction book series of all time.[78]", 'In the 1960s and 1970s, New Wave science fiction was known for its embrace of a high degree of experimentation, both in form and in content, and a highbrow and self-consciously "literary" or "artistic" sensibility.[34][79][80] In 1961, Solaris by Stanisław Lem was published in Poland.[81] The novel dealt with the theme of human limitations as its characters attempted to study a seemingly intelligent ocean on a newly discovered planet.[82][83] 1965\'s Dune by Frank Herbert featured a much more complex and detailed imagined future society than had previous science fiction.[84]', "In 1967 Anne McCaffrey began her Dragonriders of Pern science fantasy series.[85] Two of the novellas included in the first novel, Dragonflight, made McCaffrey the first woman to win a Hugo or Nebula Award.[86]  In 1968, Philip K. Dick's Do Androids Dream of Electric Sheep?, was published. It is the literary source of the Blade Runner movie franchise.[87][88] 1969's The Left Hand of Darkness by Ursula K. Le Guin was set on a planet in which the inhabitants have no fixed gender. It is one of the most influential examples of social science fiction, feminist science fiction, and anthropological science fiction.[89][90][91]", 'In 1979, Science Fiction World began publication in the People\'s Republic of China.[92] It dominates the Chinese science fiction magazine market, at one time claiming a circulation of 300,000 copies per issue and an estimated 3–5 readers per copy (giving it a total estimated readership of at least 1\xa0million), making it the world\'s most popular science fiction periodical.[93]  In 1984, William Gibson\'s first novel, Neuromancer, helped popularize cyberpunk and the word "cyberspace," a term he originally coined in his 1982 short story Burning Chrome.[94][95][96] In 1986, Shards of Honor by Lois McMaster Bujold began her Vorkosigan Saga.[97][98] 1992\'s Snow Crash by Neal Stephenson predicted immense social upheaval due to the information revolution.[99]', "In 2007, Liu Cixin's novel, The Three-Body Problem, was published in China. It was translated into English by Ken Liu and published by Tor Books in 2014,[100] and won the 2015 Hugo Award for Best Novel,[101] making Liu the first Asian writer to win the award.[102]", 'Emerging themes in late 20th and early 21st century science fiction include environmental issues, the implications of the Internet and the expanding information universe, questions about biotechnology, nanotechnology, and post-scarcity societies.[103][104] Recent trends and subgenres include steampunk,[105] biopunk,[106][107] and mundane science fiction.[108][109]', "The first, or at least one of the first, recorded science fiction film is 1902's A Trip to the Moon, directed by French filmmaker Georges Méliès.[110] It was profoundly influential on later filmmakers, bringing a different kind of creativity and fantasy to the cinematic medium.[111][112] In addition, Méliès's innovative editing and special effects techniques were widely imitated and became important elements of the medium.[113][114]", "1927's Metropolis, directed by Fritz Lang, is the first feature-length science fiction film.[115] Though not well received in its time,[116] it is now considered a great and influential film.[117][118][119] In 1954, Godzilla, directed by Ishirō Honda, began the kaiju subgenre of science fiction film, which feature large creatures of any form, usually attacking a major city or engaging other monsters in battle.[120][121]", "1968's 2001: A Space Odyssey, directed by Stanley Kubrick and based on the work of Arthur C. Clarke, rose above the mostly B-movie offerings up to that time both in scope and quality, and greatly influenced later science fiction films.[122][123][124][125] That same year, Planet of the Apes (the original), directed by Franklin J. Schaffner and based on the 1963 French novel La Planète des Singes by Pierre Boulle, was released to popular and critical acclaim, due in large part to its vivid depiction of a post-apocalyptic world in which intelligent apes dominate humans.[126]", 'In 1977, George Lucas began the Star Wars film series with the film now identified as "Star Wars: Episode IV – A New Hope."[127] The series, often called a space opera,[128] went on to become a worldwide popular culture phenomenon,[129][130] and the second-highest-grossing film series of all time.[131]', 'Since the 1980s, science fiction films, along with fantasy, horror, and superhero films, have dominated Hollywood\'s big-budget productions.[132][131] Science fiction films often "cross-over" with other genres, including animation (WALL-E – 2008, Big Hero 6 – 2014), gangster (Sky Racket – 1937), Western (Serenity – 2005), comedy (Spaceballs −1987, Galaxy Quest – 1999), war (Enemy Mine – 1985), action (Edge of Tomorrow – 2014, The Matrix – 1999), adventure (Jupiter Ascending – 2015, Interstellar – 2014), sports (Rollerball – 1975), mystery (Minority Report – 2002), thriller (Ex Machina – 2014), horror (Alien – 1979), film noir (Blade Runner – 1982), superhero (Marvel Cinematic Universe – 2008–), drama (Melancholia – 2011, Predestination – 2014), and romance (Her – 2013).[133]', 'Science fiction and television have consistently been in a close relationship. Television or television-like technologies frequently appeared in science fiction long before television itself became widely available in the late 1940s and early 1950s.[134]', "The first known science fiction television program was a thirty-five-minute adapted excerpt of the play RUR, written by the Czech playwright Karel Čapek, broadcast live from the BBC's Alexandra Palace studios on 11 February 1938.[135] The first popular science fiction program on American television was the children's adventure serial Captain Video and His Video Rangers, which ran from June 1949 to April 1955.[136]", 'The Twilight Zone (the original series), produced and narrated by Rod Serling, who also wrote or co-wrote most of the episodes, ran from 1959 to 1964. It featured fantasy, suspense, and horror as well as science fiction, with each episode being a complete story.[137][138] Critics have ranked it as one of the best TV programs of any genre.[139][140]', 'The animated series The Jetsons, while intended as comedy and only running for one season (1962–1963), predicted many inventions now in common use: flat-screen televisions, newspapers on a computer-like screen, computer viruses, video chat, tanning beds, home treadmills, and more.[141] In 1963, the time travel-themed Doctor Who premiered on BBC Television.[142] The original series ran until 1989 and was revived in 2005.[143] It has been extremely popular worldwide and has greatly influenced later TV science fiction.[144][145][146] Other programs in the 1960s included The Outer Limits (1963–1965),[147] Lost in Space (1965–1968), and The Prisoner (1967).[148][149][150]', 'Star Trek (the original series), created by Gene Roddenberry, premiered in 1966 on NBC Television and ran for three seasons.[151] It combined elements of space opera and Space Western.[152] Only mildly successful at first, the series gained popularity through syndication and extraordinary fan interest. It became a very popular and influential franchise with many films, television shows, novels, and other works and products.[153][154][155][156] Star Trek: The Next Generation (1987–1994) led to six additional live action Star Trek shows (Deep Space 9 (1993–1999), Voyager (1995–2001), Enterprise (2001–2005), Discovery (2017–present), Picard (2020–present), and Strange New Worlds (2022–present)) with more in some form of development.[157][158][159][160]', 'The miniseries V premiered in 1983 on NBC.[161]  It depicted an attempted takeover of Earth by reptilian aliens.[162] Red Dwarf, a comic science fiction series aired on BBC Two between 1988 and 1999, and on Dave since 2009.[163] The X-Files, which featured UFOs and conspiracy theories, was created by Chris Carter and broadcast by Fox Broadcasting Company from 1993 to 2002,[164][165] and again from 2016 to 2018.[166][167] Stargate, a film about ancient astronauts and interstellar teleportation, was released in 1994. Stargate SG-1 premiered in 1997 and ran for 10 seasons (1997–2007). Spin-off series included Stargate Infinity (2002–2003), Stargate Atlantis (2004–2009), and Stargate Universe (2009–2011).[168] Other 1990s series included Quantum Leap (1989–1993) and Babylon 5 (1994–1999).[169]', 'SyFy, launched in 1992 as The Sci-Fi Channel,[170] specializes in science fiction, supernatural horror, and fantasy.[171][172]', 'The space-Western series Firefly premiered in 2002 on Fox. It is set in the year 2517, after the arrival of humans in a new star system, and follows the adventures of the renegade crew of Serenity, a "Firefly-class" spaceship.[173]Orphan Black began its 5-season run in 2013, about a woman who assumes the identity of one of her several genetically identical human clones. In late 2015 SyFy premiered The Expanse to great critical acclaim, an American TV series about Humanity\'s colonization of the Solar System. Its later seasons would then be aired through Amazon Prime Video.', "Science fiction's rapid rise in popularity during the first half of the 20th century was closely tied to the popular respect paid to science at that time, as well as the rapid pace of technological innovation and new inventions.[174] Science fiction has often predicted scientific and technological progress.[175][176] Some works predict that new inventions and progress will tend to improve life and society, for instance the stories of Arthur C. Clarke and Star Trek.[177] Others, such as H.G. Wells's The Time Machine and Aldous Huxley's Brave New World, warn about possible negative consequences.[178][179]", 'In 2001 the National Science Foundation conducted a survey on "Public Attitudes and Public Understanding: Science Fiction and Pseudoscience."[180] It found that people who read or prefer science fiction may think about or relate to science differently than other people. They also tend to support the space program and the idea of contacting extraterrestrial civilizations.[180][181] Carl Sagan wrote: "Many scientists deeply involved in the exploration of the solar system (myself among them) were first turned in that direction by science fiction."[182]', 'Brian Aldiss described science fiction as "cultural wallpaper."[183] Evidence for this widespread influence can be found in trends for writers to employ science fiction as a tool for advocacy and generating cultural insights, as well as for educators when teaching across a range of academic disciplines not limited to the natural sciences.[184]  Scholar and science fiction critic George Edgar Slusser said that science fiction "is the one real international literary form we have today, and as such has branched out to visual media, interactive media and on to whatever new media the world will invent in the 21st century. Crossover issues between the sciences and the humanities are crucial for the century to come."[185]', "Science fiction has sometimes been used as a means of social protest. George Orwell's Nineteen Eighty-Four (1949) is an important work of dystopian science fiction.[186][187]  It is often invoked in protests against governments and leaders who are seen as totalitarian.[188][189] James Cameron's 2009 film Avatar was intended as a protest against imperialism, and specifically the European colonization of the Americas.[190]", "Robots, artificial humans, human clones, intelligent computers, and their possible conflicts with human society have all been major themes of science fiction since, at least, the publication of Shelly's Frankenstein. Some critics have seen this as reflecting authors’ concerns over the social alienation seen in modern society.[191]", 'Feminist science fiction poses questions about social issues such as how society constructs gender roles, the role reproduction plays in defining gender, and the inequitable political or personal power of one gender over others. Some works have illustrated these themes using utopias to explore a society in which gender differences or gender power imbalances do not exist, or dystopias to explore worlds in which gender inequalities are intensified, thus asserting a need for feminist work to continue.[192][193]', 'Climate fiction, or "cli-fi," deals with issues concerning climate change and global warming.[194][195] University courses on literature and environmental issues may include climate change fiction in their syllabi,[196] and it is often discussed by other media outside of science fiction fandom.[197]', 'Libertarian science fiction focuses on the politics and social order implied by right libertarian philosophies with an emphasis on individualism and private property, and in some cases anti-statism.[198]', 'Science fiction comedy often satirizes and criticizes present-day society, and sometimes makes fun of the conventions and clichés of more serious science fiction.[199][200]', "The potential for Science Fiction as a genre is not just limited to being a literary sandbox for exploring otherworldly narratives but can act as a vehicle to analyze and recognize a society's past, present, and potential future social relationships with the Other. More specifically, Science Fiction offers a medium and representation of Alterity and differences in social identity. [201]", 'Science fiction is often said to inspire a "sense of wonder."  Science fiction editor and critic David Hartwell wrote: "Science fiction’s appeal lies in combination of the rational, the believable, with the miraculous. It is an appeal to the sense of wonder."[202] Carl Sagan said: "One of the great benefits of science fiction is that it can convey bits and pieces, hints, and phrases, of knowledge unknown or inaccessible to the reader . . . works you ponder over as the water is running out of the bathtub or as you walk through the woods in an early winter snowfall."[182]', 'In 1967, Isaac Asimov commented on the changes then occurring in the science fiction community: "And because today’s real life so resembles day-before-yesterday’s fantasy, the old-time fans are restless. Deep within, whether they admit it or not, is a feeling of disappointment and even outrage that the outer world has invaded their private domain. They feel the loss of a \'sense of wonder\' because what was once truly confined to \'wonder\' has now become prosaic and mundane."[203]', 'The study of science fiction, or science fiction studies, is the critical assessment, interpretation, and discussion of science fiction literature, film, TV shows, new media, fandom, and fan fiction.[204] Science fiction scholars study science fiction to better understand it and its relationship to science, technology, politics, other genres, and culture-at-large.[205] Science fiction studies began around the turn of the 20th century, but it was not until later that science fiction studies solidified as a discipline with the publication of the academic journals Extrapolation (1959), Foundation: The International Review of Science Fiction (1972), and Science Fiction Studies (1973),[206][207] and the establishment of the oldest organizations devoted to the study of science fiction in 1970, the Science Fiction Research Association and the Science Fiction Foundation.[208][209] The field has grown considerably since the 1970s with the establishment of more journals, organizations, and conferences, as well as science fiction degree-granting programs such as those offered by the University of Liverpool[210] and the University of Kansas.[211]', 'Science fiction has historically been sub-divided between hard science fiction and soft science fiction, with the division centering on the feasibility of the science central to the story.[212] However, this distinction has come under increasing scrutiny in the 21st century. Some authors, such as Tade Thompson and Jeff VanderMeer, have pointed out that stories that focus explicitly on physics, astronomy, mathematics, and engineering tend to be considered "hard" science fiction, while stories that focus on botany, mycology, zoology, and the social sciences tend to be categorized as "soft," regardless of the relative rigor of the science.[213]', 'Max Gladstone defined "hard" science fiction as stories "where the math works," but pointed out that this ends up with stories that often seem "weirdly dated," as scientific paradigms shift over time.[214] Michael Swanwick dismissed the traditional definition of "hard" SF altogether, instead saying that it was defined by characters striving to solve problems "in the right way–with determination, a touch of stoicism, and the consciousness that the universe is not on his or her side."[213]', 'Ursula K. Le Guin also criticized the more traditional view on the difference between "hard" and "soft" SF: "The \'hard\' science fiction writers dismiss everything except, well, physics, astronomy, and maybe chemistry. Biology, sociology, anthropology—that\'s not science to them, that\'s soft stuff. They\'re not that interested in what human beings do, really. But I am. I draw on the social sciences a great deal."[215]', 'Respected authors have written science fiction. Mary Shelley wrote a number of science fiction novels including Frankenstein; or, The Modern Prometheus (1818), and is considered a major writer of the Romantic Age.[217] Aldous Huxley\'s Brave New World (1932) is often listed as one of England\'s most important novels, both for its criticism of modern culture and its prediction of future trends including reproductive technology and social engineering.[218][219][220][221] Kurt Vonnegut was a highly respected American author whose works contain science fiction premises or themes.[222][223][224] Other science fiction authors whose works are widely considered to be "serious" literature include Ray Bradbury (including, especially, Fahrenheit 451 (1953) and The Martian Chronicles (1951)),[225] Arthur C. Clarke (especially for Childhood\'s End),[226][227] and Paul Myron Anthony Linebarger, writing under the name Cordwainer Smith.[228] In his book "The Western Canon", literary critic Harold Bloom includes Brave New World, Solaris, Cat\'s Cradle (1963) by Vonnegut, and The Left Hand of Darkness as culturally and aesthetically significant works of western literature.', 'David Barnett  has pointed out that there are books such as The Road (2006) by Cormac McCarthy, Cloud Atlas (2004) by David Mitchell, The Gone-Away World (2008) by Nick Harkaway, The Stone Gods (2007) by Jeanette Winterson, and Oryx and Crake (2003) by Margaret Atwood, which use recognizable science fiction tropes, but whose authors and publishers do not market them as science fiction.[229] Doris Lessing, who was later awarded the Nobel Prize in literature, wrote a series of five SF novels, Canopus in Argos: Archives (1979–1983), which depict the efforts of more advanced species and civilizations to influence those less advanced, including humans on Earth.[230][231][232][233]', 'In her much reprinted 1976 essay "Science Fiction and Mrs Brown," Ursula K. Le Guin was asked: "Can a science fiction writer write a novel?" She answered: "I believe that all novels ... deal with character, and that it is to express character–not to preach doctrines [or] sing songs... that the form of the novel, so clumsy, verbose, and undramatic, so rich, elastic, and alive, has been evolved. ... The great novelists have brought us to see whatever they wish us to see through some character. Otherwise, they would not be novelists, but poets, historians, or pamphleteers."[234] Orson Scott Card, best known for his 1985 science fiction novel Ender\'s Game, has postulated that in science fiction the message and intellectual significance of the work are contained within the story itself and, therefore, does not need stylistic gimmicks or literary games.[235][236]', 'Jonathan Lethem, in a 1998 essay in the Village Voice entitled "Close Encounters: The Squandered Promise of Science Fiction," suggested that the point in 1973 when Thomas Pynchon\'s Gravity\'s Rainbow was nominated for the Nebula Award and was passed over in favor of Clarke\'s Rendezvous with Rama, stands as "a hidden tombstone marking the death of the hope that SF was about to merge with the mainstream."[237] In the same year science fiction author and physicist Gregory Benford wrote: "SF is perhaps the defining genre of the twentieth century, although its conquering armies are still camped outside the Rome of the literary citadels."[238]', 'Science fiction is being written, and has been written, by diverse authors from around the world. According to 2013 statistics by the science fiction publisher Tor Books, men outnumber women by 78% to 22% among submissions to the publisher.[239] A controversy about voting slates in the 2015 Hugo Awards highlighted tensions in the science fiction community between a trend of increasingly diverse works and authors being honored by awards, and reaction by groups of authors and fans who preferred what they considered more "traditional" science fiction.[240]', 'Among the most respected and well-known awards for science fiction are the Hugo Award for literature, presented by the World Science Fiction Society at Worldcon, and voted on by fans;[241] the Nebula Award for literature, presented by the Science Fiction and Fantasy Writers of America, and voted on by the community of authors;[242] the John W. Campbell Memorial Award for Best Science Fiction Novel, presented by a jury of writers;[243] and the Theodore Sturgeon Memorial Award for short fiction, presented by a jury.[244] One notable award for science fiction films and TV programs is the Saturn Award, which is presented annually by The Academy of Science Fiction, Fantasy, and Horror Films.[245]', "There are other national awards, like Canada's Prix Aurora Awards,[246] regional awards, like the Endeavour Award presented at Orycon for works from the U.S. Pacific Northwest,[247] and special interest or subgenre awards such as the Chesley Award for art, presented by the Association of Science Fiction & Fantasy Artists,[248] or the World Fantasy Award for fantasy.[249] Magazines may organize reader polls, notably the Locus Award.[250]", 'Conventions (in fandom, often shortened as "cons," such as "comic-con") are held in cities around the world, catering to a local, regional, national, or international membership.[251][252][253] General-interest conventions cover all aspects of science fiction, while others focus on a particular interest like media fandom, filking, and so on.[254][255] Most science fiction conventions are organized by volunteers in non-profit groups, though most media-oriented events are organized by commercial promoters.[256]', 'Science fiction fandom emerged from the letters column in Amazing Stories magazine. Soon fans began writing letters to each other, and then grouping their comments together in informal publications that became known as fanzines.[257] Once they were in regular contact, fans wanted to meet each other, and they organized local clubs.[257][258] In the 1930s, the first science fiction conventions gathered fans from a wider area.[258]', 'The earliest organized online fandom was the SF Lovers Community, originally a mailing list in the late 1970s with a text archive file that was updated regularly.[259] In the 1980s, Usenet groups greatly expanded the circle of fans online.[260] In the 1990s, the development of the World-Wide Web exploded the community of online fandom by orders of magnitude, with thousands and then millions of websites devoted to science fiction and related genres for all media.[261]', 'The first science fiction fanzine, The Comet, was published in 1930 by the Science Correspondence Club in Chicago, Illinois.[262][263] One of the best known fanzines today is Ansible, edited by David Langford, winner of numerous Hugo awards.[264][265] Other notable fanzines to win one or more Hugo awards include File 770, Mimosa, and Plokta.[266] Artists working for fanzines have frequently risen to prominence in the field, including Brad W. Foster, Teddy Harvia, and Joe Mayhew; the Hugos include a category for Best Fan Artists.[266]', 'Science fiction elements can include, among others:']}, {'headings': ['Science fiction (disambiguation)'], 'subheadings': ['Music', 'Publications', 'See also', 'Navigation menu'], 'paras': ['Science fiction is a genre of fiction dealing with the impact of imagined innovations in science or technology.', 'Science Fiction may also refer to:']}, {'headings': ['2001: A Space Odyssey (film)'], 'subheadings': ['Contents', 'Plot', 'Cast', 'Production', 'Design and special effects', 'Soundtrack', 'Theatrical run and post-premiere cuts', 'Reception', 'Interpretations', 'Home media', 'Rereleases', 'Legacy', 'See also', 'References', 'External links', 'Navigation menu'], 'paras': ['', '2001: A Space Odyssey is a 1968 epic science fiction film produced and directed by Stanley Kubrick. The screenplay was written by Kubrick and science fiction author Arthur C. Clarke, and was inspired by Clarke\'s 1951 short story "The Sentinel" and other short stories by Clarke. Clarke also published a novelisation of the film, in part written concurrently with the screenplay, after the film\'s release. The film stars Keir Dullea, Gary Lockwood, William Sylvester, and Douglas Rain, and follows a voyage by astronauts, scientists and the sentient supercomputer HAL to Jupiter to investigate an alien monolith.', 'The film is noted for its scientifically accurate depiction of space flight, pioneering special effects, and ambiguous imagery. Kubrick avoided conventional cinematic and narrative techniques; dialogue is used sparingly, and there are long sequences accompanied only by music. The soundtrack incorporates numerous works of classical music, by composers including Richard Strauss, Johann Strauss II, Aram Khachaturian, and György Ligeti.', 'The film received diverse critical responses, ranging from those who saw it as darkly apocalyptic to those who saw it as an optimistic reappraisal of the hopes of humanity. Critics noted its exploration of themes such as human evolution, technology, artificial intelligence, and the possibility of extraterrestrial life. It was nominated for four Academy Awards, winning Kubrick the award for his direction of the visual effects. The film is now widely regarded as one of the greatest and most influential films ever made. In 1991, it was deemed "culturally, historically, or aesthetically significant" by the United States Library of Congress and selected for preservation in the National Film Registry.[2][3]', 'In a prehistoric veldt, a tribe of hominins is driven away from its water hole by a rival tribe. The next day, they find an alien monolith has appeared in their midst. They then learn how to use a bone as a weapon and, after their first hunt, return to drive their rivals away with it.', 'Millions of years later, Dr. Heywood Floyd, Chairman of the United States National Council of Astronautics, travels to Clavius Base, an American lunar outpost. During a stopover at Space Station 5, he meets Russian scientists who are concerned that Clavius seems to be unresponsive. He refuses to discuss rumours of an epidemic at the base. At Clavius, Heywood addresses a meeting of personnel to whom he stresses the need for secrecy regarding their newest discovery. His mission is to investigate a recently found artefact, a monolith buried four million years earlier near the lunar crater Tycho. As he and others examine the object, it is struck by sunlight, upon which it emits a high-powered radio signal.', 'Eighteen months later, the American spacecraft Discovery One is bound for Jupiter, with mission pilots and scientists Dr. David "Dave" Bowman and Dr. Frank Poole on board, along with three other scientists in suspended animation. Most of Discovery\'s operations are controlled by HAL, a HAL 9000 computer with a human personality. When HAL reports the imminent failure of an antenna control device, Dave retrieves it in an extravehicular activity (EVA) pod, but finds nothing wrong. HAL suggests reinstalling the device and letting it fail so the problem can be verified. Mission Control advises the astronauts that results from their twin 9000 computer indicate that HAL has made an error, but HAL blames it on human error. Concerned about HAL\'s behaviour, Dave and Frank enter an EVA pod so they can talk without HAL overhearing. They agree to disconnect HAL if he is proven wrong, but HAL follows their conversation by lip reading.', "While Frank is outside the ship to replace the antenna unit, HAL takes control of his pod, setting him adrift. Dave takes another pod to rescue Frank. While he is outside, HAL turns off the life support functions of the crewmen in suspended animation, killing them. When Dave returns to the ship with Frank's body, HAL refuses to let him back in, stating that their plan to deactivate him jeopardises the mission. Dave releases Frank's body and, despite not having a spacesuit helmet, exits his pod, crosses the vacuum and opens the ship's emergency airlock manually. He goes to HAL's processor core and begins disconnecting HAL's circuits, despite HAL begging him not to. When the disconnection is complete, a prerecorded video by Heywood plays, revealing that the mission's objective is to investigate the radio signal sent from the monolith to Jupiter.", 'At Jupiter, Dave finds a third, much larger monolith orbiting the planet. He leaves Discovery in an EVA pod to investigate. He is pulled into a vortex of coloured light and observes bizarre cosmological phenomena and strange landscapes of unusual colours as he passes by. Finally he finds himself in a large neoclassical bedroom where he sees, and then becomes, older versions of himself: first standing in the bedroom, middle-aged and still in his spacesuit, then dressed in leisure attire and eating dinner, and finally as an old man lying in bed. A monolith appears at the foot of the bed, and as Dave reaches for it, he is transformed into a foetus enclosed in a transparent orb of light floating in space above the Earth.', 'After completing Dr. Strangelove (1964), director Stanley Kubrick told a publicist from Columbia Pictures that his next project would be about extraterrestrial life,[9] and resolved to make "the proverbial good science fiction movie".[10] How Kubrick became interested in creating a science fiction film is far from clear.[11] Biographer John Baxter notes possible inspirations in the late 1950s, including British productions featuring dramas on satellites and aliens modifying early humans, Metro-Goldwyn-Mayer\'s big budget CinemaScope production Forbidden Planet, and the slick widescreen cinematography and set design of Japanese kaiju (monster movie) productions (such as Godzilla and Warning from Space).[12]', "Kubrick obtained financing and distribution from the American studio Metro-Goldwyn-Mayer with the selling point that the film could be marketed in their ultra widescreen Cinerama format, recently debuted with their How the West Was Won.[13][14][15] It would be filmed and edited almost entirely in southern England, where Kubrick lived, using the facilities of MGM-British Studios and Shepperton Studios. MGM had subcontracted the production of the film to Kubrick's production company in order to qualify for the Eady Levy, a UK tax on box-office receipts used at the time to fund the production of films in Britain.[16]", "Kubrick's decision to avoid the fanciful portrayals of space found in standard popular science fiction films of the time led him to seek more realistic and accurate depictions of space travel. Illustrators such as Chesley Bonestell, Roy Carnon, and Richard McKenna were hired to produce concept drawings, sketches, and paintings of the space technology seen in the film.[17][18] Two educational films, the National Film Board of Canada's 1960 animated short documentary Universe and the 1964 New York World's Fair movie To the Moon and Beyond, were major influences.[17]", 'According to biographer Vincent LoBrutto, Universe was a visual inspiration to Kubrick.[19] The 29-minute film, which had also proved popular at NASA for its realistic portrayal of outer space, met "the standard of dynamic visionary realism that he was looking for." Wally Gentleman, one of the special-effects artists on Universe, worked briefly on 2001. Kubrick also asked Universe co-director Colin Low about animation camerawork, with Low recommending British mathematician Brian Salt, with whom Low and Roman Kroitor had previously worked on the 1957 still-animation documentary City of Gold.[20][21] Universe\'s narrator, actor Douglas Rain, was cast as the voice of HAL.[22]', 'After pre-production had begun, Kubrick saw To the Moon and Beyond, a film shown in the Transportation and Travel building at the 1964 World\'s Fair. It was filmed in Cinerama 360 and shown in the "Moon Dome". Kubrick hired the company that produced it, Graphic Films Corporation—which had been making films for NASA, the US Air Force, and various aerospace clients—as a design consultant.[17] Graphic Films\' Con Pederson, Lester Novros, and background artist Douglas Trumbull airmailed research-based concept sketches and notes covering the mechanics and physics of space travel, and created storyboards for the space flight sequences in 2001.[17] Trumbull became a special effects supervisor on 2001.[17]', 'Searching for a collaborator in the science fiction community for the writing of the script, Kubrick was advised by a mutual acquaintance, Columbia Pictures staffer Roger Caras, to talk to writer Arthur C. Clarke, who lived in Ceylon. Although convinced that Clarke was "a recluse, a nut who lives in a tree," Kubrick allowed Caras to cable the film proposal to Clarke. Clarke\'s cabled response stated that he was "frightfully interested in working with [that] enfant terrible", and added "what makes Kubrick think I\'m a recluse?"[19][23] Meeting for the first time at Trader Vic\'s in New York on 22 April 1964, the two began discussing the project that would take up the next four years of their lives.[24] Clarke kept a diary throughout his involvement with 2001, excerpts of which were published in 1972 as The Lost Worlds of 2001.[25]', 'Kubrick told Clarke he wanted to make a film about "Man\'s relationship to the universe",[26] and was, in Clarke\'s words, "determined to create a work of art which would arouse the emotions of wonder, awe\xa0... even, if appropriate, terror".[24] Clarke offered Kubrick six of his short stories, and by May 1964, Kubrick had chosen "The Sentinel" as the source material for the film. In search of more material to expand the film\'s plot, the two spent the rest of 1964 reading books on science and anthropology, screening science fiction films, and brainstorming ideas.[27] They created the plot for 2001 by integrating several different short story plots written by Clarke, along with new plot segments requested by Kubrick for the film development, and then combined them all into a single script for 2001.[28][29] Clarke said that his 1953 story "Encounter in the Dawn" inspired the film\'s "Dawn of Man" sequence.[30]', 'Kubrick and Clarke privately referred to the project as How the Solar System Was Won, a reference to how it was a follow-on to MGM\'s Cinerama epic How the West Was Won.[15] On 23 February 1965, Kubrick issued a press release announcing the title as Journey Beyond The Stars.[31] Other titles considered included Universe, Tunnel to the Stars, and Planetfall. Expressing his high expectations for the thematic importance which he associated with the film, in April 1965, eleven months after they began working on the project, Kubrick selected 2001: A Space Odyssey; Clarke said the title was "entirely" Kubrick\'s idea.[32] Intending to set the film apart from the "monsters-and-sex" type of science-fiction films of the time, Kubrick used Homer\'s The Odyssey as both a model of literary merit and a source of inspiration for the title. Kubrick said, "It occurred to us that for the Greeks the vast stretches of the sea must have had the same sort of mystery and remoteness that space has for our generation."[33]', 'How much would we appreciate La Gioconda today if Leonardo had written at the bottom of the canvas: "This lady is smiling slightly because she has rotten teeth" — or "because she\'s hiding a secret from her lover"? It would shut off the viewer\'s appreciation and shackle him to a reality other than his own. I don\'t want that to happen to 2001.', '—Stanley Kubrick, Playboy, 1968[34]', 'Originally, Kubrick and Clarke had planned to develop a 2001 novel first, free of the constraints of film, and then write the screenplay. They planned the writing credits to be "Screenplay by Stanley Kubrick and Arthur C. Clarke, based on a novel by Arthur C. Clarke and Stanley Kubrick" to reflect their preeminence in their respective fields.[35] In practice, the screenplay developed in parallel with the novel, with only some elements being common to both. In a 1970 interview, Kubrick said:', "There are a number of differences between the book and the movie. The novel, for example, attempts to explain things much more explicitly than the film does, which is inevitable in a verbal medium. The novel came about after we did a 130-page prose treatment of the film at the very outset.\xa0... Arthur took all the existing material, plus an impression of some of the rushes, and wrote the novel. As a result, there's a difference between the novel and the film\xa0... I think that the divergences between the two works are interesting.[36]", 'In the end, Clarke and Kubrick wrote parts of the novel and screenplay simultaneously, with the film version being released before the book version was published. Clarke opted for clearer explanations of the mysterious monolith and Star Gate in the novel; Kubrick made the film more cryptic by minimising dialogue and explanation.[37] Kubrick said the film is "basically a visual, nonverbal experience" that "hits the viewer at an inner level of consciousness, just as music does, or painting".[38]', 'The screenplay credits were shared whereas the 2001 novel, released shortly after the film, was attributed to Clarke alone. Clarke wrote later that "the nearest approximation to the complicated truth" is that the screenplay should be credited to "Kubrick and Clarke" and the novel to "Clarke and Kubrick".[39] Early reports about tensions involved in the writing of the film script appeared to reach a point where Kubrick was allegedly so dissatisfied with the collaboration that he approached other writers who could replace Clarke, including Michael Moorcock and J.G. Ballard. But they felt it would be disloyal to accept Kubrick\'s offer.[40] In Michael Benson\'s 2018 book Space Odyssey: Stanley Kubrick, Arthur C. Clarke, and the Making of a Masterpiece, the actual relation between Clarke and Kubrick was more complex, involving an extended interaction of Kubrick\'s multiple requests for Clarke to write new plot lines for various segments of the film, which Clarke was expected to withhold from publication until after the release of the film while receiving advances on his salary from Kubrick during film production. Clarke agreed to this, though apparently he did make several requests for Kubrick to allow him to develop his new plot lines into separate publishable stories while film production continued, which Kubrick consistently denied on the basis of Clarke\'s contractual obligation to withhold publication until release of the film.[29]', 'Astronomer Carl Sagan wrote in his 1973 book The Cosmic Connection that Clarke and Kubrick had asked him how to best depict extraterrestrial intelligence. While acknowledging Kubrick\'s desire to use actors to portray humanoid aliens for convenience\'s sake, Sagan argued that alien life forms were unlikely to bear any resemblance to terrestrial life, and that to do so would introduce "at least an element of falseness" to the film. Sagan proposed that the film should simply suggest extraterrestrial superintelligence, rather than depict it. He attended the premiere and was "pleased to see that I had been of some help."[41] Sagan had met with Clarke and Kubrick only once, in 1964; and Kubrick subsequently directed several attempts to portray credible aliens, only to abandon the idea near the end of post-production. Benson asserts it is unlikely that Sagan\'s advice had any direct influence.[29] Kubrick hinted at the nature of the mysterious unseen alien race in 2001 by suggesting that given millions of years of evolution, they progressed from biological beings to "immortal machine entities" and then into "beings of pure energy and spirit" with "limitless capabilities and ungraspable intelligence".[42]', "In a 1980 interview (not released during Kubrick's lifetime), Kubrick explains one of the film's closing scenes, where Bowman is depicted in old age after his journey through the Star Gate:", 'The idea was supposed to be that he is taken in by godlike entities, creatures of pure energy and intelligence with no shape or form. They put him in what I suppose you could describe as a human zoo to study him, and his whole life passes from that point on in that room. And he has no sense of time.\xa0... [W]hen they get finished with him, as happens in so many myths of all cultures in the world, he is transformed into some kind of super being and sent back to Earth, transformed and made some kind of superman. We have to only guess what happens when he goes back. It is the pattern of a great deal of mythology, and that is what we were trying to suggest.[43]', 'The script went through many stages. In early 1965, when backing was secured for the film, Clarke and Kubrick still had no firm idea of what would happen to Bowman after the Star Gate sequence. Initially all of Discovery\'s astronauts were to survive the journey; by 3 October, Clarke and Kubrick had decided to make Bowman the sole survivor and have him regress to infancy. By 17 October, Kubrick had come up with what Clarke called a "wild idea of slightly fag robots who create a Victorian environment to put our heroes at their ease."[39] HAL 9000 was originally named Athena after the Greek goddess of wisdom and had a feminine voice and persona.[39]', 'Early drafts included a prologue containing interviews with scientists about extraterrestrial life,[44] voice-over narration (a feature in all of Kubrick\'s previous films),[a] a stronger emphasis on the prevailing Cold War balance of terror, and a different and more explicitly explained breakdown for HAL.[46][47] Other changes include a different monolith for the "Dawn of Man" sequence, discarded when early prototypes did not photograph well; the use of Saturn as the final destination of the Discovery mission rather than Jupiter, discarded when the special effects team could not develop a convincing rendition of Saturn\'s rings; and the finale of the Star Child exploding nuclear weapons carried by Earth-orbiting satellites,[47] which Kubrick discarded for its similarity to his previous film, Dr. Strangelove.[44][47] The finale and many of the other discarded screenplay ideas survived in Clarke\'s novel.[47]', 'Kubrick made further changes to make the film more nonverbal, to communicate on a visual and visceral level rather than through conventional narrative.[34] By the time shooting began, Kubrick had removed much of the dialogue and narration.[48] Long periods without dialogue permeate the film: the film has no dialogue for roughly the first and last twenty minutes,[49] as well as for the 10 minutes from Floyd\'s Moonbus landing near the monolith until Poole watches a BBC newscast on Discovery. What dialogue remains is notable for its banality (making the computer HAL seem to have more emotion than the humans) when juxtaposed with the epic space scenes.[48] Vincent LoBrutto wrote that Clarke\'s novel has its own "strong narrative structure" and precision, while the narrative of the film remains symbolic, in accord with Kubrick\'s final intentions.[50]', 'Principal photography began on 29 December 1965, in Stage H at Shepperton Studios, Shepperton, England. The studio was chosen because it could house the 60-by-120-by-60-foot (18\xa0m ×\xa037\xa0m ×\xa018\xa0m) pit for the Tycho crater excavation scene, the first to be shot. In January 1966, the production moved to the smaller MGM-British Studios in Borehamwood, where the live-action and special-effects filming was done, starting with the scenes involving Floyd on the Orion spaceplane;[51] it was described as a "huge throbbing nerve center\xa0... with much the same frenetic atmosphere as a Cape Kennedy blockhouse during the final stages of Countdown."[52] The only scene not filmed in a studio—and the last live-action scene shot for the film—was the skull-smashing sequence, in which Moonwatcher (Richter) wields his newfound bone "weapon-tool" against a pile of nearby animal bones. A small elevated platform was built in a field near the studio so that the camera could shoot upward with the sky as background, avoiding cars and trucks passing by in the distance.[53][54] The Dawn of Man sequence that opens the film was filmed at Borehamwood by John Alcott after Geoffrey Unsworth left to work on other projects.[55][56] The still photographs in the background for the Dawn of Man sequence were photographed in Namibia.[57]', 'Filming of actors was completed in September 1967,[58] and from June 1966 until March 1968, Kubrick spent most of his time working on the 205 special-effects shots in the film.[36] He ordered the special-effects technicians to use the painstaking process of creating all visual effects seen in the film "in camera", avoiding degraded picture quality from the use of blue screen and travelling matte techniques. Although this technique, known as "held takes", resulted in a much better image, it meant exposed film would be stored for long periods of time between shots, sometimes as long as a year.[59] In March 1968, Kubrick finished the "pre-premiere" editing of the film, making his final cuts just days before the film\'s general release in April 1968.[36]', 'The film was announced in 1965 as a "Cinerama"[60] film and was photographed in Super Panavision 70 (which uses a 65\xa0mm negative combined with spherical lenses to create an aspect ratio of 2.20:1). It would eventually be released in a limited "roadshow" Cinerama version, then in 70\xa0mm and 35\xa0mm versions.[61][62] Colour processing and 35\xa0mm release prints were done using Technicolor\'s dye transfer process. The 70\xa0mm prints were made by MGM Laboratories, Inc. on Metrocolor. The production was $4.5\xa0million over the initial $6\xa0million budget and 16 months behind schedule.[63]', 'For the opening sequence involving tribes of apes, professional mime Daniel Richter played the lead ape and choreographed the movements of the other man-apes, who were mostly portrayed by his mime troupe.[53]', "Kubrick and Clarke consulted IBM on plans for HAL, though plans to use the company's logo never materialised.[64]", "The film was edited before it was publicly screened, cutting out, among other things, a painting class on the lunar base that included Kubrick's daughters, additional scenes of life on the base, and Floyd buying a bush baby for his daughter from a department store via videophone.[65] A ten-minute black-and-white opening sequence featuring interviews with scientists, including Freeman Dyson discussing off-Earth life,[66] was removed after an early screening for MGM executives.[67]", 'From early in production, Kubrick decided that he wanted the film to be a primarily nonverbal experience[68] that did not rely on the traditional techniques of narrative cinema, and in which music would play a vital role in evoking particular moods. About half the music in the film appears either before the first line of dialogue or after the final line. Almost no music is heard during scenes with dialogue.[69]', "The film is notable for its innovative use of classical music taken from existing commercial recordings. Most feature films, then and now, are typically accompanied by elaborate film scores or songs written specially for them by professional composers. In the early stages of production, Kubrick commissioned a score for 2001 from Hollywood composer Alex North, who had written the score for Spartacus and also had worked on Dr. Strangelove.[70] During post-production, Kubrick chose to abandon North's music in favour of the now-familiar classical pieces he had earlier chosen as temporary music for the film. North did not learn that his score had been abandoned until he saw the film's premiere.[69]", 'Kubrick involved himself in every aspect of production, even choosing the fabric for his actors\' costumes,[71] and selecting notable pieces of contemporary furniture for use in the film. When Floyd exits the Space Station\xa05 elevator, he is greeted by an attendant seated behind a slightly modified George Nelson Action Office desk from Herman Miller\'s 1964 "Action Office" series.[b][72][c] Danish designer Arne Jacobsen designed the cutlery used by the Discovery astronauts in the film.[73][74][75]', 'Other examples of modern furniture in the film are the bright red Djinn chairs seen prominently throughout the space station[76][77] and Eero Saarinen\'s 1956 pedestal tables. Olivier Mourgue, designer of the Djinn chair, has used the connection to 2001 in his advertising; a frame from the film\'s space station sequence and three production stills appear on the homepage of Mourgue\'s website.[78] Shortly before Kubrick\'s death, film critic Alexander Walker informed Kubrick of Mourgue\'s use of the film, joking to him "You\'re keeping the price up".[79] Commenting on their use in the film, Walker writes:', 'Everyone recalls one early sequence in the film, the space hotel, primarily because the custom-made Olivier Mourgue furnishings, those foam-filled sofas, undulant and serpentine, are covered in scarlet fabric and are the first stabs of colour one sees. They resemble Rorschach "blots" against the pristine purity of the rest of the lobby.[80]', "Detailed instructions in relatively small print for various technological devices appear at several points in the film, the most visible of which are the lengthy instructions for the zero-gravity toilet on the Aries Moon shuttle. Similar detailed instructions for replacing the explosive bolts also appear on the hatches of the EVA pods, most visibly in closeup just before Bowman's pod leaves the ship to rescue Frank Poole.[d]", 'The film features an extensive use of Eurostile Bold Extended, Futura and other sans serif typefaces as design elements of the 2001 world.[82] Computer displays show high-resolution fonts, colour, and graphics that were far in advance of what most computers were capable of in the 1960s, when the film was made.[81]', 'Kubrick was personally involved in the design of the monolith and its form for the film. The first design for the monolith for the 2001 film was a transparent tetrahedral pyramid. This was taken from the short story "The Sentinel" that the first story was based on.[83][84]', "A London firm was approached by Kubrick to provide a 12-foot (3.7\xa0m) transparent plexiglass pyramid, and due to construction constraints they recommended a flat slab shape. Kubrick approved, but was disappointed with the glassy appearance of the transparent prop on set, leading art director Anthony Masters to suggest making the monolith's surface matte black.[29]", 'To heighten the reality of the film, very intricate models of the various spacecraft and locations were built. Their sizes ranged from about two-foot-long models of satellites and the Aries translunar shuttle up to the 55-foot (17\xa0m)-long model of the Discovery One spacecraft. "In-camera" techniques were again used as much as possible to combine models and background shots together to prevent degradation of the image through duplication.[85][86]', 'In shots where there was no perspective change, still shots of the models were photographed and positive paper prints were made. The image of the model was cut out of the photographic print and mounted on glass and filmed on an animation stand. The undeveloped film was re-wound to film the star background with the silhouette of the model photograph acting as a matte to block out where the spaceship image was.[85]', "Shots where the spacecraft had parts in motion or the perspective changed were shot by directly filming the model. For most shots the model was stationary and camera was driven along a track on a special mount, the motor of which was mechanically linked to the camera motor—making it possible to repeat camera moves and match speeds exactly. Elements of the scene were recorded on the same piece of film in separate passes to combine the lit model, stars, planets, or other spacecraft in the same shot. In moving shots of the long Discovery One spacecraft, in order to keep the entire model in focus (and preserve its sense of scale), the camera's aperture was stopped down for maximum depth-of-field, and each frame was exposed for several seconds.[87] Many matting techniques were tried to block out the stars behind the models, with filmmakers sometimes resorting to hand-tracing frame by frame around the image of the spacecraft (rotoscoping) to create the matte.[85][88]", 'Some shots required exposing the film again to record previously filmed live-action shots of the people appearing in the windows of the spacecraft or structures. This was achieved by projecting the window action onto the models in a separate camera pass or, when two-dimensional photographs were used, projecting from the backside through a hole cut in the photograph.[87]', 'All of the shots required multiple takes so that some film could be developed and printed to check exposure, density, alignment of elements, and to supply footage used for other photographic effects, such as for matting.[85][88]', 'For spacecraft interior shots, ostensibly containing a giant centrifuge that produces artificial gravity, Kubrick had a 30-short-ton (27\xa0t) rotating "ferris wheel" built by Vickers-Armstrong Engineering Group at a cost of $750,000. The set was 38 feet (12\xa0m) in diameter and 10 feet (3.0\xa0m) wide.[89] Various scenes in the Discovery centrifuge were shot by securing set pieces within the wheel, then rotating it while the actor walked or ran in sync with its motion, keeping him at the bottom of the wheel as it turned. The camera could be fixed to the inside of the rotating wheel to show the actor walking completely "around" the set, or mounted in such a way that the wheel rotated independently of the stationary camera, as in the jogging scene where the camera appears to alternately precede and follow the running actor.[90]', 'The shots where the actors appear on opposite sides of the wheel required one of the actors to be strapped securely into place at the "top" of the wheel as it moved to allow the other actor to walk to the "bottom" of the wheel to join him. The most notable case is when Bowman enters the centrifuge from the central hub on a ladder, and joins Poole, who is eating on the other side of the centrifuge. This required Gary Lockwood to be strapped into a seat while Keir Dullea walked toward him from the opposite side of the wheel as it turned with him.[90]', 'Another rotating set appeared in an earlier sequence on board the Aries trans-lunar shuttle. A stewardess is shown preparing in-flight meals, then carrying them into a circular walkway. Attached to the set as it rotates 180 degrees, the camera\'s point of view remains constant, and she appears to walk up the "side" of the circular walkway, and steps, now in an "upside-down" orientation, into a connecting hallway.[91]', "The realistic-looking effects of the astronauts floating weightless in space and inside the spacecraft were accomplished by suspending the actors from wires attached to the top of the set and placing the camera beneath them. The actors' bodies blocked the camera's view of the wires and appeared to float. For the shot of Poole floating into the pod's arms during Bowman's recovery of him, a stuntman on a wire portrayed the movements of an unconscious man and was shot in slow motion to enhance the illusion of drifting through space.[92] The scene showing Bowman entering the emergency airlock from the EVA pod was done similarly: an off-camera stagehand, standing on a platform, held the wire suspending Dullea above the camera positioned at the bottom of the vertically oriented airlock. At the proper moment, the stage-hand first loosened his grip on the wire, causing Dullea to fall toward the camera, then, while holding the wire firmly, jumped off the platform, causing Dullea to ascend back toward the hatch.[93]", 'The methods used were alleged to have placed stuntman Bill Weston\'s life in danger. Weston recalled that he filmed one sequence without air-holes in his suit, risking asphyxiation. "Even when the tank was feeding air into the suit, there was no place for the carbon dioxide Weston exhaled to go. So it simply built up inside, incrementally causing a heightened heart rate, rapid breathing, fatigue, clumsiness, and eventually, unconsciousness."[94] Weston said Kubrick was warned "we\'ve got to get him back" but reportedly replied, "Damn it, we just started. Leave him up there! Leave him up there!"[95] When Weston lost consciousness, filming ceased, and he was brought down. "They brought the tower in, and I went looking for Stanley,\xa0... I was going to shove MGM right up his\xa0... And the thing is, Stanley had left the studio and sent Victor [Lyndon, the associate producer] to talk to me." Weston claimed Kubrick fled the studio for "two or three days.\xa0... I know he didn\'t come in the next day, and I\'m sure it wasn\'t the day after. Because I was going to do him."[96]', 'The coloured lights in the Star Gate sequence were accomplished by slit-scan photography of thousands of high-contrast images on film, including Op art paintings, architectural drawings, Moiré patterns, printed circuits, and electron-microscope photographs of molecular and crystal structures. Known to staff as "Manhattan Project", the shots of various nebula-like phenomena, including the expanding star field, were coloured paints and chemicals swirling in a pool-like device known as a cloud tank, shot in slow motion in a dark room.[97] The live-action landscape shots were filmed in the Hebridean islands, the mountains of northern Scotland, and Monument Valley. The colouring and negative-image effects were achieved with different colour filters in the process of making duplicate negatives in an optical printer.[98]', '"Not one foot of this film was made with computer-generated special effects. Everything you see in this film or saw in this film was done physically or chemically, one way or the other."', "2001 contains a famous example of a match cut, a type of cut in which two shots are matched by action or subject matter.[100][101] After Moonwatcher uses a bone to kill another ape at the watering hole, he throws it triumphantly into the air; as the bone spins in the air, the film cuts to an orbiting satellite, marking the end of the prologue.[102] The match cut draws a connection between the two objects as exemplars of primitive and advanced tools respectively, and demonstrates humanity's technological progress since the time of early hominids.[103]", '2001 pioneered the use of front projection with retroreflective matting. Kubrick used the technique to produce the backdrops in the Africa scenes and the scene when astronauts walk on the Moon.[104][56]', 'The technique consisted of a separate scenery projector set at a right angle to the camera and a half-silvered mirror placed at an angle in front that reflected the projected image forward in line with the camera lens onto a backdrop of retroreflective material. The reflective directional screen behind the actors could reflect light from the projected image 100 times more efficiently than the foreground subject did. The lighting of the foreground subject had to be balanced with the image from the screen, so that the part of the scenery image that fell on the foreground subject was too faint to show on the finished film. The exception was the eyes of the leopard in the "Dawn of Man" sequence, which glowed due to the projector illumination. Kubrick described this as "a happy accident".[105]', 'Front projection had been used in smaller settings before 2001, mostly for still photography or television production, using small still images and projectors. The expansive backdrops for the African scenes required a screen 40 feet (12\xa0m) tall and 110 feet (34\xa0m) wide, far larger than had been used before. When the reflective material was applied to the backdrop in 100-foot (30\xa0m) strips, variations at the seams of the strips led to visual artefacts; to solve this, the crew tore the material into smaller chunks and applied them in a random "camouflage" pattern on the backdrop. The existing projectors using 4-×-5-inch (10\xa0×\xa013\xa0cm) transparencies resulted in grainy images when projected that large, so the crew worked with MGM\'s special-effects supervisor Tom Howard to build a custom projector using 8-×-10-inch (20\xa0×\xa025\xa0cm) transparencies, which required the largest water-cooled arc lamp available.[105] The technique was used widely in the film industry thereafter until it was replaced by blue/green screen systems in the 1990s.[105]', "The initial MGM soundtrack album release contained none of the material from the altered and uncredited rendition of Ligeti's Aventures used in the film, used a different recording of Also sprach Zarathustra (performed by the Berlin Philharmonic conducted by Karl Böhm) from that heard in the film, and a longer excerpt of Lux Aeterna than that in the film.[106]", 'In 1996, Turner Entertainment/Rhino Records released a new soundtrack on CD that included the film\'s rendition of "Aventures", the version of "Zarathustra" used in the film, and the shorter version of Lux Aeterna from the film. As additional "bonus tracks" at the end, the CD includes the versions of "Zarathustra" and Lux Aeterna on the old MGM soundtrack album, an unaltered performance of "Aventures", and a nine-minute compilation of all of HAL\'s dialogue.[106]', "Alex North's unused music was first released in Telarc's issue of the main theme on Hollywood's Greatest Hits, Vol. 2, a compilation album by Erich Kunzel and the Cincinnati Pops Orchestra. All of the music North originally wrote was recorded commercially by his friend and colleague Jerry Goldsmith with the National Philharmonic Orchestra and released on Varèse Sarabande CDs shortly after Telarc's first theme release and before North's death. Eventually, a mono mix-down of North's original recordings was released as a limited-edition CD by Intrada Records.[107]", "The film's world premiere was on 2 April 1968, at the Uptown Theater in Washington, D.C. with a 160-minute cut.[108] It opened the next day at the Loew's Capitol in New York and the following day at the Warner Hollywood Theatre in Los Angeles.[108] The original version was also shown in Boston. ", 'Kubrick and editor Ray Lovejoy edited the film between 5 April and\xa09, 1968. Kubrick\'s rationale for trimming the film was to tighten the narrative. Reviews suggested the film suffered from its departure from traditional cinematic storytelling.[109] Kubrick said, "I didn\'t believe that the trims made a critical difference.\xa0... The people who like it like it no matter what its length, and the same holds true for the people who hate it."[65] The cut footage is reported as being 19[110][111] or 17[112] minutes long. It includes scenes revealing details about life on Discovery: additional space walks, Bowman retrieving a spare part from an octagonal corridor, elements from the Poole murder sequence—including space-walk preparation and HAL turning off radio contact with Poole—and a close-up of Bowman picking up a slipper during his walk in the alien room.[65] Jerome Agel describes the cut scenes as comprising "Dawn of Man, Orion, Poole exercising in the centrifuge, and Poole\'s pod exiting from Discovery."[113] The new cut was approximately 143 minutes long,[1] around 88 minutes for the first section, followed by an intermission, and 55 minutes in the second section.[114] Detailed instructions were sent to theatre owners already showing the film so that they could make the specified trims themselves.[citation needed] Some of the cuts may have been poorly done in a particular theatre, possibly causing the version seen by viewers early in the film\'s run to vary from theatre to theatre.', 'According to his brother-in-law, Jan Harlan, Kubrick was adamant that the trims were never to be seen and had the negatives, which he had kept in his garage, burned shortly before his death. This was confirmed by former Kubrick assistant Leon Vitali: "I\'ll tell you right now, okay, on Clockwork Orange, The Shining, Barry Lyndon, some little parts of 2001, we had thousands of cans of negative outtakes and print, which we had stored in an area at his house where we worked out of, which he personally supervised the loading of it to a truck and then I went down to a big industrial waste lot and burned it. That\'s what he wanted."[115] However, in December 2010, Douglas Trumbull, the film\'s visual effects supervisor, announced that Warner Bros. had found 17 minutes of lost footage from the post-premiere cuts, "perfectly preserved", in a Kansas salt mine vault used by Warners for storage.[116][113][112] No plans have been announced for the rediscovered footage.[117]', 'The revised version was ready for the expansion of the roadshow release to four other U.S. cities (Chicago, Denver, Detroit and Houston), on 10 April 1968, and internationally in five cities the following day,[113][118] where the shortened version was shown in 70mm format in the 2.21:1 aspect ratio and used a six-track stereo magnetic soundtrack.[113]', 'By the end of May, the film had opened in 22 cities in the United States and Canada and in another 36 in June.[119] The general release of the film in its 35\xa0mm anamorphic format took place in autumn 1968 and used either a four-track magnetic stereo soundtrack or an optical monaural one.[120]', 'The original 70-millimetre release, like many Super Panavision 70 films of the era such as Grand Prix, was advertised as being in "Cinerama" in cinemas equipped with special projection optics and a deeply curved screen. In standard cinemas, the film was identified as a 70-millimetre production. The original release of 2001: A Space Odyssey in 70-millimetre Cinerama with six-track sound played continually for more than a year in several venues, and for 103 weeks in Los Angeles.[120]', 'As was typical of most films of the era released both as a "roadshow" (in Cinerama format in the case of 2001) and general release (in 70-millimetre in the case of 2001), the entrance music, intermission music (and intermission altogether), and postcredit exit music were cut from most prints of the latter version, although these have been restored to most DVD releases.[121][122]', "In its first nine weeks from 22 locations, it grossed $2\xa0million in the United States and Canada.[119] The film earned $8.5\xa0million in theatrical gross rentals from roadshow engagements throughout 1968,[123][124] contributing to North American rentals of $16.4\xa0million and worldwide rentals of $21.9\xa0million during its original release.[125] The film's high costs, in excess of $10\xa0million, meant that the initial returns from the 1968 release left it $800,000 in the red; but the successful re-release in 1971 made it profitable.[126][127][128] By June 1974, the film had rentals from the United States and Canada of $20.3\xa0million (gross of $58\xa0million)[126] and international rentals of $7.5\xa0million.[114] The film had a reissue on a test basis on 24 July 1974 at the Cinerama Dome in Los Angeles and grossed $53,000 in its first week, which led to an expanded reissue.[114] Further re-releases followed, giving a cumulative gross of over $60\xa0million in the United States and Canada.[129] Taking its re-releases into account, it is the highest-grossing film of 1968 in the United States and Canada.[130] Worldwide, it has grossed $146\xa0million across all releases,[e] although some estimates place the gross higher, at over $190\xa0million.[132]", 'Upon release, 2001 polarised critical opinion, receiving both praise and derision, with many New York-based critics being especially harsh. Kubrick called them "dogmatically atheistic and materialistic and earthbound".[133] Some critics viewed the original 161-minute cut shown at premieres in Washington D.C., New York, and Los Angeles.[134] Keir Dullea says that during the New York premiere, 250 people walked out; in L.A., Rock Hudson not only left early but "was heard to mutter, \'What is this bullshit?\'"[133] "But a few months into the release, they realised a lot of people were watching it while smoking funny cigarettes. Someone in San Francisco even ran right through the screen screaming: \'It\'s God!\' So they came up with a new poster that said: \'2001 – the ultimate trip!\'"[135]', 'In The New Yorker, Penelope Gilliatt said it was "some kind of great film, and an unforgettable endeavor\xa0... The film is hypnotically entertaining, and it is funny without once being gaggy, but it is also rather harrowing."[136] Charles Champlin of the Los Angeles Times wrote that it was "the picture that science fiction fans of every age and in every corner of the world have prayed (sometimes forlornly) that the industry might some day give them. It is an ultimate statement of the science fiction film, an awesome realization of the spatial future\xa0... it is a milestone, a landmark for a spacemark, in the art of film."[137] Louise Sweeney of The Christian Science Monitor felt that 2001 was "a brilliant intergalactic satire on modern technology. It\'s also a dazzling 160-minute tour on the Kubrick filmship through the universe out there beyond our earth."[138] Philip French wrote that the film was "perhaps the first multi-million-dollar supercolossal movie since D.W. Griffith\'s Intolerance fifty years ago which can be regarded as the work of one man\xa0... Space Odyssey is important as the high-water mark of science-fiction movie making, or at least of the genre\'s futuristic branch."[139] The Boston Globe\'s review called it "the world\'s most extraordinary film. Nothing like it has ever been shown in Boston before or, for that matter, anywhere\xa0... The film is as exciting as the discovery of a new dimension in life."[140] Roger Ebert gave the film four stars in his original review, saying the film "succeeds magnificently on a cosmic scale."[49] He later put it on his Top 10 list for Sight & Sound.[141] Time provided at least seven different mini-reviews of the film in various issues in 1968, each one slightly more positive than the preceding one; in the final review dated 27 December 1968, the magazine called 2001 "an epic film about the history and future of mankind, brilliantly directed by Stanley Kubrick. The special effects are mindblowing."[142]', 'Others were unimpressed. Pauline Kael called it "a monumentally unimaginative movie."[143] Stanley Kauffmann of The New Republic described it as "a film that is so dull, it even dulls our interest in the technical ingenuity for the sake of which Kubrick has allowed it to become dull."[144] The Soviet film director Andrei Tarkovsky found the film to be an inadequate addition to the science fiction genre of filmmaking.[29] Renata Adler of The New York Times wrote that it was "somewhere between hypnotic and immensely boring."[145] Variety\'s Robert B. Frederick (\'Robe\') believed the film was a "[b]ig, beautiful, but plodding sci-fi epic\xa0... A major achievement in cinematography and special effects, 2001 lacks dramatic appeal to a large degree and only conveys suspense after the halfway mark."[109] Andrew Sarris called it "one of the grimmest films I have ever seen in my life\xa0... 2001 is a disaster because it is much too abstract to make its abstract points."[146] (Sarris reversed his opinion upon a second viewing, and declared, "2001 is indeed a major work by a major artist."[147]) John Simon felt it was "a regrettable failure, although not a total one. This film is fascinating when it concentrates on apes or machines\xa0... and dreadful when it deals with the in-betweens: humans\xa0... 2001, for all its lively visual and mechanical spectacle, is a kind of space-Spartacus and, more pretentious still, a shaggy God story."[148] Historian Arthur M. Schlesinger, Jr. deemed the film "morally pretentious, intellectually obscure and inordinately long\xa0... a film out of control".[149] In a 2001 review, the BBC said that its slow pacing often alienates modern audiences more than it did upon its initial release.[150]', '2001: A Space Odyssey is now considered one of the major artistic works of the 20th century, with many critics and filmmakers considering it Kubrick\'s masterpiece.[151] Director Martin Scorsese has listed it as one of his favourite films of all time.[152] In the 1980s,[153] critic David Denby compared Kubrick to the monolith from 2001: A Space Odyssey, calling him "a force of supernatural intelligence, appearing at great intervals amid high-pitched shrieks, who gives the world a violent kick up the next rung of the evolutionary ladder".[154] By the start of the 21st century, 2001: A Space Odyssey had become recognised as among the best films ever made by such sources as the British Film Institute (BFI). The Village Voice ranked the film at number 11 in its Top 250 "Best Films of the Century" list in 1999, based on a poll of critics.[155] In January 2002, the film was voted no. 1 on the list of the "Top 100 Essential Films of All Time" by the National Society of Film Critics.[156][157] Sight & Sound magazine ranked the film 12th in its greatest films of all-time list in 1982,[158] tenth in 1992 critics poll of greatest films,[159] sixth in the top ten films of all time in its 2002[160] and 2012 critics\' polls.[161] editions; it also tied for second place in the magazine\'s 2012 directors\' poll.[161] The film was voted no. 43 on the list of "100 Greatest Films" by the prominent French magazine Cahiers du cinéma in 2008.[162] In 2010, The Guardian named it "the best sci-fi and fantasy film of all time".[163] The film ranked 4th in BBC\'s 2015 list of the 100 greatest American films.[164]', 'On review aggregation website Rotten Tomatoes, the film has a "Certified Fresh" rating of 92% based on 115 reviews, with an average rating of 9.30/10. The site\'s critical consensus reads: "One of the most influential of all sci-fi films – and one of the most controversial – Stanley Kubrick\'s 2001 is a delicate, poetic meditation on the ingenuity – and folly – of mankind."[165] Review aggregation website Metacritic, which uses a weighted average, has assigned the film a score of 84 out of 100, based on 25 critic reviews, indicating "universal acclaim".[166]', 'The film won the Hugo Award for best dramatic presentation, as voted by science fiction fans and published science-fiction writers.[167] Ray Bradbury praised the film\'s photography, but disliked the banality of most of the dialogue, and believed that the audience does not care when Poole dies.[168] Both he and Lester del Rey disliked the film\'s feeling of sterility and blandness in the human encounters amidst the technological wonders, while both praised the pictorial element of the film. Reporting that "half the audience had left by intermission", Del Rey described the film as dull, confusing, and boring ("the first of the New Wave-Thing movies, with the usual empty symbols"), predicting "[i]t will probably be a box-office disaster, too, and thus set major science-fiction movie making back another ten years".[169] Samuel R. Delany was impressed by how the film undercuts the audience\'s normal sense of space and orientation in several ways. Like Bradbury, Delany noticed the banality of the dialogue (he stated that characters say nothing meaningful), but regarded this as a dramatic strength, a prelude to the rebirth at the conclusion of the film.[170] Without analysing the film in detail, Isaac Asimov spoke well of it in his autobiography and other essays. James P. Hogan liked the film but complained that the ending did not make any sense to him, leading to a bet about whether he could write something better: "I stole Arthur\'s plot idea shamelessly and produced Inherit the Stars."[171]', 'In 1969, a United States Department of State committee chose 2001 as the American entry at the 6th Moscow International Film Festival.[180]', '2001 was ranked 15th on the American Film Institute\'s 2007 100 Years\xa0... 100 Movies[181] (22 in 1998),[182] was no. 40 on its 100 Years, 100 Thrills,[183] was included on its 100 Years, 100 Quotes (no. 78 "Open the pod bay doors, HAL."),[184] and HAL 9000 was the no. 13 villain in 100 Years\xa0... 100 Heroes and Villains.[185] The film was also no. 47 on AFI\'s 100 Years ... 100 Cheers[186] and the no.\xa01 science fiction film on AFI\'s 10 Top 10.[187] 2001 was the only science fiction film to make Sight & Sound\'s 2012 list of the ten best films,[188] and tops the Online Film Critics Society list of greatest science fiction films of all time.[189] In 2012, the Motion Picture Editors Guild listed the film as the 19th best-edited film of all time based on a survey of its membership.[190] Other lists that include the film are 50 Films to See Before You Die (#6), The Village Voice 100 Best Films of the 20th century (#11), the Sight & Sound 2002[160] and 2012 Top Ten poll (#6), and Roger Ebert\'s Top Ten (1968) (#2). In 1995, the Vatican named it one of the 45 best films ever made (and included it in a sub-list of the "Top Ten Art Movies" of all time.)[191] In 1998, Time Out conducted a reader\'s poll and 2001: A Space Odyssey was voted the "greatest film of all time".[192] Entertainment Weekly voted it no. 26 on their list of 100 Greatest Movies of All Time.[193] In 2017, Empire magazine\'s readers\' poll ranked the film  21st on its list of "The 100 Greatest Movies".[194] In the Sight & Sound poll of 480 directors published in December 2022, 2001: A Space Odyssey was voted as the Greatest Film of All Time, ahead of Citizen Kane and The Godfather.[195]', "Since its premiere, 2001: A Space Odyssey has been analysed and interpreted by professional critics and theorists, amateur writers, and science fiction fans. In his monograph for BFI analysing the film, Peter Krämer summarised the diverse interpretations as ranging from those who saw it as darkly apocalyptic in tone to those who saw it as an optimistic reappraisal of the hopes of mankind and humanity.[196] Questions about 2001 range from uncertainty about its implications for humanity's origins and destiny in the universe[197] to interpreting elements of the film's more enigmatic scenes, such as the meaning of the monolith, or the fate of astronaut David Bowman. There are also simpler and more mundane questions about the plot, in particular the causes of HAL's breakdown (explained in earlier drafts but kept mysterious in the film).[198][43][199][200]", 'A spectrum of diverse interpretative opinions would form after the film\'s release, appearing to divide theatre audiences from the opinions of critics. Krämer writes: "Many people sent letters to Kubrick to tell him about their responses to 2001, most of them regarding the film—in particular the ending—as an optimistic statement about humanity, which is seen to be born and reborn. The film\'s reviewers and academic critics, by contrast, have tended to understand the film as a pessimistic account of human nature and humanity\'s future. The most extreme of these interpretations state that the foetus floating above the Earth will destroy it."[201]', 'Some of the critics\' cataclysmic interpretations were informed by Kubrick\'s prior direction of the Cold War film Dr. Strangelove, immediately before 2001, which resulted in dark speculation about the nuclear weapons orbiting the Earth in 2001. These interpretations were challenged by Clarke, who said: "Many readers have interpreted the last paragraph of the book to mean that he (the foetus) destroyed Earth, perhaps for the purpose of creating a new Heaven. This idea never occurred to me; it seems clear that he triggered the orbiting nuclear bombs harmlessly\xa0...".[196] In response to Jeremy Bernstein\'s dark interpretation of the film\'s ending, Kubrick said: "The book does not end with the destruction of the Earth."[196]', 'Regarding the film as a whole, Kubrick encouraged people to make their own interpretations and refused to offer an explanation of "what really happened". In a 1968 interview with Playboy magazine, he said:', "You're free to speculate as you wish about the philosophical and allegorical meaning of the film—and such speculation is one indication that it has succeeded in gripping the audience at a deep level—but I don't want to spell out a verbal road map for 2001 that every viewer will feel obligated to pursue or else fear he's missed the point.[42]", 'In a subsequent discussion of the film with Joseph Gelmis, Kubrick said his main aim was to avoid "intellectual verbalization" and reach "the viewer\'s subconscious." But he said he did not strive for ambiguity—it was simply an inevitable outcome of making the film nonverbal. Still, he acknowledged this ambiguity was an invaluable asset to the film. He was willing then to give a fairly straightforward explanation of the plot on what he called the "simplest level," but unwilling to discuss the film\'s metaphysical interpretation, which he felt should be left up to viewers.[202]', "For some readers, Clarke's more straightforward novel based on the script is key to interpreting the film. The novel explicitly identifies the monolith as a tool created by an alien race that has been through many stages of evolution, moving from organic form to biomechanical, and finally achieving a state of pure energy. These aliens travel the cosmos assisting lesser species to take evolutionary steps. Conversely, film critic Penelope Houston wrote in 1971 that because the novel differs in many key aspects from the film, it perhaps should not be regarded as the skeleton key to unlock it.[203]", 'Carolyn Geduld writes that what "structurally unites all four episodes of the film" is the monolith, the film\'s largest and most unresolvable enigma.[204] Vincent LoBrutto\'s biography of Kubrick says that for many, Clarke\'s novel supplements the understanding of the monolith which is more ambiguously depicted in the film.[205] Similarly, Geduld observes that "the monolith\xa0... has a very simple explanation in Clarke\'s novel", though she later asserts that even the novel does not fully explain the ending.[204]', 'Bob McClay\'s Rolling Stone review describes a parallelism between the monolith\'s first appearance in which tool usage is imparted to the apes (thus \'beginning\' mankind) and the completion of "another evolution" in the fourth and final encounter[206] with the monolith. In a similar vein, Tim Dirks ends his synopsis saying "[t]he cyclical evolution from ape to man to spaceman to angel-starchild-superman is complete."[207]', 'Humanity\'s first and second encounters with the monolith have visual elements in common; both the apes, and later the astronauts, touch it gingerly with their hands, and both sequences conclude with near-identical images of the Sun appearing directly over it (the first with a crescent moon adjacent to it in the sky, the second with a near-identical crescent Earth in the same position), echoing the Sun–Earth–Moon alignment seen at the very beginning of the film.[208] The second encounter also suggests the triggering of the monolith\'s radio signal to Jupiter by the presence of humans, echoing the premise of Clarke\'s source story "The Sentinel".[209]', 'The monolith is the subject of the film\'s final line of dialogue (spoken at the end of the "Jupiter Mission" segment): "Its origin and purpose still a total mystery." Reviewers McClay and Roger Ebert wrote that the monolith is the main element of mystery in the film; Ebert described "the shock of the monolith\'s straight edges and square corners among the weathered rocks," and the apes warily circling it as prefiguring man reaching "for the stars."[49] Patrick Webster suggests the final line relates to how the film should be approached as a whole: "The line appends not merely to the discovery of the monolith on the Moon, but to our understanding of the film in the light of the ultimate questions it raises about the mystery of the universe."[210]', 'Clarke indicated his preferred reading of the ending of 2001 as oriented toward the creation of "a new heaven" provided by the Star Child.[196] His view was corroborated in a posthumously released interview with Kubrick.[43] Kubrick says that Bowman is elevated to a higher level of being that represents the next stage of human evolution. The film also conveys what some viewers have described as a sense of the sublime and numinous.[49] Ebert writes in his essay on 2001 in The Great Movies:', "North's [rejected] score, which is available on a recording, is a good job of film composition, but would have been wrong for 2001 because, like all scores, it attempts to underline the action—to give us emotional cues. The classical music chosen by Kubrick exists outside the action. It uplifts. It wants to be sublime; it brings a seriousness and transcendence to the visuals.[49]", 'In a book on architecture, Gregory Caicco writes that Space Odyssey illustrates how our quest for space is motivated by two contradictory desires, a "desire for the sublime" characterised by a need to encounter something totally other than ourselves—"something numinous"—and the conflicting desire for a beauty that makes us feel no longer "lost in space," but at home.[211] Similarly, an article in The Greenwood Encyclopedia of Science Fiction and Fantasy, titled "Sense of Wonder," describes how 2001 creates a "numinous sense of wonder" by portraying a universe that inspires a sense of awe but that at the same time we feel we can understand.[212] Christopher Palmer wrote that "the sublime and the banal" coexist in the film, as it implies that to get into space, people had to suspend the "sense of wonder" that motivated them to explore it.[213]', 'The reasons for HAL\'s malfunction and subsequent malignant behaviour have elicited much discussion. He has been compared to Frankenstein\'s monster. In Clarke\'s novel, HAL malfunctions because of being ordered to lie to the crew of Discovery and withhold confidential information from them, namely the confidentially programmed mission priority over expendable human life, despite being constructed for "the accurate processing of information without distortion or concealment". This would not be addressed on film until the 1984 follow-up, 2010: The Year We Make Contact. Film critic Roger Ebert wrote that HAL, as the supposedly perfect computer, is actually the most human of the characters.[49] In an interview with Joseph Gelmis in 1969, Kubrick said that HAL "had an acute emotional crisis because he could not accept evidence of his own fallibility".[214]', 'Multiple allegorical interpretations of 2001 have been proposed. The symbolism of life and death can be seen through the final moments of the film, which are defined by the image of the "Star Child," an in utero foetus that draws on the work of Lennart Nilsson.[215] The Star Child signifies a "great new beginning,"[215] and is depicted naked and ungirded but with its eyes wide open.[216] Leonard F. Wheat sees 2001 as a multi-layered allegory, commenting simultaneously on Nietzsche, Homer, and the relationship of man to machine.[217] Rolling Stone reviewer Bob McClay sees the film as like a four-movement symphony, its story told with "deliberate realism".[218]', 'Kubrick originally planned a voice-over to reveal that the satellites seen after the prologue are nuclear weapons,[219] and that the Star Child would detonate the weapons at the end of the film[220] but felt this would create associations with Dr. Strangelove and decided not to make it obvious that they were "war machines". A few weeks before the film\'s release, the U.S. and Soviet governments had agreed not to put any nuclear weapons into outer space.[221]', 'In a book he wrote with Kubrick\'s assistance, Alexander Walker states that Kubrick eventually decided that nuclear weapons had "no place at all in the film\'s thematic development", being an "orbiting red herring" that would "merely have raised irrelevant questions to suggest this as a reality of the twenty-first century".[219]', 'Kubrick scholar Michel Ciment, discussing Kubrick\'s attitude toward human aggression and instinct, observes: "The bone cast into the air by the ape (now become a man) is transformed at the other extreme of civilization, by one of those abrupt ellipses characteristic of the director, into a spacecraft on its way to the moon."[222] In contrast to Ciment\'s reading of a cut to a serene "other extreme of civilization", science fiction novelist Robert Sawyer, in the Canadian documentary 2001 and Beyond, says he sees it as a cut from a bone to a nuclear weapons platform, explaining that "what we see is not how far we\'ve leaped ahead, what we see is that today, \'2001\', and four million years ago on the African veldt, it\'s exactly the same—the power of mankind is the power of its weapons. It\'s a continuation, not a discontinuity in that jump."[223]', 'The film has been released in several forms:', "The film was re-released in 1974, 1977, 1980[123] and 1993.[227] In 2001, a restoration of the 70\xa0mm version was screened at the Ebert's Overlooked Film Festival, and the production was also reissued to selected film houses in North America, Europe and Asia.[228][229]", "For the film's 50th anniversary, Warner Bros. struck new 70mm prints from printing elements made directly from the original film negative.[230] This was done under the supervision of film director Christopher Nolan, who has spoken of 2001's influence on his career. Following a showing at the 2018 Cannes Film Festival introduced by Nolan, the film had a limited worldwide release at select 70mm-equipped theatres in the summer of 2018,[231][232] followed by a one-week run in North American IMAX theatres (including five locations equipped with 70\xa0mm IMAX projectors).[233]", 'On 3 December 2018, an 8K Ultra-high definition television version of the film was reported to have been broadcast in select theatres and shopping-mall demonstration stations in Japan.[234]', "Stanley Kubrick made the ultimate science fiction movie, and it is going to be very hard for someone to come along and make a better movie, as far as I'm concerned. On a technical level, it [Star Wars] can be compared, but personally I think that 2001 is far superior.", '—George Lucas, 1977[120]', '2001: A Space Odyssey is widely regarded as among the greatest and most influential films ever made.[235] In 1991, it was deemed "culturally, historically, or aesthetically significant" by the United States Library of Congress and selected for preservation in the National Film Registry.[236] In 2010, it was named the greatest film of all time by The Moving Arts Film Journal.[237]', 'The influence of 2001 on subsequent filmmakers is considerable. Steven Spielberg, George Lucas, and others—including many special effects technicians—discuss the impact the film has had on them in a featurette titled Standing on the Shoulders of Kubrick: The Legacy of 2001, included in the 2007 DVD release of the film. Spielberg calls it his film generation\'s "big bang", while Lucas says it was "hugely inspirational", calling Kubrick "the filmmaker\'s filmmaker". Sydney Pollack calls it "groundbreaking", and William Friedkin says 2001 is "the grandfather of all such films". At the 2007 Venice film festival, director Ridley Scott said he believed 2001 was the unbeatable film that in a sense killed the science fiction genre.[238] Similarly, film critic Michel Ciment in his essay "Odyssey of Stanley Kubrick" wrote, "Kubrick has conceived a film which in one stroke has made the whole science fiction cinema obsolete."[239]', 'Others credit 2001 with opening up a market for films such as Close Encounters of the Third Kind, Alien, Blade Runner, Contact, and Interstellar, proving that big-budget "serious" science-fiction films can be commercially successful, and establishing the "sci-fi blockbuster" as a Hollywood staple.[240] Science magazine Discover\'s blogger Stephen Cass, discussing the film\'s considerable impact on subsequent science fiction, writes that "the balletic spacecraft scenes set to sweeping classical music, the tarantula-soft tones of HAL 9000, and the ultimate alien artifact, the monolith, have all become enduring cultural icons in their own right".[241] Trumbull said that when working on Star Trek: The Motion Picture he made a scene without dialogue because of "something I really learned with Kubrick and 2001: Stop talking for a while, and let it all flow".[242]', 'Kubrick did not envision a sequel to 2001. Fearing the later exploitation and recycling of his material in other productions (as was done with the props from MGM\'s Forbidden Planet), he ordered all sets, props, miniatures, production blueprints, and prints of unused scenes destroyed.[citation needed] Most of these materials were lost, with some exceptions: a 2001 spacesuit backpack appeared in the "Close Up" episode of the Gerry Anderson series UFO,[221][243][244][245] and one of HAL\'s eyepieces is in the possession of the author of Hal\'s Legacy, David G. Stork. In 2012, Lockheed engineer Adam Johnson, working with Frederick I. Ordway III, science adviser to Kubrick, wrote the book 2001: The Lost Science, which for the first time featured many of the blueprints of the spacecraft and film sets that previously had been thought destroyed. Clarke wrote three sequel novels: 2010: Odyssey Two (1982), 2061: Odyssey Three (1987), and 3001: The Final Odyssey (1997). The only filmed sequel, 2010: The Year We Make Contact, released in 1984, was based on Clarke\'s 1982 novel. Kubrick was not involved; it was directed as a spin-off by Peter Hyams in a more conventional style. The other two novels have not been adapted for the screen, although actor Tom Hanks in June 1999 expressed a passing interest in possible adaptations.[246]', 'To commemorate the 50th anniversary of the film\'s release, an exhibit called "The Barmecide Feast" opened on 8 April 2018, in the Smithsonian Institution\'s National Air and Space Museum. The exhibit features a fully realised, full-scale reflection of the neo-classical hotel room from the film\'s penultimate scene.[247][248] Director Christopher Nolan presented a mastered 70\xa0mm print of 2001 for the film\'s 50th anniversary at the 2018 Cannes Film Festival on 12 May.[231][249] The new 70\xa0mm print is a photochemical recreation made from the original camera negative, for the first time since the film\'s original theatrical run.[230][250] Further, an exhibit entitled "Envisioning 2001: Stanley Kubrick\'s Space Odyssey" presented at the Museum of the Moving Image in Astoria, Queens, New York City opened in January 2020.[251]', 'In July 2020, a silver space suit was sold at auction in Los Angeles for $370,000, exceeding its estimate of $200,000–300,000.  Four layers of paint indicate it was used in multiple scenes, including the Clavius Moon base sequence. The helmet had been painted green at one stage, leading to a belief that it may have been worn during the scene where Dave Bowman disconnects HAL 9000.[252]', 'Informational notes', 'Citations', 'Bibliography', 'Further reading']}, {'headings': ['Stanley Kubrick'], 'subheadings': ['Contents', 'Early life', 'Photographic career', 'Film career', 'Career influences', 'Directing techniques', 'Personal life', 'Legacy', 'Accolades', 'See also', 'Notes', 'References', 'Sources', 'External links', 'Navigation menu'], 'paras': ['', 'Stanley Kubrick (/ˈkuːbrɪk/; July 26, 1928 – March 7, 1999) was an American film director, producer, screenwriter, and photographer. Widely considered one of the greatest filmmakers of all time, his films, almost all of which are adaptations of novels or short stories, cover a wide range of genres and are noted for their innovative cinematography, dark humor, realistic attention to detail and extensive set designs.', 'Kubrick was raised in the Bronx, New York City, and attended William Howard Taft High School from 1941 to 1945. He received average grades but displayed a keen interest in literature, photography, and film from a young age, and taught himself all aspects of film production and directing after graduating from high school. After working as a photographer for Look magazine in the late 1940s and early 1950s, he began making short films on shoestring budgets, and made his first major Hollywood film, The Killing, for United Artists in 1956. This was followed by two collaborations with Kirk Douglas: the war picture Paths of Glory (1957) and the historical epic Spartacus (1960).', 'Creative differences arising from his work with Douglas and the film studios, a dislike of the Hollywood industry, and a growing concern about crime in America prompted Kubrick to move to the United Kingdom in 1961, where he spent most of his remaining life and career. His home at Childwickbury Manor in Hertfordshire, which he shared with his wife Christiane, became his workplace, where he did his writing, research, editing, and management of production details. This allowed him to have almost complete artistic control over his films, but with the rare advantage of having financial support from major Hollywood studios. His first productions in Britain were two films with Peter Sellers: Lolita (1962), an adaptation of the Vladimir Nabokov novel, and the Cold War black comedy Dr. Strangelove (1964).', 'A demanding perfectionist, Kubrick assumed control over most aspects of the filmmaking process, from direction and writing to editing, and took painstaking care with researching his films and staging scenes, working in close coordination with his actors, crew, and other collaborators. He often asked for several dozen retakes of the same shot in a movie, which resulted in many conflicts with his casts. Despite the resulting notoriety among actors, many of Kubrick\'s films broke new ground in cinematography. The scientific realism and innovative special effects of the science fiction epic 2001: A Space Odyssey (1968) were without precedent in the history of cinema, and the film earned him his only personal Oscar, for Best Visual Effects. Steven Spielberg has referred to the film as his generation\'s "big bang"; it is regarded as one of the greatest films ever made.', "While many of Kubrick's films were controversial and initially received mixed reviews upon release—particularly the brutal A Clockwork Orange (1971), which Kubrick pulled from circulation in the UK following a mass media frenzy—most were nominated for Oscars, Golden Globes, or BAFTA Awards, and underwent critical reevaluations. For the 18th-century period film Barry Lyndon (1975), Kubrick obtained lenses developed by Zeiss for NASA, to film scenes under natural candlelight. With the horror film The Shining (1980), he became one of the first directors to make use of a Steadicam for stabilized and fluid tracking shots, a technology vital to his Vietnam War film Full Metal Jacket (1987). His last film, Eyes Wide Shut, was completed shortly before his death in 1999 at the age of 70.", "Kubrick was born on July 26, 1928, in the Lying-In Hospital in Manhattan, New York City, to a Jewish family.[1][2] He was the first of two children of Jacob Leonard Kubrick (May 21, 1902\xa0– October 19, 1985), known as Jack or Jacques, and his wife Sadie Gertrude Kubrick (née  Perveler; October 28, 1903\xa0– April 23, 1985), known as Gert. His sister Barbara Mary Kubrick was born in May 1934.[3] Jack Kubrick, whose parents and paternal grandparents were of Polish-Jewish and Romanian-Jewish origin,[1] was a homeopathic doctor,[4] graduating from the New York Homeopathic Medical College in 1927, the same year he married Kubrick's mother, the child of Austrian-Jewish immigrants.[5] Kubrick's great-grandfather, Hersh Kubrick, arrived at Ellis Island via Liverpool by ship on December 27, 1899, at the age of 47, leaving behind his wife and two grown children, one of whom was Stanley's grandfather Elias, to start a new life with a younger woman.[6] Elias Kubrick followed in 1902.[7] At Stanley's birth the Kubricks lived in the Bronx.[8] His parents married in a Jewish ceremony, but Kubrick did not have a religious upbringing and later professed an atheistic view of the universe.[9] His father was a physician and, by the standards of the West Bronx, the family was fairly wealthy.[10]", 'Soon after his sister\'s birth, Kubrick began schooling in Public School 3 in the Bronx and moved to Public School 90 in June 1938. His IQ was discovered to be above average but his attendance was poor.[2] He displayed an interest in literature from a young age and began reading Greek and Roman myths and the fables of the Grimm brothers, which "instilled in him a lifelong affinity with Europe".[11] He spent most Saturdays during the summer watching the New York Yankees and later photographed two boys watching the game in an assignment for Look magazine to emulate his own childhood excitement with baseball.[10] When Kubrick was 12, his father Jack taught him chess. The game remained a lifelong interest of Kubrick\'s,[12] appearing in many of his films.[13] Kubrick, who later became a member of the United States Chess Federation, explained that chess helped him develop "patience and discipline" in making decisions.[14] When Kubrick was 13, his father bought him a Graflex camera, triggering a fascination with still photography. He befriended a neighbor, Marvin Traub, who shared his passion for photography.[15] Traub had his own darkroom where he and the young Kubrick would spend many hours perusing photographs and watching the chemicals "magically make images on photographic paper".[3] The two indulged in numerous photographic projects for which they roamed the streets looking for interesting subjects to capture and spent time in local cinemas studying films. Freelance photographer Weegee (Arthur Fellig) had a considerable influence on Kubrick\'s development as a photographer; Kubrick later hired Fellig as the special stills photographer for Dr. Strangelove (1964).[16] As a teenager, Kubrick was also interested in jazz and briefly attempted a career as a drummer.[17]', "Kubrick attended William Howard Taft High School from 1941 to 1945.[18] Though he joined the school's photography club, which permitted him to photograph the school's events in their magazine,[3] he was a mediocre student, with a 67/D+ grade average.[19] Introverted and shy, Kubrick had a low attendance record and often skipped school to watch double-feature films.[20] He graduated in 1945 but his poor grades, combined with the demand for college admissions from soldiers returning from the Second World War, eliminated any hope of higher education. Later in life Kubrick spoke disdainfully of his education and of American schooling as a whole, maintaining that schools were ineffective in stimulating critical thinking and student interest. His father was disappointed in his son's failure to achieve the excellence in school of which he knew Stanley was fully capable. Jack also encouraged Stanley to read from the family library at home, while permitting Stanley to take up photography as a serious hobby.[21]", 'While in high school, Kubrick was chosen as an official school photographer. In the mid-1940s, since he was unable to gain admission to day session classes at colleges, he briefly attended evening classes at the City College of New York.[22] Eventually, he sold a photographic series to Look magazine,[23][a] which was printed on June 26, 1945. Kubrick supplemented his income by playing chess "for quarters" in Washington Square Park and various Manhattan chess clubs.[25]', 'In 1946, he became an apprentice photographer for Look and later a full-time staff photographer. G. Warren Schloat, Jr., another new photographer for the magazine at the time, recalled that he thought Kubrick lacked the personality to make it as a director in Hollywood, remarking, "Stanley was a quiet fellow. He didn\'t say much. He was thin, skinny, and kind of poor—like we all were."[26] Kubrick quickly became known for his story-telling in photographs. His first, published on April 16, 1946, was entitled "A Short Story from a Movie Balcony" and staged a fracas between a man and a woman, during which the man is slapped in the face, caught genuinely by surprise.[23] In another assignment, 18 pictures were taken of various people waiting in a dental office. It has been said retrospectively that this project demonstrated an early interest of Kubrick in capturing individuals and their feelings in mundane environments.[27] In 1948, he was sent to Portugal to document a travel piece, and covered the Ringling Bros. and Barnum & Bailey Circus in Sarasota, Florida.[28][b]', 'A boxing enthusiast, Kubrick eventually began photographing boxing matches for the magazine. His earliest, "Prizefighter", was published on January 18, 1949, and captured a boxing match and the events leading up to it, featuring Walter Cartier.[30] On April 2, 1949, he published photo essay "Chicago-City of Extremes" in Look, which displayed his talent early on for creating atmosphere with imagery. The following year, in July 1950, the magazine published his photo essay, "Working Debutante – Betsy von Furstenberg", which featured a Pablo Picasso portrait of Angel F. de Soto in the background.[31] Kubrick was also assigned to photograph numerous jazz musicians, from Frank Sinatra and Erroll Garner to George Lewis, Eddie Condon, Phil Napoleon, Papa Celestin, Alphonse Picou, Muggsy Spanier, Sharkey Bonano, and others.[32]', 'Kubrick married his high-school sweetheart Toba Metz on May 28, 1948. They lived together in a small apartment at 36 West 16th Street, off Sixth Avenue just north of Greenwich Village.[33] During this time, Kubrick began frequenting film screenings at the Museum of Modern Art and New York City cinemas. He was inspired by the complex, fluid camerawork of director Max Ophüls, whose films influenced Kubrick\'s visual style, and by the director Elia Kazan, whom he described as America\'s "best director" at that time, with his ability of "performing miracles" with his actors.[34] Friends began to notice Kubrick had become obsessed with the art of filmmaking—one friend, David Vaughan, observed that Kubrick would scrutinize the film at the cinema when it went silent, and would go back to reading his paper when people started talking.[23] He spent many hours reading books on film theory and writing notes. He was particularly inspired by Sergei Eisenstein and Arthur Rothstein, the photographic technical director of Look magazine.[35][c]', "Kubrick shared a love of film with his school friend Alexander Singer, who after graduating from high school had the intention of directing a film version of Homer's Iliad. Through Singer, who worked in the offices of the newsreel production company, The March of Time, Kubrick learned it could cost $40,000 to make a proper short film, money he could not afford. He had $1500 in savings and produced a few short documentaries fueled by encouragement from Singer. He began learning all he could about filmmaking on his own, calling film suppliers, laboratories, and equipment rental houses.[36]", 'Kubrick decided to make a short film documentary about boxer Walter Cartier, whom he had photographed and written about for Look magazine a year earlier. He rented a camera and produced a 16-minute black-and-white documentary, Day of the Fight. Kubrick found the money independently to finance it. He had considered asking Montgomery Clift to narrate it, whom he had met during a photographic session for Look, but settled on CBS news veteran Douglas Edwards.[37] According to Paul Duncan the film was "remarkably accomplished for a first film", and used a backward tracking shot to film a scene in which Cartier and his brother walk towards the camera, a device which later became one of Kubrick\'s characteristic camera movements.[38] Vincent Cartier, Walter\'s brother and manager, later reflected on his observations of Kubrick during the filming. He said, "Stanley was a very stoic, impassive but imaginative type person with strong, imaginative thoughts. He commanded respect in a quiet, shy way. Whatever he wanted, you complied, he just captivated you. Anybody who worked with Stanley did just what Stanley wanted".[36][d] After a score was added by Singer\'s friend Gerald Fried, Kubrick had spent $3900 in making it, and sold it to RKO-Pathé for $4000, which was the most the company had ever paid for a short film at the time.[38] Kubrick described his first effort at filmmaking as having been valuable since he believed himself to have been forced to do most of the work,[39] and he later declared that the "best education in film is to make one".[3]', 'Inspired by this early success, Kubrick quit his job at Look and visited professional filmmakers in New York City, asking many detailed questions about the technical aspects of filmmaking. He stated that he was given the confidence during this period to become a filmmaker because of the number of bad films he had seen, remarking, "I don\'t know a goddamn thing about movies, but I know I can make a better film than that".[40] He began making Flying Padre (1951), a film which documents Reverend Fred Stadtmueller, who travels some 4,000 miles to visit his 11 churches. The film was originally going to be called "Sky Pilot", a pun on the slang term for a priest.[41] During the course of the film, the priest performs a burial service, confronts a boy bullying a girl, and makes an emergency flight to aid a sick mother and baby into an ambulance. Several of the views from and of the plane in Flying Padre are later echoed in 2001: A Space Odyssey (1968) with the footage of the spacecraft, and a series of close-ups on the faces of people attending the funeral were most likely inspired by Sergei Eisenstein\'s Battleship Potemkin (1925) and Ivan the Terrible (1944/1958).[38]', "Flying Padre was followed by The Seafarers (1953), Kubrick's first color film, which was shot for the Seafarers International Union in June 1953. It depicted the logistics of a democratic union and focused more on the amenities of seafaring other than the act. For the cafeteria scene in the film, Kubrick chose a dolly shot to establish the life of the seafarer's community; this kind of shot would later become a signature technique. The sequence of Paul Hall, secretary-treasurer of the SIU Atlantic and gulf district, speaking to members of the union echoes scenes from Eisenstein's Strike (1925) and October (1928).[42] Day of the Fight, Flying Padre and The Seafarers constitute Kubrick's only surviving documentary works; some historians believe he made others.[43]", "After raising $1000 showing his short films to friends and family, Kubrick found the finances to begin making his first feature film, Fear and Desire (1953), originally running with the title The Trap, written by his friend Howard Sackler. Kubrick's uncle, Martin Perveler, a Los Angeles pharmacy owner, invested a further $9000 on condition that he be credited as executive producer of the film.[44] Kubrick assembled several actors and a small crew totaling 14 people (five actors, five crewmen, and four others to help transport the equipment) and flew to the San Gabriel Mountains in California for a five-week, low-budget shoot.[44]   Later renamed The Shape of Fear before finally being named Fear and Desire, it is a fictional allegory about a team of soldiers who survive a plane crash and are caught behind enemy lines in a war. During the course of the film, one of the soldiers becomes infatuated with an attractive girl in the woods and binds her to a tree. This scene is noted for its close-ups on the face of the actress. Kubrick had intended for Fear and Desire to be a silent picture in order to ensure low production costs; the added sounds, effects, and music ultimately brought production costs to around $53,000, exceeding the budget.[45] He was bailed out by producer Richard de Rochemont on the condition that he help in de Rochemont's production of a five-part television series about Abraham Lincoln on location in Hodgenville, Kentucky.[46]", 'Fear and Desire was a commercial failure, but garnered several positive reviews upon release. Critics such as the reviewer from The New York Times believed that Kubrick\'s professionalism as a photographer shone through in the picture, and that he "artistically caught glimpses of the grotesque attitudes of death, the wolfishness of hungry men, as well as their bestiality, and in one scene, the wracking effect of lust on a pitifully juvenile soldier and the pinioned girl he is guarding". Columbia University scholar Mark Van Doren was highly impressed by the scenes with the girl bound to the tree, remarking that it would live on as a "beautiful, terrifying and weird" sequence which illustrated Kubrick\'s immense talent and guaranteed his future success.[47] Kubrick himself later expressed embarrassment with Fear and Desire, and attempted over the years to keep prints of the film out of circulation.[48][e] During the production of the film, Kubrick almost killed his cast with poisonous gasses by mistake.[49]', 'Following Fear and Desire, Kubrick began working on ideas for a new boxing film. Due to the commercial failure of his first feature, Kubrick avoided asking for further investments, but commenced a film noir script with Howard O. Sackler. Originally under the title Kiss Me, Kill Me, and then The Nymph and the Maniac, Killer\'s Kiss (1955) is a 67-minute film noir about a young heavyweight boxer\'s involvement with a woman being abused by her criminal boss. Like Fear and Desire, it was privately funded by Kubrick\'s family and friends, with some $40,000 put forward from Bronx pharmacist Morris Bousse.[42] Kubrick began shooting footage in Times Square, and frequently explored during the filming process, experimenting with cinematography and considering the use of unconventional angles and imagery. He initially chose to record the sound on location, but encountered difficulties with shadows from the microphone booms, restricting camera movement. His decision to drop the sound in favor of imagery was a costly one; after 12–14 weeks shooting the picture, he spent some seven months and $35,000 working on the sound.[50]Alfred Hitchcock\'s Blackmail (1929) directly influenced the film with the painting laughing at a character, and Martin Scorsese has, in turn, cited Kubrick\'s innovative shooting angles and atmospheric shots in Killer\'s Kiss as an influence on Raging Bull (1980).[51] Actress Irene Kane, the star of Killer\'s Kiss, observed: "Stanley\'s a fascinating character. He thinks movies should move, with a minimum of dialogue, and he\'s all for sex and sadism".[52] Killer\'s Kiss met with limited commercial success and made very little money in comparison with its production budget of $75,000.[51] Critics have praised the film\'s camerawork, but its acting and story are generally considered mediocre.[53][f]', 'While playing chess in Washington Square, Kubrick met producer James B. Harris, who considered Kubrick "the most intelligent, most creative person I have ever come in contact with." The two formed the Harris-Kubrick Pictures Corporation in 1955.[56] Harris purchased the rights to Lionel White\'s novel Clean Break for $10,000[g] and Kubrick wrote the script,[58] but at Kubrick\'s suggestion, they hired film noir novelist Jim Thompson to write the dialog for the film—which became The Killing (1956)—about a meticulously planned racetrack robbery gone wrong. The film starred Sterling Hayden, who had impressed Kubrick with his performance in The Asphalt Jungle (1950).[59]', 'Kubrick and Harris moved to Los Angeles and signed with the Jaffe Agency to shoot the picture, which became Kubrick\'s first full-length feature film shot with a professional cast and crew. The Union in Hollywood stated that Kubrick would not be permitted to be both the director and the cinematographer, resulting in the hiring of veteran cinematographer Lucien Ballard. Kubrick agreed to waive his fee for the production, which was shot in 24 days on a budget of $330,000.[60] He clashed with Ballard during the shooting, and on one occasion Kubrick threatened to fire Ballard following a camera dispute, despite being aged only 27 and 20 years Ballard\'s junior.[59] Hayden recalled Kubrick was "cold and detached. Very mechanical, always confident. I\'ve worked with few directors who are that good".[61]', "The Killing failed to secure a proper release across the United States; the film made little money, and was promoted only at the last minute, as a second feature to the Western movie Bandido! (1956). Several contemporary critics lauded the film, with a reviewer for Time comparing its camerawork to that of Orson Welles.[62] Today, critics generally consider The Killing to be among the best films of Kubrick's early career; its nonlinear narrative and clinical execution also had a major influence on later directors of crime films, including Quentin Tarantino. Dore Schary of Metro-Goldwyn-Mayer was highly impressed as well, and offered Kubrick and Harris $75,000 to write, direct, and produce a film, which ultimately became Paths of Glory (1957).[63][h]", 'Paths of Glory, set during World War I, is based on Humphrey Cobb\'s 1935 antiwar novel. Schary was familiar with the novel, but stated that MGM would not finance another war picture, given their backing of the anti-war film The Red Badge of Courage (1951).[i] After Schary was fired by MGM in a major shake-up, Kubrick and Harris managed to interest Kirk Douglas in playing Colonel Dax.[65][j] Douglas, in turn, signed Harris-Kubrick Pictures to a three-picture co-production deal with his film production company, Bryna Productions, which secured a financing and distribution deal for Paths of Glory and two subsequent films with United Artists.[66][67][68] The film, shot in Munich, from March 1957,[69] follows a French army unit ordered on an impossible mission, and follows with a war trial of three soldiers, arbitrarily chosen, for misconduct. Dax is assigned to defend the men at Court Martial. For the battle scene, Kubrick meticulously lined up six cameras one after the other along the boundary of no-man\'s land, with each camera capturing a specific field and numbered, and gave each of the hundreds of extras a number for the zone in which they would die.[70] Kubrick operated an Arriflex camera for the battle, zooming in on Douglas. Paths of Glory became Kubrick\'s first significant commercial success, and established him as an up-and-coming young filmmaker. Critics praised the film\'s unsentimental, spare, and unvarnished combat scenes and its raw, black-and-white cinematography.[71] Despite the praise, the Christmas release date was criticized,[72] and the subject was controversial in Europe. The film was banned in France until 1974 for its "unflattering" depiction of the French military, and was censored by the Swiss Army until 1970.[71]', "In October 1957, after Paths of Glory had its world premiere in Germany, Bryna Productions optioned Canadian church minister-turned-master-safecracker Herbert Emerson Wilsons's autobiography, I Stole $16,000,000, especially for Stanley Kubrick and James B. Harris.[73][74] The picture was to be the second in the co-production deal between Bryna Productions and Harris-Kubrick Pictures, which Kubrick was to write and direct, Harris to co-produce and Douglas to co-produce and star.[73] In November 1957, Gavin Lambert was signed as story editor for I Stole $16,000,000, and with Kubrick, finished a script titled God Fearing Man, but the picture was never filmed.[75]", 'Marlon Brando contacted Kubrick, asking him to direct a film adaptation of the Charles Neider western novel, The Authentic Death of Hendry Jones, featuring Pat Garrett and Billy the Kid.[71][k] Brando was impressed, saying "Stanley is unusually perceptive, and delicately attuned to people. He has an adroit intellect, and is a creative thinker—not a repeater, not a fact-gatherer. He digests what he learns and brings to a new project an original point of view and a reserved passion".[77] The two worked on a script for six months, begun by a then unknown Sam Peckinpah. Many disputes broke out over the project, and in the end, Kubrick distanced himself from what would become One-Eyed Jacks (1961).[l]', 'In February 1959, Kubrick received a phone call from Kirk Douglas asking him to direct Spartacus (1960), based on the historical Spartacus and the Third Servile War. Douglas had acquired the rights to the novel by Howard Fast and blacklisted screenwriter Dalton Trumbo began penning the script.[82] It was produced by Douglas, who also starred as Spartacus, and cast Laurence Olivier as his foe, the Roman general and politician Marcus Licinius Crassus. Douglas hired Kubrick for a reported $150,000 fee to take over direction soon after he fired director Anthony Mann.[83] Kubrick had, at 31, already directed four feature films, and this became his largest by far, with a cast of over 10,000 and a budget of $6 million.[m] At the time, this was the most expensive film ever made in America, and Kubrick became the youngest director in Hollywood history to make an epic.[85] It was the first time that Kubrick filmed using the anamorphic 35mm horizontal Super Technirama process to achieve ultra-high definition, which allowed him to capture large panoramic scenes, including one with 8,000 trained soldiers from Spain representing the Roman army.[n]', 'Disputes broke out during the filming of Spartacus. Kubrick complained about not having full creative control over the artistic aspects, insisting on improvising extensively during the production.[87][o] Kubrick and Douglas were also at odds over the script, with Kubrick angering Douglas when he cut all but two of his lines from the opening 30 minutes.[91] Despite the on-set troubles, Spartacus took $14.6 million at the box office in its first run.[87] The film established Kubrick as a major director, receiving six Academy Award nominations and winning four; it ultimately convinced him that if so much could be made of such a problematic production, he could achieve anything.[92] Spartacus also marked the end of the working relationship between Kubrick and Douglas.[p]', 'Kubrick and Harris decided to film Kubrick\'s next movie Lolita (1962) in England, due to clauses placed on the contract by producers Warner Bros. that gave them complete control over the film, and the fact that the Eady plan permitted producers to write off the costs if 80% of the crew were British. Instead, they signed a $1 million deal with Eliot Hyman\'s Associated Artists Productions, and a clause which gave them the artistic freedom that they desired.[95] Lolita, Kubrick\'s first attempt at black comedy, was an adaptation of the novel of the same name by Vladimir Nabokov, the story of a middle-aged college professor becoming infatuated with a 12-year-old girl. Stylistically, Lolita, starring Peter Sellers, James Mason, Shelley Winters, and Sue Lyon, was a transitional film for Kubrick, "marking the turning point from a naturalistic cinema\xa0... to the surrealism of the later films", according to film critic Gene Youngblood.[96] Kubrick was impressed by the range of actor Peter Sellers and gave him one of his first opportunities to improvise wildly during shooting, while filming him with three cameras.[97][q]', 'Kubrick shot Lolita over 88 days on a $2 million budget at Elstree Studios, between October 1960 and March 1961.[100] Kubrick often clashed with Shelley Winters, whom he found "very difficult" and demanding, and nearly fired at one point.[101] Because of its provocative story, Lolita was Kubrick\'s first film to generate controversy; he was ultimately forced to comply with censors and remove much of the erotic element of the relationship between Mason\'s Humbert and Lyon\'s Lolita which had been evident in Nabokov\'s novel.[102] The film was not a major critical or commercial success, earning $3.7 million at the box office on its opening run.[103][r] Lolita has since become critically acclaimed.[104]', 'Kubrick\'s next project was Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964), another satirical black comedy. Kubrick became preoccupied with the issue of nuclear war as the Cold War unfolded in the 1950s, and even considered moving to Australia because he feared that New York City might be a likely target for the Russians. He studied over 40 military and political research books on the subject and eventually reached the conclusion that "nobody really knew anything and the whole situation was absurd".[105]', 'After buying the rights to the novel Red Alert, Kubrick collaborated with its author, Peter George, on the script. It was originally written as a serious political thriller, but Kubrick decided that a "serious treatment" of the subject would not be believable, and thought that some of its most salient points would be fodder for comedy.[106] Kubrick\'s longtime producer and friend, James B. Harris, thought the film should be serious, and the two parted ways, amicably, over this disagreement—Harris going on to produce and direct the serious cold-war thriller The Bedford Incident.[107][108][109]  Kubrick and Red Alert author George then reworked the script as a satire (provisionally titled "The Delicate Balance of Terror") in which the plot of Red Alert was situated as a film-within-a-film made by an alien intelligence, but this idea was also abandoned, and Kubrick decided to make the film as "an outrageous black comedy".[110]', 'Just before filming began, Kubrick hired noted journalist and satirical author Terry Southern to transform the script into its final form, a black comedy, loaded with sexual innuendo,[111] becoming a film which showed Kubrick\'s talents as a "unique kind of absurdist" according to the film scholar Abrams.[112] Southern made major contributions to the final script, and was co-credited (above Peter George) in the film\'s opening titles; his perceived role in the writing later led to a public rift between Kubrick and Peter George, who subsequently complained in a letter to Life magazine that Southern\'s intense but relatively brief (November 16 to December 28, 1962) involvement with the project was being given undue prominence in the media, while his own role as the author of the film\'s source novel, and his ten-month stint as the script\'s co-writer, were being downplayed – a perception Kubrick evidently did little to address.[113]', 'Kubrick found that Dr. Strangelove, a $2 million production which employed what became the "first important visual effects crew in the world",[114] would be impossible to make in the U.S. for various technical and political reasons, forcing him to move production to England. It was shot in 15 weeks, ending in April 1963, after which Kubrick spent eight months editing it.[115] Peter Sellers again agreed to work with Kubrick, and ended up playing three different roles in the film.[s]', 'Upon release, the film stirred up much controversy and mixed opinions. The New York Times film critic Bosley Crowther worried that it was a "discredit and even contempt for our whole defense establishment\xa0... the most shattering sick joke I\'ve ever come across", [117] while Robert Brustein of Out of This World in a February 1970 article called it a "juvenalian satire".[115] Kubrick responded to the criticism, stating: "A satirist is someone who has a very skeptical view of human nature, but who still has the optimism to make some sort of a joke out of it. However brutal that joke might be".[118] Today, the film is considered to be one of the sharpest comedy films ever made, and holds a near-perfect 98% rating on Rotten Tomatoes based on 91 reviews as of November\xa02020[update].[119] It was named the 39th-greatest American film and third-greatest American comedy film of all time by the American Film Institute,[120][121] and in 2010, it was named the sixth-best comedy film of all time by The Guardian.[122]', 'Kubrick spent five years developing his next film, 2001: A Space Odyssey (1968), having been highly impressed with science fiction writer Arthur C. Clarke\'s novel Childhood\'s End, about a superior alien race who assist mankind in eliminating their old selves. After meeting Clarke in New York City in April 1964, Kubrick made the suggestion to work on his 1948 short story The Sentinel, in which a monolith  found on the Moon alerts aliens of mankind.[123] That year, Clarke began writing the novel 2001: A Space Odyssey and collaborated with Kubrick on a screenplay. The film\'s theme, the birthing of one intelligence by another, is developed in two parallel intersecting stories on two different time scales. One depicts evolutionary transitions between various stages of man, from ape to "star child", as man is reborn into a new existence, each step shepherded by an enigmatic alien intelligence seen only in its artifacts: a series of seemingly indestructible eons-old black monoliths. In space, the enemy is a supercomputer known as HAL who runs the spaceship, a character which novelist Clancy Sigal described as being "far, far more human, more humorous and conceivably decent than anything else that may emerge from this far-seeing enterprise".[124][t]', 'Kubrick intensively researched for the film, paying particular attention to accuracy and detail in what the future might look like. He was granted permission by NASA to observe the spacecraft being used in the Ranger 9 mission for accuracy.[126] Filming commenced on December 29, 1965, with the excavation of the monolith on the moon,[127] and footage was shot in Namib Desert in early 1967, with the ape scenes completed later that year. The special effects team continued working until the end of the year to complete the film, taking the cost to $10.5 million.[127] 2001: A Space Odyssey was conceived as a Cinerama spectacle and was photographed in Super Panavision 70, giving the viewer a "dazzling mix of imagination and science" through ground-breaking effects, which earned Kubrick his only personal Oscar, an Academy Award for Visual Effects.[127][u] Kubrick said of the concept of the film in an interview with Rolling Stone: "On the deepest psychological level, the film\'s plot symbolized the search for God, and finally postulates what is little less than a scientific definition of God. The film revolves around this metaphysical conception, and the realistic hardware and the documentary feelings about everything were necessary in order to undermine your built-in resistance to the poetical concept".[129]', 'Upon release in 1968, 2001: A Space Odyssey was not an immediate hit among critics, who faulted its lack of dialog, slow pacing, and seemingly impenetrable storyline.[130] The film appeared to defy genre convention, much unlike any science-fiction movie before it,[131] and clearly different from any of Kubrick\'s earlier works. Kubrick was particularly outraged by a scathing review from Pauline Kael, who called it "the biggest amateur movie of them all", with Kubrick doing "really every dumb thing he ever wanted to do".[132] Despite mixed contemporary critical reviews, 2001 gradually gained popularity and earned $31 million worldwide by the end of 1972.[127][v] Today, it is widely considered to be one of the greatest and most influential films ever made and is a staple on All Time Top 10 lists.[134][135] Baxter describes the film as "one of the most admired and discussed creations in the history of cinema",[136] and Steven Spielberg has referred to it as "the big bang of his film making generation".[137] For biographer Vincent LoBrutto it "positioned Stanley Kubrick as a pure artist ranked among the masters of cinema".[138]', 'After completing 2001: A Space Odyssey, Kubrick searched for a project that he could film quickly on a more modest budget. He settled on A Clockwork Orange (1971) at the end of 1969, an exploration of violence and experimental rehabilitation by law enforcement authorities, based around the character of Alex (portrayed by Malcolm McDowell). Kubrick had received a copy of Anthony Burgess\'s novel of the same name from Terry Southern while they were working on Dr. Strangelove, but had rejected it on the grounds that Nadsat,[w] a street language for young teenagers, was too difficult to comprehend. The decision to make a film about the degeneration of youth reflected contemporary concerns in 1969; the New Hollywood movement was creating a great number of films that depicted the sexuality and rebelliousness of young people.[139] A Clockwork Orange was shot over 1970–1971 on a budget of £2 million.[140] Kubrick abandoned his use of CinemaScope in filming, deciding that the 1.66:1 widescreen format was, in the words of Baxter, an "acceptable compromise between spectacle and intimacy", and favored his "rigorously symmetrical framing", which "increased the beauty of his compositions".[141] The film heavily features "pop erotica" of the period, including a giant white plastic set of male genitals, decor which Kubrick had intended to give it a "slightly futuristic" look.[142] McDowell\'s role in Lindsay Anderson\'s if.... (1968) was crucial to his casting as Alex,[x] and Kubrick professed that he probably would not have made the film if McDowell had been unavailable.[144]', 'Because of its depiction of teenage violence, A Clockwork Orange became one of the most controversial films of its time, and part of an ongoing debate about violence and its glorification in cinema. It received an X rating, or certificate, in both the UK and US, on its release just before Christmas 1971, though many critics saw much of the violence depicted in the film as satirical, and less violent than Straw Dogs, which had been released a month earlier.[145] Kubrick personally pulled the film from release in the United Kingdom after receiving death threats following a series of copycat crimes based on the film; it was thus completely unavailable legally in the UK until after Kubrick\'s death, and not re-released until 2000.[146][y] John Trevelyan, the censor of the film, personally considered A Clockwork Orange to be "perhaps the most brilliant piece of cinematic art I\'ve ever seen," and believed it to present an "intellectual argument rather than a sadistic spectacle" in its depiction of violence, but acknowledged that many would not agree.[148] Negative media hype over the film notwithstanding, A Clockwork Orange received four Academy Award nominations, for Best Picture, Best Director, Best Screenplay and Best Editing, and was named by the New York Film Critics Circle as the Best Film of 1971.[149] After William Friedkin won Best Director for The French Connection that year, he told the press: "Speaking personally, I think Stanley Kubrick is the best American film-maker of the year. In fact, not just this year, but the best, period."[150]', "Barry Lyndon (1975) is an adaptation of William Makepeace Thackeray's The Luck of Barry Lyndon, a picaresque novel about the adventures of an 18th-century Irish rogue and social climber. John Calley of Warner Bros. agreed in 1972 to invest $2.5 million into the film, on condition that Kubrick approach major Hollywood stars, to ensure success.[151] Like previous films, Kubrick and his art department conducted an enormous amount of research on the 18th century. Extensive photographs were taken of locations and artwork in particular, and paintings were meticulously replicated from works of the great masters of the period in the film.[152][z] The film was shot on location in Ireland, beginning in the autumn of 1973, at a cost of $11 million with a cast and crew of 170.[154] The decision to shoot in Ireland stemmed from the fact that it still retained many buildings from the 18th century period which England lacked.[155] The production was problematic from the start, plagued with heavy rain and political strife involving Northern Ireland at the time.[156] After Kubrick received death threats from the IRA in 1974 due to the shooting scenes with English soldiers, he fled Ireland with his family on a ferry from Dún Laoghaire under an assumed identity and resumed filming in England.[157]", 'Baxter notes that Barry Lyndon was the film which made Kubrick notorious for paying scrupulous attention to detail, often demanding twenty or thirty retakes of the same scene to perfect his art.[158] Often considered to be his most authentic-looking picture,[159] the cinematography and lighting techniques that Kubrick and cinematographer John Alcott used in Barry Lyndon were highly innovative. Interior scenes were shot with a specially adapted high-speed f/0.7 Zeiss camera lens originally developed for NASA to be used in satellite photography. The lenses allowed many scenes to be lit only with candlelight, creating two-dimensional, diffused-light images reminiscent of 18th-century paintings.[160] Cinematographer Allen Daviau states that the method gives the audience a way of seeing the characters and scenes as they would have been seen by people at the time.[161] Many of the fight scenes were shot with a hand-held camera to produce a "sense of documentary realism and immediacy".[162]', 'Barry Lyndon found a great audience in France, but was a box office failure, grossing just $9.5 million in the American market, not even close to the $30 million Warner Bros. needed to generate a profit.[163] The pace and length of Barry Lyndon at three hours put off many American critics and audiences, but the film was nominated for seven Academy Awards and won four, including Best Art Direction, Best Cinematography, Best Costume Design, and Best Musical Score, more than any other Kubrick film. As with most of Kubrick\'s films, Barry Lyndon\'s reputation has grown through the years and it is now considered to be one of his best, particularly among filmmakers and critics. Numerous polls, such as The Village Voice (1999), Sight & Sound (2002), and Time (2005), have rated it as one of the greatest films ever made.[164][165][166] As of March\xa02019[update], it has a 94% rating on Rotten Tomatoes, based on 64 reviews.[167] Roger Ebert referred to it as "one of the most beautiful films ever made\xa0... certainly in every frame a Kubrick film: technically awesome, emotionally distant, remorseless in its doubt of human goodness".[168]', 'The Shining, released in 1980, was adapted from the novel of the same name by bestselling horror writer Stephen King. The film stars Jack Nicholson as a writer who takes a job as a winter caretaker of an isolated hotel in the Rocky Mountains. He spends the winter there with his wife, played by Shelley Duvall, and their young son, who displays paranormal abilities. During their stay, they confront both Jack\'s descent into madness and apparent supernatural horrors lurking in the hotel. Kubrick gave his actors freedom to extend the script and even improvise on occasion, and as a result, Nicholson was responsible for the \'Here\'s Johnny!\' line and the scene in which he\'s sitting at the typewriter and unleashes his anger upon his wife.[169] Kubrick often demanded up to 70 or 80 retakes of the same scene. Duvall, whom Kubrick intentionally isolated and argued with, was forced to perform the exhausting baseball bat scene 127 times.[170] The bar scene with the ghostly bartender was shot 36 times, while the kitchen scene between the characters of Danny (Danny Lloyd) and Halloran (Scatman Crothers) ran to 148 takes.[171] The aerial shots of the Overlook Hotel were shot at Timberline Lodge on Mount Hood in Oregon, while the interiors of the hotel were shot at Elstree Studios in England between May 1978 and April 1979.[172] Cardboard models were made of all of the sets of the film, and the lighting of them was a massive undertaking, which took four months of electrical wiring.[173] Kubrick made extensive use of the newly invented Steadicam, a weight-balanced camera support, which allowed for smooth hand-held camera movement in scenes where a conventional camera track was impractical. According to Garrett Brown, Steadicam\'s inventor, it was the first picture to use its full potential.[174] The Shining was not the only horror film to which Kubrick had been linked; he had turned down the directing of both The Exorcist (1973) and Exorcist II: The Heretic (1977), despite once saying in 1966 to a friend that he had long desired to "make the world\'s scariest movie, involving a series of episodes that would play upon the nightmare fears of the audience".[175]', "Five days after release on May 23, 1980, Kubrick ordered the deletion of a final scene, in which the hotel manager Ullman (Barry Nelson) visits Wendy (Shelley Duvall) in hospital, believing it unnecessary after witnessing the audience excitement in cinemas at the film's climax.[176] The Shining opened to strong box office takings, earning $1 million on the first weekend and earning $30.9 million in America by the end of the year.[172] The original critical response was mixed, and King detested the film and disliked Kubrick.[177] The Shining is now considered to be a horror classic,[178] and the American Film Institute has ranked it as the 27th greatest thriller film of all time.[179]", 'Kubrick met author Michael Herr through mutual friend David Cornwell (novelist John le Carré) in 1980, and became interested in his book Dispatches, about the Vietnam War.[180] Herr had recently written Martin Sheen\'s narration for Apocalypse Now (1979). Kubrick was also intrigued by Gustav Hasford\'s Vietnam War novel The Short-Timers. With the vision in mind to shoot what would become Full Metal Jacket (1987), Kubrick began working with both Herr and Hasford separately on a script. He eventually found Hasford\'s novel to be "brutally honest" and decided to shoot a film which closely follows the novel.[180] All of the film was shot at a cost of $17 million within a 30-mile radius of his house between August 1985 and September 1986, later than scheduled as Kubrick shut down production for five months following a near-fatal accident with a jeep involving Lee Ermey.[181] A derelict gasworks in Beckton in the London Docklands area posed as the ruined city of Huế,[182] which makes the film visually very different from other Vietnam War films. Around 200 palm trees were imported via 40-foot trailers by road from North Africa, at a cost of £1000 a tree, and thousands of plastic plants were ordered from Hong Kong to provide foliage for the film.[183] Kubrick explained he made the film look realistic by using natural light, and achieved a "newsreel effect" by making the Steadicam shots less steady, [184] which reviewers and commentators thought contributed to the bleakness and seriousness of the film.[185]', 'According to critic Michel Ciment, the film contained some of Kubrick\'s trademark characteristics, such as his selection of ironic music, portrayals of men being dehumanized, and attention to extreme detail to achieve realism. In a later scene, United States Marines patrol the ruins of an abandoned and destroyed city singing the theme song to the Mickey Mouse Club as a sardonic counterpoint.[186] The film opened strongly in June 1987, taking over $30 million in the first 50 days alone,[187] but critically it was overshadowed by the success of Oliver Stone\'s Platoon, released a year earlier.[188] Co-star Matthew Modine stated one of Kubrick\'s favorite reviews read: "The first half of FMJ is brilliant. Then the film degenerates into a masterpiece."[189] Roger Ebert was not particularly impressed with it, awarding it a mediocre 2.5 out of 4. He concluded: "Stanley Kubrick\'s Full Metal Jacket is more like a book of short stories than a novel", a "strangely shapeless film from the man whose work usually imposes a ferociously consistent vision on his material".[190]', 'Kubrick\'s final film was Eyes Wide Shut (1999), starring Tom Cruise and Nicole Kidman as a Manhattan couple on a sexual odyssey. Tom Cruise portrays a doctor who witnesses a bizarre masked quasireligious orgiastic ritual at a country mansion, a discovery which later threatens his life. The story is based on Arthur Schnitzler\'s 1926 Freudian novella Traumnovelle (Dream Story in English), which Kubrick relocated from turn-of-the-century Vienna to New York City in the 1990s. Kubrick said of the novel: "A difficult book to describe—what good book isn\'t. It explores the sexual ambivalence of a happy marriage and tries to equate the importance of sexual dreams and might-have-beens with reality. All of Schnitzler\'s work is psychologically brilliant".[191] Kubrick was almost 70, but worked relentlessly for 15 months to get the film out by its planned release date of July 16, 1999. He commenced a script with Frederic Raphael,[162] and worked 18 hours a day, while maintaining complete confidentiality about the film.[192]', 'Eyes Wide Shut, like Lolita and A Clockwork Orange before it, faced censorship before release. Kubrick sent an unfinished preview copy to the stars and producers a few months before release, but his sudden death on March 7, 1999, came a few days after he finished editing. He never saw the final version released to the public,[193] but he did see the preview of the film with Warner Bros., Cruise, and Kidman, and had reportedly told Warner executive Julian Senior that it was his "best film ever".[194] At the time, critical opinion of the film was mixed, and it was viewed less favorably than most of Kubrick\'s films. Roger Ebert awarded it 3.5 out of 4 stars, comparing the structure to a thriller and writing that it is "like an erotic daydream about chances missed and opportunities avoided", and thought that Kubrick\'s use of lighting at Christmas made the film "all a little garish, like an urban sideshow".[195] Stephen Hunter of The Washington Post disliked the film, writing that it "is actually sad, rather than bad. It feels creaky, ancient, hopelessly out of touch, infatuated with the hot taboos of his youth and unable to connect with that twisty thing contemporary sexuality has become."[196]', 'Throughout the 1980s and early 1990s, Kubrick collaborated with Brian Aldiss on expanding his short story "Supertoys Last All Summer Long" into a three-act film. It was a futuristic fairy tale about a robot that resembles and behaves as a child, and his efforts to become a \'real boy\' in a manner similar to Pinocchio. Kubrick approached Spielberg in 1995 with the AI script with the possibility of Steven Spielberg directing it and Kubrick producing it.[188] Kubrick reportedly held long telephone discussions with Spielberg regarding the film, and, according to Spielberg, at one point stated that the subject matter was closer to Spielberg\'s sensibilities than his.[197]', "Following Kubrick's 1999 death, Spielberg took the drafts and notes left by Kubrick and his writers and composed a new screenplay based on an earlier 90-page story treatment by Ian Watson written under Kubrick's supervision and specifications.[198] In association with what remained of Kubrick's production unit, he directed the movie A.I. Artificial Intelligence (2001)[198][199] which was produced by Kubrick's longtime producer (and brother-in-law) Jan Harlan.[200] Sets, costumes, and art direction were based on the works of conceptual artist Chris Baker, who had also done much of his work under Kubrick's supervision.[201]", 'Spielberg was able to function autonomously in Kubrick\'s absence, but said he felt "inhibited to honor him", and followed Kubrick\'s visual schema with as much fidelity as he could. Spielberg, who once referred to Kubrick as "the greatest master I ever served", now with production underway, admitted, "I felt like I was being coached by a ghost."[202] The film was released in June 2001. It contains a posthumous production credit for Stanley Kubrick at the beginning and the brief dedication "For Stanley Kubrick" at the end. John Williams\'s score contains many allusions to pieces heard in other Kubrick films.[203]', 'Following 2001: A Space Odyssey, Kubrick planned to make a film about the life of Napoleon. Fascinated by the French leader\'s life and "self-destruction",[204] Kubrick spent a great deal of time planning the film\'s development and conducted about two years of research into Napoleon\'s life, reading several hundred books and gaining access to his personal memoirs and commentaries. He tried to see every film about Napoleon and found none of them appealing, including Abel Gance\'s 1927 film which is generally considered to be a masterpiece, but for Kubrick, a "really terrible" movie.[205] LoBrutto states that Napoleon was an ideal subject for Kubrick, embracing Kubrick\'s "passion for control, power, obsession, strategy, and the military", while Napoleon\'s psychological intensity and depth, logistical genius and war, sex, and the evil nature of man were all ingredients which deeply appealed to Kubrick.[206]', 'Kubrick drafted a screenplay in 1961, and envisaged making a "grandiose" epic, with up to 40,000 infantry and 10,000 cavalry. He intended hiring the armed forces of an entire country to make the film, as he considered Napoleonic battles to be "so beautiful, like vast lethal ballets", with an "aesthetic brilliance that doesn\'t require a military mind to appreciate". He wanted them replicated as authentically as possible on screen.[207] Kubrick sent research teams to scout for locations across Europe, and commissioned screenwriter and director Andrew Birkin, one of his young assistants on 2001, to the Isle of Elba, Austerlitz, and Waterloo, taking thousands of pictures for his later perusal. Kubrick approached numerous stars to play leading roles, including Audrey Hepburn for Empress Josephine, a part which she could not accept due to semiretirement.[208]British actors David Hemmings and Ian Holm were considered for the lead role of Napoleon, before Jack Nicholson was cast.[209] The film was well into preproduction and ready to begin filming in 1969 when MGM cancelled the project. Numerous reasons have been cited for the abandonment of the project, including its projected cost, a change of ownership at MGM,[204] and the poor reception that the 1970 Soviet film about Napoleon, Waterloo, received. In 2011, Taschen published the book Stanley Kubrick\'s Napoleon: The Greatest Movie Never Made, a large volume compilation of literature and source documents from Kubrick, such as scene photo ideas and copies of letters Kubrick wrote and received. In March 2013, Steven Spielberg, who previously collaborated with Kubrick on A.I. Artificial Intelligence and is a passionate admirer of his work, announced that he would be developing Napoleon as a TV miniseries based on Kubrick\'s original screenplay.[210]', 'In the 1950s, Kubrick and Harris developed a sitcom starring Ernie Kovacs and a film adaption of the book I Stole $16,000,000, but nothing came of them.[71] Tony Frewin, an assistant who worked with the director for a long period of time, revealed in a 2013 Atlantic article: "[Kubrick] was limitlessly interested in anything to do with Nazis and desperately wanted to make a film on the subject." Kubrick had intended to make a film about Dietrich Schulz-Köhn\xa0[de], a Nazi officer who used the pen name "Dr. Jazz" to write reviews of German music scenes during the Nazi era. Kubrick had been given a copy of the Mike Zwerin book Swing Under the Nazis after he had finished production on Full Metal Jacket, the front cover of which featured a photograph of Schulz-Köhn. A screenplay was never completed and Kubrick\'s adaptation was never initiated.[211] The unfinished Aryan Papers, based on Louis Begley\'s debut novel Wartime Lies, was a factor in the abandonment of the project. Work on Aryan Papers depressed Kubrick enormously, and he eventually decided that Steven Spielberg\'s Schindler\'s List (1993) covered much of the same material.[188]', 'According to biographer John Baxter, Kubrick had shown an interest in directing a pornographic film based on a satirical novel written by Terry Southern, titled Blue Movie, about a director who makes Hollywood\'s first big-budget porn film. Baxter claims that Kubrick concluded he did not have the patience or temperament to become involved in the porn industry, and Southern stated that Kubrick was "too ultra conservative" towards sexuality to have gone ahead with it, but liked the idea.[212] Kubrick was unable to direct a film of Umberto Eco\'s Foucault\'s Pendulum as Eco had given his publisher instructions to never sell the film rights to any of his books after his dissatisfaction with the film version of The Name of the Rose.[213] Also, when the film rights to Tolkien\'s The Lord of the Rings were sold to United Artists, the Beatles approached Kubrick to direct them in a film adaptation, but Kubrick was unwilling to produce a film based on a very popular book.[214]', 'Anyone who has ever been privileged to direct a film knows that, although it can be like trying to write War and Peace in a bumper car at an amusement park, when you finally get it right, there are not many joys in life that can equal the feeling.', 'As a young man, Kubrick was fascinated by the films of Soviet filmmakers such as Sergei Eisenstein and Vsevolod Pudovkin.[216] Kubrick read Pudovkin\'s seminal theoretical work, Film Technique, which argues that editing makes film a unique art form, and it needs to be employed to manipulate the medium to its fullest. Kubrick recommended this work to others for many years. Thomas Nelson describes this book as "the greatest influence of any single written work on the evolution of [Kubrick\'s] private aesthetics". Kubrick also found the ideas of Konstantin Stanislavski to be essential to his understanding the basics of directing, and gave himself a crash course to learn his methods.[217]', 'Kubrick\'s family and many critics felt that his Jewish ancestry may have contributed to his worldview and aspects of his films. After his death, both his daughter and wife stated that he was not religious, but "did not deny his Jewishness, not at all". His daughter noted that he wanted to make a film about the Holocaust, the Aryan Papers, having spent years researching the subject.[218] Most of Kubrick\'s friends and early photography and film collaborators were Jewish, and his first two marriages were to daughters of recent Jewish immigrants from Europe. British screenwriter Frederic Raphael, who worked closely with Kubrick in his final years, believes that the originality of Kubrick\'s films was partly because he "had a (Jewish?) respect for scholars". He declared that it was "absurd to try to understand Stanley Kubrick without reckoning on Jewishness as a fundamental aspect of his mentality".[219]', 'Walker notes that Kubrick was influenced by the tracking and "fluid camera" styles of director Max Ophüls, and used them in many of his films, including Paths of Glory and 2001: A Space Odyssey. Kubrick noted how in Ophuls\' films "the camera went through every wall and every floor".[220] He once named Ophüls\' Le Plaisir (1952) as his favorite film. According to film historian John Wakeman, Ophüls himself learned the technique from director Anatole Litvak in the 1930s, when he was his assistant, and whose work was "replete with the camera trackings, pans and swoops which later became the trademark of Max Ophüls".[221] Geoffrey Cocks believes that Kubrick was also influenced by Ophüls\' stories of thwarted love and a preoccupation with predatory men, while Herr notes that Kubrick was deeply inspired by G. W. Pabst, who earlier tried, but was unable to adapt Schnitzler\'s Traumnovelle, the basis of Eyes Wide Shut.[222] Film historian/critic Robert Kolker sees the influence of Welles\' moving camera shots on Kubrick\'s style. LoBrutto notes that Kubrick identified with Welles and that this influenced the making of The Killing, with its "multiple points of view, extreme angles, and deep focus".[223][224]', 'Kubrick admired the work of Ingmar Bergman and expressed it in personal letter: "Your vision of life has moved me deeply, much more deeply than I have ever been moved by any films. I believe you are the greatest film-maker at work today [...], unsurpassed by anyone in the creation of mood and atmosphere, the subtlety of performance, the avoidance of the obvious, the truthfulness and completeness of characterization. To this one must also add everything else that goes into the making of a film; [...] and I shall look forward with eagerness to each of your films."[225]', "When the American magazine Cinema asked Kubrick in 1963 to name his favorite films, he listed Italian director Federico Fellini's I Vitelloni as number one in his Top 10 list.[226]", "Kubrick's films typically involve expressions of an inner struggle, examined from different perspectives.[215]He was very careful not to present his own views of the meaning of his films and to leave them open to interpretation. He explained in a 1960 interview with Robert Emmett Ginna:", '"One of the things I always find extremely difficult, when a picture\'s finished, is when a writer or a film reviewer asks, \'Now, what is it that you were trying to say in that picture?\' And without being thought too presumptuous for using this analogy, I like to remember what T. S. Eliot said to someone who had asked him—I believe it was The Waste Land—what he meant by the poem. He replied, \'I meant what I said.\' If I could have said it any differently, I would have".[227]', 'Kubrick likened the understanding of his films to popular music, in that whatever the background or intellect of the individual, a Beatles record, for instance, can be appreciated both by the Alabama truck driver and the young Cambridge intellectual, because their "emotions and subconscious are far more similar than their intellects". He believed that the subconscious emotional reaction experienced by audiences was far more powerful in the film medium than in any other traditional verbal form, and this was one of the reasons why he often relied on long periods in his films without dialogue, placing emphasis on images and sound.[227] In a 1975 Time magazine interview, Kubrick further stated: "The essence of a dramatic form is to let an idea come over people without it being plainly stated. When you say something directly, it is simply not as potent as it is when you allow people to discover it for themselves."[40] He also said: "Realism is probably the best way to dramatize argument and ideas. Fantasy may deal best with themes which lie primarily in the unconscious".[228]', 'Diane Johnson, who co-wrote the screenplay for The Shining with Kubrick, notes that he "always said that it was better to adapt a book rather than write an original screenplay, and that you should choose a work that isn\'t a masterpiece so you can improve on it. Which is what he\'s always done, except with Lolita".[229] When deciding on a subject for a film, there were many aspects that he looked for, and he always made films which would "appeal to every sort of viewer, whatever their expectation of film".[230] According to his co-producer Jan Harlan, Kubrick mostly "wanted to make films about things that mattered, that not only had form, but substance".[231] Kubrick believed that audiences quite often were attracted to "enigmas and allegories" and did not like films in which everything was spelled out clearly.[232]', 'Sexuality in Kubrick\'s films is usually depicted outside matrimonial relationships in hostile situations. Baxter states that Kubrick explores the "furtive and violent side alleys of the sexual experience: voyeurism, domination, bondage and rape" in his films.[233] He further points out that films like A Clockwork Orange are "powerfully homoerotic", from Alex walking about his parents\' flat in his Y-fronts, one eye being "made up with doll-like false eyelashes", to his innocent acceptance of the sexual advances of his post-corrective adviser Deltroid (Aubrey Morris).[234] Indeed, the film is believed to have been at least partially inspired by the landmark work of queer cinema Funeral Parade of Roses.[235] Film critic Adrian Turner notes that Kubrick\'s films appear to be "preoccupied with questions of universal and inherited evil", and Malcolm McDowell referred to his humor as "black as coal", questioning his outlook on humanity.[236] A few of his pictures were obvious satires and black comedies, such as Lolita and Dr. Strangelove; many of his other films also contained less visible elements of satire or irony. His films are unpredictable, examining "the duality and contradictions that exist in all of us".[237] Ciment notes how Kubrick often tried to confound audience expectations by establishing radically different moods from one film to the next, remarking that he was almost "obsessed with contradicting himself, with making each work a critique of the previous one".[238]Kubrick stated that "there is no deliberate pattern to the stories that I have chosen to make into films. About the only factor at work each time is that I try not to repeat myself".[239] As a result, Kubrick was often misunderstood by critics, and only once did he have unanimously positive reviews upon the release of a film—for Paths of Glory.[240]', 'Film author Patrick Webster considers Kubrick\'s methods of writing and developing scenes to fit with the classical auteur theory of directing, allowing collaboration and improvisation with the actors during filming.[241] Malcolm McDowell recalled Kubrick\'s collaborative emphasis during their discussions and his willingness to allow him to improvise a scene, stating that "there was a script and we followed it, but when it didn\'t work he knew it, and we had to keep rehearsing endlessly until we were bored with it".[242]Once Kubrick was confident in the overall staging of a scene, and felt the actors were prepared, he would then develop the visual aspects, including camera and lighting placement. Walker believes that Kubrick was one of "very few film directors competent to instruct their lighting photographers in the precise effect they want".[243] Baxter believes that Kubrick was heavily influenced by his ancestry and always possessed a European perspective to filmmaking, particularly the Austro-Hungarian empire and his admiration for Max Ophuls and Richard Strauss.[244]', 'Gilbert Adair, writing in a review for Full Metal Jacket, commented that "Kubrick\'s approach to language has always been of a reductive and uncompromisingly deterministic nature. He appears to view it as the exclusive product of environmental conditioning, only very marginally influenced by concepts of subjectivity and interiority, by all whims, shades and modulations of personal expression".[245] Johnson notes that although Kubrick was a "visual filmmaker", he also loved words and was like a writer in his approach, very sensitive to the story itself, which he found unique.[246] Before shooting began, Kubrick tried to have the script as complete as possible, but still allowed himself enough space to make changes during the filming, finding it "more profitable to avoid locking up any ideas about staging or camera or even dialogue prior to rehearsals" as he put it.[243] Kubrick told Robert Emmett Ginna: "I think you have to view the entire problem of putting the story you want to tell up there on that light square. It begins with the selection of the property; it continues through the creation of the story, the sets, the costumes, the photography and the acting. And when the picture is shot, it\'s only partially finished. I think the cutting is just a continuation of directing a movie. I think the use of music effects, opticals and finally main titles are all part of telling the story. And I think the fragmentation of these jobs, by different people, is a very bad thing".[159] Kubrick also said: "I think that the best plot is no apparent plot. I like a slow start, the start that gets under the audience\'s skin and involves them so that they can appreciate grace notes and soft tones and don\'t have to be pounded over the head with plot points and suspense tools."[154]', "They work with Stanley and go through hells that nothing in their careers could have prepared them for, they think they must have been mad to get involved, they think that they'd die before they would ever work with him again, that fixated maniac; and when it's all behind them and the profound fatigue of so much intensity has worn off, they'd do anything in the world to work for him again. For the rest of their professional lives they long to work with someone who cared the way Stanley did, someone they could learn from. They look for someone to respect the way they'd come to respect him, but they can never find anybody\xa0... I've heard this story so many times.", '—Michael Herr, screenwriter for Full Metal Jacket on actors working with Kubrick.[247]', 'Kubrick was notorious for demanding multiple takes during filming to perfect his art, and his relentless approach was often extremely demanding for his actors. Jack Nicholson remarked that Kubrick would often demand up to fifty takes of a scene.[248] Nicole Kidman explains that the large number of takes he often required stopped actors from consciously thinking about technique, thereby helping them enter a "deeper place".[249] Kubrick\'s high take ratio was considered by some critics as "irrational"; he firmly believed that actors were at their best during the filming, as opposed to rehearsals, due to the sense of intense excitement that it generates.[250] Kubrick explained: "Actors are essentially emotion-producing instruments, and some are always tuned and ready while others will reach a fantastic pitch on one take and never equal it again, no matter how hard they try"\xa0...[251]', '"When you make a movie, it takes a few days just to get used to the crew, because it is like getting undressed in front of fifty people. Once you\'re accustomed to them, the presence of even one other person on set is discordant and tends to produce self-consciousness in the actors, and certainly in itself".[252] He also told biographer Michel Ciment: "It\'s invariably because the actors don\'t know their lines, or don\'t know them well enough. An actor can only do one thing at a time, and when he learned his lines only well enough to say them while he\'s thinking about them, he will always have trouble as soon as he has to work on the emotions of the scene or find camera marks. In a strong emotional scene, it is always best to be able to shoot in complete takes to allow the actor a continuity of emotion, and it is rare for most actors to reach their peak more than once or twice. There are, occasionally, scenes which benefit from extra takes, but even then, I\'m not sure that the early takes aren\'t just glorified rehearsals with the adding adrenaline of film running through the camera."[253]', 'Kubrick would devote his personal breaks to having lengthy discussions with actors. Among those who valued his attention was Tony Curtis, star of Spartacus, who said Kubrick was his favorite director, adding, "his greatest effectiveness was his one-on-one relationship with actors."[94] He further added, "Kubrick had his own approach to film-making. He wanted to see the actor\'s faces. He didn\'t want cameras always in a wide shot twenty-five feet away, he wanted close-ups, he wanted to keep the camera moving. That was his style."[85] Similarly, Malcolm McDowell recalls the long discussions he had with Kubrick to help him develop his character in A Clockwork Orange, noting that on set he felt entirely uninhibited and free, which is what made Kubrick "such a great director".[248] Kubrick also allowed actors at times to improvise and to "break the rules", particularly with Peter Sellers in Lolita, which became a turning point in his career as it allowed him to work creatively during the shooting, as opposed to the preproduction stage.[254]During an interview, Ryan O\'Neal recalled Kubrick\'s directing style: "God, he works you hard. He moves you, pushes you, helps you, gets cross with you, but above all he teaches you the value of a good director. Stanley brought out aspects of my personality and acting instincts that had been dormant\xa0... My strong suspicion [was] that I was involved in something great".[255] He further added that working with Kubrick was "a stunning experience" and that he never recovered from working with somebody of such magnificence.[256]', 'Kubrick credited the ease with which he filmed scenes to his early years as a photographer.[257] He rarely added camera instructions in the script, preferring to handle that after a scene is created, as the visual part of film-making came easiest to him.[258] Even in deciding which props and settings would be used, Kubrick paid meticulous attention to detail and tried to collect as much background material as possible, functioning rather like what he described as "a detective".[259] Cinematographer John Alcott, who worked closely with Kubrick on four of his films, and won an Oscar for Best Cinematography on Barry Lyndon, remarked that Kubrick "questions everything",[260] and was involved in the technical aspects of film-making including camera placement, scene composition, choice of lens, and even operating the camera which would usually be left to the cinematographer. Alcott considered Kubrick to be the "nearest thing to genius I\'ve ever worked with, with all the problems of a genius".[261]', 'Among Kubrick\'s innovations in cinematography are his use of special effects, as in 2001, where he used both slit-scan photography and front-screen projection, which won Kubrick his only Oscar for special effects. Some reviewers have described and illustrated with video clips, Kubrick\'s use of "one-point perspective", which leads the viewer\'s eye towards a central vanishing point. The technique relies on creating a complex visual symmetry using parallel lines in a scene which all converge on that single point, leading away from the viewer. Combined with camera motion it could produce an effect that one writer describes as "hypnotic and thrilling".[262] The Shining was among the first half-dozen features to use the then-revolutionary Steadicam (after the 1976 films Bound for Glory, Marathon Man and Rocky). Kubrick used it to its fullest potential, which gave the audience smooth, stabilized, motion-tracking by the camera. Kubrick described Steadicam as being like a "magic carpet", allowing "fast, flowing, camera movements" in the maze in The Shining which otherwise would have been impossible.[263]', 'Kubrick was among the first directors to use video assist during filming. At the time he began using it in 1966, it was considered cutting-edge technology, requiring him to build his own system. Having it in place during the filming of 2001, he was able to view a video of a take immediately after it was filmed.[264] On some films, such as Barry Lyndon, he used custom made zoom lenses, which allowed him to start a scene with a close-up and slowly zoom out to capture the full panorama of scenery and to film long takes under changing outdoor lighting conditions by making aperture adjustments while the cameras rolled. LoBrutto notes that Kubrick\'s technical knowledge about lenses "dazzled the manufacturer\'s engineers, who found him to be unprecedented among contemporary filmmakers".[265] For Barry Lyndon he also used a specially adapted high-speed (f/0.7) Zeiss camera lens, originally developed for NASA, to shoot numerous scenes lit only with candlelight. Actor Steven Berkoff recalls that Kubrick wanted scenes to be shot using "pure candlelight", and in doing so Kubrick "made a unique contribution to the art of filmmaking going back to painting\xa0... You almost posed like for portraits."[266] LoBrutto notes that cinematographers all over the world wanted to know about Kubrick\'s "magic lens" and that he became a "legend" among cameramen around the world.[267]', 'Kubrick spent extensive hours editing, often working seven days a week, and more hours a day as he got closer to deadlines.[268] For Kubrick, written dialogue was one element to be put in balance with mise en scène (set arrangements), music, and especially, editing. Inspired by Pudovkin\'s treatise on film editing, Kubrick realized that one could create a performance in the editing room and often "re-direct" a film, and he remarked: "I love editing. I think I like it more than any other phase of filmmaking\xa0... Editing is the only unique aspect of filmmaking which does not resemble any other art form—a point so important it cannot be overstressed\xa0... It can make or break a film".[268] Biographer John Baxter stated that "Instead of finding the intellectual spine of a film in the script before starting work, Kubrick felt his way towards the final version of a film by shooting each scene from many angles and demanding scores of takes on each line. Then over months\xa0... he arranged and rearranged the tens of thousands of scraps of film to fit a vision that really only began to emerge during editing".[269]', 'Kubrick\'s attention to music was an aspect of what many referred to as his "perfectionism" and extreme attention to minute details, which his wife Christiane attributed to an addiction to music. In his last six films, Kubrick usually chose music from existing sources, especially classical compositions. He preferred selecting recorded music over having it composed for a film, believing that no hired composer could do as well as the public domain classical composers. He also felt that building scenes from great music often created the "most memorable scenes" in the best films.[270] In one instance, for a scene in Barry Lyndon which was written into the screenplay as merely, "Barry duels with Lord Bullingdon", he spent forty-two working days in the editing phase. During that period, he listened to what LoBrutto describes as "every available recording of seventeenth-and eighteenth- century music, acquiring thousands of records to find Handel\'s sarabande used to score the scene".[271] Nicholson likewise observed his attention to music, stating that Kubrick "listened constantly to music until he discovered something he felt was right or that excited him".[240]', 'Kubrick is credited with introducing Hungarian composer György Ligeti to a broad Western audience by including his music in 2001, The Shining and Eyes Wide Shut. According to Baxter, the music in 2001 was "at the forefront of Kubrick\'s mind" when he conceived the film.[272] During earlier screening he played music by Mendelssohn[aa] and Vaughan Williams, and Kubrick and writer Clarke had listened to Carl Orff\'s transcription of Carmina Burana, consisting of 13th century sacred and secular songs.[272] Ligeti\'s music employed the new style of micropolyphony, which used sustained dissonant chords that shift slowly over time, a style he originated. Its inclusion in the film became a "boon for the relatively unknown composer" partly because it was introduced alongside background by Johann Strauss and Richard Strauss.[274]', 'In addition to Ligeti, Kubrick enjoyed a collaboration with composer Wendy Carlos, whose 1968 album Switched-On Bach—which re-interpreted baroque music through the use of a Moog synthesizer—caught his attention. In 1971, Carlos composed and recorded music for the soundtrack of A Clockwork Orange. Additional music not used in the film was released in 1972 as Wendy Carlos\'s Clockwork Orange. Kubrick later collaborated with Carlos on The Shining (1980). The opening of the film employs Carlos\' rendering of "Dies Irae" (Day of Wrath) from Hector Berlioz\'s Symphonie Fantastique.[275]', "Kubrick married his high-school sweetheart Toba Metz, a caricaturist, on May 29, 1948, when he was 19 years old.[23] The couple lived together in Greenwich Village and divorced three years later in 1951. He met his second wife, the Austrian-born dancer and theatrical designer Ruth Sobotka, in 1952. They lived together in New York City's East Village beginning in 1952, married in January 1955 and moved to Hollywood in July 1955, where she played a brief part as a ballet dancer in Kubrick's film, Killer's Kiss (1955). The following year, she was art director for his film, The Killing (1956). They divorced in 1957.[276]", "During the production of Paths of Glory in Munich in early 1957, Kubrick met and romanced the German actress Christiane Harlan, who played a small though memorable role in the film. Kubrick married Harlan in 1958 and the couple remained together for 40 years, until his death in 1999. Besides his stepdaughter, they had two daughters together: Anya Renata (April 6, 1959 – July 7, 2009) and Vivian Vanessa (born August 5, 1960).[277] In 1959, they settled into a home at 316 South Camden Drive in Beverly Hills with Harlan's daughter, Katherina, aged six.[278] They also lived in New York City, during which time Christiane studied art at the Art Students League of New York, later becoming an independent artist.[279] The couple moved to the United Kingdom in 1961 to make Lolita, and Kubrick hired Peter Sellers to star in his next film, Dr. Strangelove. Sellers was unable to leave the UK, so Kubrick made Britain his permanent home thereafter. The move was quite convenient to Kubrick, since he shunned the Hollywood system and its publicity machine and he and Christiane had become alarmed with the increase in violence in New York City.[280]", 'In 1965, the Kubricks bought Abbots Mead on Barnet Lane, just south-west of the Elstree/Borehamwood studio complex in England. Kubrick worked almost exclusively from this home for 14 years where, he researched, invented special effects techniques, designed ultra-low light lenses for specially modified cameras, pre-produced, edited, post-produced, advertised, distributed and carefully managed all aspects of four of his films. In 1978, Kubrick moved into Childwickbury Manor in Hertfordshire, a mainly 18th-century stately home, which was once owned by a wealthy racehorse owner, about 30\xa0mi (50\xa0km) north of London and a 10-minute drive from his previous home at Abbotts Mead. His new home became a workplace for Kubrick and his wife, "a perfect family factory" as Christiane called it,[281] and Kubrick converted the stables into extra production rooms besides ones within the home that he used for editing and storage.[282]', 'A workaholic, Kubrick rarely took a vacation or left England during the forty years before his death.[283] LoBrutto notes that Kubrick\'s confined way of living and desire for privacy has led to spurious stories about his reclusiveness, similar to those of Greta Garbo, Howard Hughes and J. D. Salinger.[284] Michael Herr, Kubrick\'s co-screenwriter on Full Metal Jacket, who knew him well, considers his "reclusiveness" to be myth: "[He] was in fact a complete failure as a recluse, unless you believe that a recluse is simply someone who seldom leaves his house. Stanley saw a lot of people\xa0... he was one of the most gregarious men I ever knew and it didn\'t change anything that most of this conviviality went on over the phone." [285] LoBrutto states that one of the reasons he acquired a reputation as a recluse was that he insisted in remaining near his home but the reason for this was that for Kubrick there were only three places on the planet he could make high quality films with the necessary technical expertise and equipment: Los Angeles, New York City or around London. He disliked living in Los Angeles and thought London a superior film production center to New York City.[286]', 'As a person, Kubrick was described by Norman Lloyd as "a very dark, sort of a glowering type who was very serious".[287] Marisa Berenson, who starred in Barry Lyndon, fondly recalled: "There was great tenderness in him and he was passionate about his work. What was striking was his enormous intelligence but he also had a great sense of humor. He was a very shy person and self-protective but he was filled with the thing that drove him twenty-four hours of the day."[288] Kubrick was particularly fond of machines and technical equipment, to the point that his wife Christiane once stated that "Stanley would be happy with eight tape recorders and one pair of pants".[289] Kubrick had obtained a pilot\'s license in August 1947 and some have claimed that he later developed a fear of flying, stemming from an incident in the early 1950s when a colleague was killed in a plane crash. Kubrick had been sent the charred remains of his camera and notebooks which, according to Duncan, traumatized him for life.[87][ab] Kubrick also had a strong mistrust of doctors and medicine.[291]', 'On March 7, 1999, six days after screening a final cut of Eyes Wide Shut for his family and the stars, Kubrick died in his sleep at the age of 70, suffering a heart attack.[292] His funeral was held five days later at Childwickbury Manor, with only close friends and family in attendance, totaling about 100 people. The media were kept a mile away outside the entrance gate.[293] Alexander Walker, who attended the funeral, described it as a "family farewell,\xa0... almost like an English picnic", with cellists, clarinetists and singers providing song and music from many of his favorite classical compositions. Kaddish, the Jewish prayer typically said by mourners and in other contexts, was recited. A few of his obituaries mentioned his Jewish background.[294] Among those who gave eulogies were Terry Semel, Jan Harlan, Steven Spielberg, Nicole Kidman and Tom Cruise. He was buried next to his favorite tree on the estate. In her book dedicated to Kubrick, his wife Christiane included one of his favorite quotations of Oscar Wilde: "The tragedy of old age is not that one is old but that one is young."[295]', 'Part of the New Hollywood film-making wave, Kubrick\'s films are considered by film historian Michel Ciment to be "among the most important contributions to world cinema in the twentieth century",[34] and he is frequently cited as one of the greatest and most influential directors in the history of cinema.[296][297] According to film historian and Kubrick scholar Robert Kolker,[298][299][300][301] Kubrick\'s films were "more intellectually rigorous than the work of any other American filmmaker."[298] Leading directors, including Martin Scorsese,[302][303] Steven Spielberg,[304] Wes Anderson,[305] George Lucas,[306] James Cameron,[307] Terry Gilliam,[308] the Coen brothers,[309] Ridley Scott,[310] and George A. Romero,[311] have cited Kubrick as a source of inspiration, and additionally in the case of Spielberg and Scott, collaboration.[304][312] On the DVD of Eyes Wide Shut, Steven Spielberg comments that the way Kubrick "tells a story is antithetical to the way we are accustomed to receiving stories" and that "nobody could shoot a picture better in history".[313] Orson Welles, one of Kubrick\'s greatest personal influences and favorite directors, said that: "Among those whom I would call \'younger generation\', Kubrick appears to me to be a giant."[314]', "Kubrick continues to be cited as a major influence by many directors, including Christopher Nolan,[315] Todd Field,[316] David Fincher, Guillermo del Toro, David Lynch,[317] Lars von Trier,[318] Tim Burton,[319] Michael Mann,[320] and Gaspar Noé.[321] Many filmmakers imitate Kubrick's inventive and unique use of camera movement and framing, as well as his use of music, including Frank Darabont.[322]", 'Artists in fields other than film have also expressed admiration for Kubrick. English musician and poet PJ Harvey, in an interview about her 2011 album Let England Shake, argued that "something about [...] what is not said in his films...there\'s so much space, so many things that are silent – and somehow, in that space and silence everything becomes clear. With every film, he seems to capture the essence of life itself, particularly in films like Paths of Glory, 2001: A Space Odyssey, Barry Lyndon...those are some of my favorites."[323] The music video for Kanye West\'s 2010 song "Runaway" was inspired by Eyes Wide Shut.[324] Pop singer Lady Gaga\'s concert shows have included the use of dialogue, costumes, and music from A Clockwork Orange.[325]', 'In 2000, BAFTA renamed their Britannia lifetime achievement award the "Stanley Kubrick Britannia Award",[326] joining the likes of D. W. Griffith, Laurence Olivier, Cecil B. DeMille, and Irving Thalberg, all of whom have annual awards named after them. Kubrick won this award in 1999, and subsequent recipients have included George Lucas, Warren Beatty, Tom Cruise, Robert De Niro, Clint Eastwood, and Daniel Day-Lewis. Many people who worked with Kubrick on his films created the 2001 documentary Stanley Kubrick: A Life in Pictures, produced and directed by Kubrick\'s brother-in-law, Jan Harlan, who had executive produced Kubrick\'s last four films.[327]', 'The first public exhibition of material from Kubrick\'s personal archives was presented jointly in 2004 by the Deutsches Filmmuseum and Deutsches Architekturmuseum in Frankfurt, Germany, in cooperation with Christiane Kubrick and Jan Harlan / The Stanley Kubrick Estate.[328] In 2009, an exhibition of paintings and photos inspired by Kubrick\'s films was held in Dublin, Ireland, entitled "Stanley Kubrick: Taming Light".[329] On October 30, 2012, an exhibition devoted to Kubrick opened at the Los Angeles County Museum of Art (LACMA) and concluded in June 2013. Exhibits include a wide collection of documents, photographs and on-set material assembled from 800 boxes of personal archives that were stored in Kubrick\'s home-workplace in the UK.[330] Many celebrities attended and spoke at the museum\'s pre-opening gala, including Steven Spielberg, Tom Hanks and Jack Nicholson,[331] while Kubrick\'s widow, Christiane, appeared at the pre-gala press review.[332] In October 2013, the Brazil São Paulo International Film Festival paid tribute to Kubrick, staging an exhibit of his work and a retrospective of his films. The exhibit opened at the Toronto International Film Festival (TIFF) in late 2014 and ended in January 2015.[333]', "Kubrick is widely referenced in popular culture; for example, the TV series The Simpsons is said to contain more references to Kubrick films than any other pop culture phenomenon.[334] When the Directors Guild of Great Britain gave Kubrick a lifetime achievement award, they included a cut-together sequence of all the homages from the show.[335][336] Several works have been created that related to Kubrick's life, including the made-for-TV mockumentary Dark Side of the Moon (2002), which is a parody of the pervasive conspiracy theory that Kubrick had been involved with the faked footage of the NASA moon landings during the filming of 2001: A Space Odyssey. Colour Me Kubrick (2005) was authorized by Kubrick's family and starred John Malkovich as Alan Conway, a con artist who had assumed Kubrick's identity in the 1990s.[337] In the 2004 film The Life and Death of Peter Sellers, Kubrick was portrayed by Stanley Tucci; the film documents the filming of Dr. Strangelove.[338]", "In April 2018, the month that marked the 50th anniversary of 2001: A Space Odyssey, the International Astronomical Union named the largest mountain of Pluto's moon Charon after Kubrick.[339][340]", "From October 2019 to March 2020, the Skirball Cultural Center hosted an exhibition called Through a Different Lens: Stanley Kubrick Photographs, a show focusing on Kubrick's early career.[341][342][343]", '']}, {'headings': ['Arthur C. Clarke'], 'subheadings': ['Contents', 'Biography', 'Science-fiction writer', 'Science writer', 'Geostationary communications satellite', 'Undersea explorer', 'Views', 'Themes, style, and influences', 'Awards, honours, and other recognition', 'Selected bibliography', 'Media appearances', 'See also', 'Notes', 'References', 'External links', 'Navigation menu'], 'paras': ['', 'Sir Arthur Charles Clarke CBE FRAS (16 December 1917\xa0– 19 March 2008) was an English science-fiction writer, science writer, futurist,[3] inventor, undersea explorer, and television series host.', 'He co-wrote the screenplay for the 1968 film 2001: A Space Odyssey, widely regarded as one of the most influential films of all time.[4][5] Clarke was a science fiction writer, an avid populariser of space travel, and a futurist of a distinguished ability. He wrote many books and many essays for popular magazines. In 1961, he received the Kalinga Prize, a UNESCO award for popularising science. Clarke\'s science and science-fiction writings earned him the moniker "Prophet of the Space Age".[6] His science-fiction writings in particular earned him a number of Hugo and Nebula awards, which along with a large readership, made him one of the towering figures of the genre. For many years Clarke, Robert Heinlein, and Isaac Asimov were known as the "Big Three" of science fiction.[7]', 'Clarke was a lifelong proponent of space travel. In 1934, while still a teenager, he joined the BIS, British Interplanetary Society. In 1945, he proposed a satellite communication system using geostationary orbits.[8] He was the chairman of the British Interplanetary Society from 1946 to 1947 and again in 1951–1953.[9]', "Clarke immigrated to Ceylon (now Sri Lanka) in 1956, to pursue his interest in scuba diving.[10] That year, he discovered the underwater ruins of the ancient original Koneswaram Temple in Trincomalee. Clarke augmented his popularity in the 1980s, as the host of television shows such as Arthur C. Clarke's Mysterious World. He lived in Sri Lanka until his death.[11]", 'Clarke was appointed Commander of the Order of the British Empire (CBE) in 1989 "for services to British cultural interests in Sri Lanka".[12] He was knighted in 1998[13][14] and was awarded Sri Lanka\'s highest civil honour, Sri Lankabhimanya, in 2005.[15]', 'Clarke was born in Minehead, Somerset, England,[16] and grew up in nearby Bishops Lydeard. As a boy, he lived on a farm, where he enjoyed stargazing, fossil collecting, and reading American science-fiction pulp magazines. He received his secondary education at Huish school in Taunton. Some of his early influences included dinosaur cigarette cards, which led to an enthusiasm for fossils starting about 1925. Clarke attributed his interest in science fiction to reading three items: the November 1928 issue of Amazing Stories in 1929; Last and First Men by Olaf Stapledon in 1930; and The Conquest of Space by David Lasser in 1931.[17]', 'In his teens, he joined the Junior Astronomical Association and contributed to Urania, the society\'s journal, which was edited in Glasgow by Marion Eadie. At Clarke\'s request, she added an "Astronautics" section, which featured a series of articles written by him on spacecraft and space travel. Clarke also contributed pieces to the "Debates and Discussions Corner", a counterpoint to a Urania article offering the case against space travel, and also his recollections of the Walt Disney film Fantasia. He moved to London in 1936 and joined the Board of Education as a pensions auditor.[18] He and some fellow science-fiction writers shared a flat in Gray\'s Inn Road, where he got the nickname "Ego" because of his absorption in subjects that interested him,[19] and later named his office filled with memorabilia as his "ego chamber".[20]', "During the Second World War from 1941 to 1946, he served in the Royal Air Force as a radar specialist and was involved in the early-warning radar defence system, which contributed to the RAF's success during the Battle of Britain. Clarke spent most of his wartime service working on ground-controlled approach (GCA) radar, as documented in the semiautobiographical Glide Path, his only non-science-fiction novel. Although GCA did not see much practical use during the war, after several years of development it proved vital to the Berlin Airlift of 1948–1949. Clarke initially served in the ranks and was a corporal instructor on radar at No.\xa02 Radio School, RAF Yatesbury in Wiltshire. He was commissioned as a pilot officer (technical branch) on 27 May 1943.[21] He was promoted flying officer on 27 November 1943.[22] He was appointed chief training instructor at RAF Honiley in Warwickshire and was demobilised with the rank of flight lieutenant.", "After the war, he attained a first-class degree in mathematics and physics from King's College London.[23][24][25] After this, he worked as assistant editor at Physics Abstracts.[26] Clarke then served as president of the British Interplanetary Society from 1946 to 1947 and again from 1951 to 1953.[27]", 'Although he was not the originator of the concept of geostationary satellites, one of his most important contributions in this field was his idea that they would be ideal telecommunications relays. He advanced this idea in a paper privately circulated among the core technical members of the British Interplanetary Society in 1945. The concept was published in Wireless World in October of that year.[8] Clarke also wrote a number of nonfiction books describing the technical details and societal implications of rocketry and space flight. The most notable of these may be Interplanetary Flight: An Introduction to Astronautics (1950), The Exploration of Space (1951), and The Promise of Space (1968). In recognition of these contributions, the geostationary orbit 36,000 kilometres (22,000\xa0mi) above the equator is officially recognised by the International Astronomical Union as the Clarke Orbit.[28]', 'His 1951 book, The Exploration of Space, was used by the rocket pioneer Wernher von Braun to convince President John F. Kennedy that it was possible to go to the Moon.[29]', 'Following the 1968 release of 2001, Clarke became much in demand as a commentator on science and technology, especially at the time of the Apollo space program. On 20 July 1969, Clarke appeared as a commentator for the CBS News broadcast of the Apollo 11 Moon landing.[30][31]', 'Clarke lived in Sri Lanka from 1956 until his death in 2008, first in Unawatuna on the south coast, and then in Colombo.[32] Initially, he and his friend Mike Wilson travelled around Sri Lanka, diving in the coral waters around the coast with the Beachcombers Club. In 1957, during a dive trip off Trincomalee, Clarke discovered the underwater ruins of a temple, which subsequently made the region popular with divers.[33] He described it in his 1957 book The Reefs of Taprobane. This was his second diving book after the 1956 The Coast of Coral.[34] Though Clarke lived mostly in Colombo, he set up a small dive school and a simple dive shop near Trincomalee. He dived often at Hikkaduwa, Trincomalee, and Nilaveli.[35]', 'The Sri Lankan government offered Clarke resident guest status in 1975.[36] He was held in such high esteem that when fellow science-fiction writer Robert A. Heinlein came to visit, the Sri Lanka Air Force provided a helicopter to take them around the country.[37] In the early 1970s, Clarke signed a three-book publishing deal, a record for a science-fiction writer at the time. The first of the three was Rendezvous with Rama in 1973, which won all the main genre awards[38] and spawned sequels that along with the 2001 series formed the backbone of his later career.', 'In 1986, Clarke was named a Grand Master by the Science Fiction Writers of America.[39]', 'In 1988, he was diagnosed with post-polio syndrome, having originally contracted polio in 1962, and needed to use a wheelchair most of the time thereafter.[32] Clarke was for many years a vice-patron of the British Polio Fellowship.[40]', 'In the 1989 Queen\'s Birthday Honours, Clarke was appointed Commander of the Order of the British Empire (CBE) "for services to British cultural interests in Sri Lanka".[12] The same year, he became the first chancellor of the International Space University, serving from 1989 to 2004. He also served as chancellor of Moratuwa University in Sri Lanka from 1979 to 2002.', 'In 1994, Clarke appeared in a science-fiction film; he portrayed himself in the telefilm Without Warning, an American production about an apocalyptic alien first-contact scenario presented in the form of a faux newscast.', 'Clarke also became active in promoting the protection of gorillas and became a patron of the Gorilla Organization, which fights for the preservation of gorillas.[41] When tantalum mining for mobile phone manufacture threatened the gorillas in 2001, he lent his voice to their cause.[42] The dive shop that he set up continues to operate from Trincomalee through the Arthur C Clarke Foundation.[43]', "In the 1980s and early 1990s, Clarke presented his television programmes Arthur C. Clarke's Mysterious World, Arthur C. Clarke's World of Strange Powers, and Arthur C. Clarke's Mysterious Universe.", 'On a trip to Florida in 1953,[1] Clarke met and quickly married Marilyn Mayfield, a 22-year-old American divorcee with a young son. They separated permanently after six months, although the divorce was not finalised until 1964.[44] "The marriage was incompatible from the beginning," said Clarke.[44] Marilyn never remarried and died in 1991. Clarke himself also never remarried, but was close to a Sri Lankan man, Leslie Ekanayake (13 July 1947\xa0– 4\xa0July 1977), whom Clarke called his "only perfect friend of a lifetime" in the dedication to his novel The Fountains of Paradise.[a] Clarke is buried with Ekanayake, who predeceased him by three decades, in Colombo\'s central cemetery.[45] In his biography of Stanley Kubrick, John Baxter cites Clarke\'s homosexuality as a reason why he relocated, due to more tolerant laws with regard to homosexuality in Sri Lanka.[46] In 1998, the Sunday Mirror reported that he paid Sri Lankan boys for sex, leading to the cancellation of plans for Prince Charles to knight him on a visit to the country.[47][48] The accusation was subsequently found to be baseless by the Sri Lankan police.[49][50] Journalists who enquired of Clarke whether he was gay were told, "No, merely mildly cheerful."[32] However, Michael Moorcock wrote:', "Everyone knew he was gay. In the 1950s, I'd go out drinking with his boyfriend. We met his protégés, western and eastern, and their families, people who had only the most generous praise for his kindness. Self-absorbed he might be and a teetotaller, but an impeccable gent through and through.[51]", 'In an interview in the July 1986 issue of Playboy magazine, when asked if he had had a bisexual experience, Clarke stated, "Of course. Who hasn\'t?"[52] In his obituary, Clarke\'s friend Kerry O\'Quinn wrote: "Yes, Arthur was gay\xa0... As Isaac Asimov once told me, \'I think he simply found he preferred men.\' Arthur didn\'t publicise his sexuality\xa0– that wasn\'t the focus of his life\xa0– but if asked, he was open and honest."[53]', 'Clarke accumulated a vast collection of manuscripts and personal memoirs, maintained by his brother Fred Clarke in Taunton, Somerset, England, and referred to as the "Clarkives". Clarke said some of his private diaries will not be published until 30 years after his death. When asked why they were sealed, he answered, "Well, there might be all sorts of embarrassing things in them."[3]', 'On 26 May 2000, he was made a Knight Bachelor "for services to literature" at a ceremony in Colombo.[14][b][54] The award of a knighthood had been announced in the 1998 New Year Honours list,[13][55] but investiture with the award had been delayed, at Clarke\'s request, because of an accusation by the British tabloid the Sunday Mirror of paying boys for sex.[56][57] The charge was subsequently found to be baseless by the Sri Lankan police.[49][50] According to The Daily Telegraph, the Mirror subsequently published an apology, and Clarke chose not to sue for defamation.[58] The Independent reported that a similar story was not published, allegedly because Clarke was a friend of newspaper tycoon Rupert Murdoch.[59] Clarke himself said, "I take an extremely dim view of people mucking about with boys", and Rupert Murdoch promised him the reporters responsible would never work in Fleet Street again.[60] Clarke was then duly knighted.', 'Although he and his home were unharmed by the 2004 Indian Ocean earthquake tsunami, his "Arthur C. Clarke Diving School" (now called "Underwater Safaris")[61] at Hikkaduwa near Galle was destroyed.[62] He made humanitarian appeals, and the Arthur C. Clarke Foundation worked towards better disaster notification systems.[63]', "Because of his post-polio deficits, which limited his ability to travel and gave him halting speech, most of Clarke's communications in his last years were in the form of recorded addresses. In July 2007, he provided a video address for the Robert A. Heinlein Centennial in which he closed his comments with a goodbye to his fans. In September 2007, he provided a video greeting for NASA's Cassini probe's flyby of Iapetus (which plays an important role in the book of 2001: A Space Odyssey).[64] In December 2007 on his 90th birthday, Clarke recorded a video message to his friends and fans bidding them good-bye.[65]", 'Clarke died in Colombo on 19 March 2008, at the age of 90.[32][66][67][68] His aide described the cause as respiratory complications and heart failure stemming from post-polio syndrome.[69]', 'Just hours before Clarke\'s death, a major gamma-ray burst (GRB) reached Earth. Known as GRB 080319B, the burst set a new record as the farthest object that can be seen from Earth with the naked eye.[70] It occurred about 7.5\xa0billion years ago, the light taking that long to reach Earth.[70] Larry Sessions, a science writer for Sky and Telescope magazine blogging on earthsky.org, suggested that the burst be named the "Clarke Event".[71][72] American Atheist Magazine wrote of the idea: "It would be a fitting tribute to a man who contributed so much, and helped lift our eyes and our minds to a cosmos once thought to be province only of gods."[73]', "A few days before he died, he had reviewed the manuscript of his final work, The Last Theorem, on which he had collaborated by e-mail with contemporary Frederik Pohl.[74] The book was published after Clarke's death.[75] Clarke was buried alongside his partner, Leslie Ekanayake, in Colombo in traditional Sri Lankan fashion on 22 March. His younger brother, Fred Clarke, and his Sri Lankan adoptive family were among the thousands in attendance.[76]", "Clarke's papers were donated to the National Air and Space Museum in 2014.[77][78]", 'While Clarke had a few stories published in fanzines, between 1937 and 1945, his first professional sale appeared in Astounding Science Fiction in 1946: "Loophole" was published in April, while "Rescue Party", his first sale, was published in May.[c] Along with his writing, Clarke briefly worked as assistant editor of Science Abstracts (1949) before devoting himself in 1951 to full-time writing.', 'Clarke began carving out his reputation as a "scientific" science-fiction writer with his first science-fiction novel, Against the Fall of Night, published as a novella in 1948. It was very popular and considered ground-breaking work for some of the concepts it contained. Clarke revised and expanded the novella into a full novel, which was published in 1953. Clarke later rewrote and expanded this work a third time to become The City and the Stars in 1956, which rapidly became a definitive must-read in the field. His third science-fiction novel, Childhood\'s End, was also published in 1953, cementing his popularity. Clarke capped the first phase of his writing career with his sixth novel, A Fall of Moondust, in 1961, which is also an acknowledged classic of the period.', 'During this time, Clarke corresponded with C. S. Lewis in the 1940s and 1950s and they once met in an Oxford pub, the Eastgate, to discuss science fiction and space travel. Clarke voiced great praise for Lewis upon his death, saying the Ransom trilogy was one of the few works of science fiction that should be considered literature.[79]', 'In 1948, he wrote "The Sentinel" for a BBC competition. Though the story was rejected, it changed the course of Clarke\'s career. Not only was it the basis for 2001: A Space Odyssey, but "The Sentinel" also introduced a more cosmic element to Clarke\'s work. Many of Clarke\'s later works feature a technologically advanced but still-prejudiced mankind being confronted by a superior alien intelligence. In the cases of Childhood\'s End, and the 2001 series, this encounter produces a conceptual breakthrough that accelerates humanity into the next stage of its evolution. This also applies in the far-distant past (but our future) in The City and the Stars (and its original version, Against the Fall of Night).', 'In Clarke\'s authorised biography, Neil McAleer writes: "many readers and critics still consider Childhood\'s End Arthur C. Clarke\'s best novel."[44] But Clarke did not use ESP in any of his later stories, saying, "I\'ve always been interested in ESP, and of course, Childhood\'s End was about that. But I\'ve grown disillusioned, partly because after all this time, they\'re still arguing about whether these things happen. I suspect that telepathy does happen."[80]', 'A collection of early essays was published in The View from Serendip (1977), which also included one short piece of fiction, "When the Twerms Came". Clarke also wrote short stories under the pseudonyms of E.\xa0G.\xa0O\'Brien and Charles Willis.[81] Almost all of his short stories can be found in the book The Collected Stories of Arthur C. Clarke (2001).', 'For much of the later 20th century, Clarke, Isaac Asimov, and Robert A. Heinlein were informally known as the "Big Three" of science-fiction writers.[7] Clarke and Heinlein began writing to each other after The Exploration of Space was published in 1951, and first met in person the following year. They remained on cordial terms for many years, including during visits to the United States and Sri Lanka.', 'Clarke and Asimov first met in New York City in 1953, and they traded friendly insults and gibes for decades. They established an oral agreement, the "Clarke–Asimov Treaty", that when asked who was better, the two would say Clarke was the better science-fiction writer and Asimov was the better science writer. In 1972, Clarke put the "treaty" on paper in his dedication to Report on Planet Three and Other Speculations.[44][82]', "In 1984, Clarke testified before Congress against the Strategic Defense Initiative (SDI).[83] Later, at the home of Larry Niven in California, a concerned Heinlein attacked Clarke's views on United States foreign and space policy (especially the SDI), vigorously advocating a strong defence posture. Although the two later reconciled formally, they remained distant until Heinlein's death in 1988.[44]", "2001: A Space Odyssey, Clarke's most famous work, was extended well beyond the 1968 movie as the Space Odyssey series. In 1982, Clarke wrote a sequel to 2001 titled 2010: Odyssey Two, which was made into a film in 1984. Clarke wrote two further sequels which have not been adapted into motion pictures: 2061: Odyssey Three (published in 1987) and 3001: The Final Odyssey (published in 1997).", '2061: Odyssey Three involves a visit to Halley\'s Comet on its next plunge through the Inner Solar System and a spaceship crash on the Jovian moon Europa. The whereabouts of astronaut Dave Bowman (the "Star Child"), the artificial intelligence HAL 9000, and the development of native life on Europa, protected by the alien Monolith, are revealed.', "Finally, in 3001: The Final Odyssey, astronaut Frank Poole's freeze-dried body, found by a spaceship beyond the orbit of Neptune, is revived by advanced medical science. The novel details the threat posed to humanity by the alien monoliths, whose actions are not always as their builders had intended.", 'Clarke\'s first venture into film was 2001: A Space Odyssey, directed by Stanley Kubrick. Kubrick and Clarke had met in New York City in 1964 to discuss the possibility of a collaborative film project. As the idea developed, they decided to loosely base the story on Clarke\'s short story, "The Sentinel", written in 1948 as an entry in a BBC short-story competition. Originally, Clarke was going to write the screenplay for the film, but Kubrick suggested during one of their brainstorming meetings that before beginning on the actual script, they should let their imaginations soar free by writing a novel first, on which they would base the film. "This is more or less the way it worked out, though toward the end, novel and screenplay were being written simultaneously, with feedback in both directions. Thus, I rewrote some sections after seeing the movie rushes\xa0– a rather expensive method of literary creation, which few other authors can have enjoyed."[84] The novel ended up being published a few months after the release of the movie.', 'Due to the hectic schedule of the film\'s production, Kubrick and Clarke had difficulty collaborating on the book. Clarke completed a draft of the novel at the end of 1964 with the plan to publish in 1965 in advance of the film\'s release in 1966. After many delays, the film was released in the spring of 1968, before the book was completed. The book was credited to Clarke alone. Clarke later complained that this had the effect of making the book into a novelisation, and that Kubrick had manipulated circumstances to downplay Clarke\'s authorship. For these and other reasons, the details of the story differ slightly from the book to the movie. The film contains little explanation for the events taking place. Clarke, though, wrote thorough explanations of "cause and effect" for the events in the novel. James Randi later recounted that upon seeing the premiere of 2001, Clarke left the theatre at the intermission in tears, after having watched an eleven-minute scene (which did not make it into general release) where an astronaut is doing nothing more than jogging inside the spaceship, which was Kubrick\'s idea of showing the audience how boring space travels could be.[85]', 'In 1972, Clarke published The Lost Worlds of 2001, which included his accounts of the production, and alternative versions of key scenes. The "special edition" of the novel A Space Odyssey (released in 1999) contains an introduction by Clarke in which he documents the events leading to the release of the novel and film.', 'In 1982, Clarke continued the 2001 epic with a sequel, 2010: Odyssey Two. This novel was also made into a film, 2010, directed by Peter Hyams for release in 1984. Because of the political environment in America in the 1980s, the film presents a Cold War theme, with the looming tensions of nuclear warfare not featured in the novel. The film was not considered to be as revolutionary or artistic as 2001, but the reviews were still positive.', "Clarke's email correspondence with Hyams was published in 1984.[86] Titled The Odyssey File: The Making of 2010, and co-authored with Hyams, it illustrates his fascination with the then-pioneering medium of email and its use for them to communicate on an almost daily basis at the time of planning and production of the film while living on opposite sides of the world. The book also included Clarke's personal list of the best science-fiction films ever made.", "Clarke appeared in the film, first as the man feeding the pigeons while Dr. Heywood Floyd is engaged in a conversation in front of the White House. Later, in the hospital scene with David Bowman's mother, an image of the cover of Time portrays Clarke as the American President and Kubrick as the Soviet Premier.", 'Clarke\'s award-winning novel Rendezvous with Rama (1973) was optioned for filmmaking in the early 21st century[87][88] but this motion picture was in "development hell" as of 2014[update]. In the early 2000s, actor Morgan Freeman expressed his desire to produce a movie based on Rendezvous with Rama. After a drawn-out development process, which Freeman attributed to difficulties in getting financing, it appeared in 2003 that this project might be proceeding, but this was very dubious.[87] The film was to be produced by Freeman\'s production company, Revelations Entertainment, and David Fincher has been touted on Revelations\' Rama web page as far back as 2001 as the film\'s director.[88] After years of no progress, Fincher stated in an interview in late 2007 (in which he also opined the novel as being influential on the films Alien and Star Trek: The Motion Picture) that he is still attached to helm.[89] Revelations indicated that Stel Pavlou had written the adaptation.', 'In late 2008, Fincher stated the movie is unlikely to be made. "It looks like it\'s not going to happen. There\'s no script and as you know, Morgan Freeman\'s not in the best of health right now. We\'ve been trying to do it but it\'s probably not going to happen."[90] In 2010, though, the film was announced as still planned for future production and both Freeman and Fincher mentioned it as still needing a worthy script.[91]', "In late 2021, it was announced that Denis Villeneuve would direct the adaptation of Rendezvous with Rama, following the successful and critically praised release of Villeneuve's adaption of Frank Herbert's Dune. Freeman is listed as a producer.[92]", 'Clarke published a number of nonfiction books with essays, speeches, addresses, etc. Several of his nonfiction books are composed of chapters that can stand on their own as separate essays.', 'In particular, Clarke was a populariser of the concept of space travel. In 1950, he wrote Interplanetary Flight, a book outlining the basics of space flight for laymen. Later books about space travel included The Exploration of Space (1951), The Challenge of the Spaceship (1959), Voices from the Sky (1965), The Promise of Space (1968, rev. ed. 1970), and Report on Planet Three (1972) along with many others.', "His books on space travel usually included chapters about other aspects of science and technology, such as computers and bioengineering. He predicted telecommunication satellites (albeit serviced by astronauts in space suits, who would replace the satellite's vacuum tubes as they burned out).[93]", 'His many predictions culminated in 1958 when he began a series of magazine essays which eventually became Profiles of the Future, published in book form in 1962.[94] A timetable[95] up to the year 2100 describes inventions and ideas including such things as a "global library" for 2005. The same work also contained "Clarke\'s First Law" and text that became Clarke\'s three laws in later editions.[44]', 'In a 1959 essay, Clarke predicted global satellite TV broadcasts that would cross national boundaries indiscriminately and would bring hundreds of channels available anywhere in the world. He also envisioned a "personal transceiver, so small and compact that every man carries one". He wrote: "the time will come when we will be able to call a person anywhere on Earth merely by dialing a number." Such a device would also, in Clarke\'s vision, include means for global positioning so "no one need ever again be lost". Later, in Profiles of the Future, he predicted the advent of such a device taking place in the mid-1980s.[94] ', " Clarke described a global computer network similar to the modern World Wide Web in a 1964 presentation for the BBC's Horizon programme, predicting that, by the 21st century, access to information and even physical tasks such as surgery could be accomplished remotely and instantaneously from anywhere in the world using internet and satellite communication.[96]", 'In a 1974 interview with the Australian Broadcasting Corporation, the interviewer asked Clarke how he believed the computer would change the future for the everyday person, and what life would be like in the year 2001. Clarke accurately predicted many things that became reality, including online banking, online shopping, and other now commonplace things. Responding to a question about how the interviewer\'s son\'s life would be different, Clarke responded: "He will have, in his own house, not a computer as big as this, [points to nearby computer], but at least, a console through which he can talk, through his friendly local computer and get all the information he needs, for his everyday life, like his bank statements, his theatre reservations, all the information you need in the course of living in our complex modern society, this will be in a compact form in his own house\xa0... and he will take it as much for granted as we take the telephone."[97]', "An extensive selection of Clarke's essays and book chapters (from 1934 to 1998; 110 pieces, 63 of them previously uncollected in his books) can be found in the book Greetings, Carbon-Based Bipeds! (2000), together with a new introduction and many prefatory notes. Another collection of essays, all previously collected, is By Space Possessed (1993). Clarke's technical papers, together with several essays and extensive autobiographical material, are collected in Ascent to Orbit: A Scientific Autobiography (1984).", 'Clarke contributed to the popularity of the idea that geostationary satellites would be ideal telecommunications relays. He first described this in a letter to the editor of Wireless World in February 1945[98] and elaborated on the concept in a paper titled Extra-Terrestrial Relays\xa0– Can Rocket Stations Give Worldwide Radio Coverage?, published in Wireless World in October 1945.[8] The geostationary orbit is sometimes known as the Clarke Orbit or the Clarke Belt in his honour.[99][100][101]', 'It is not clear that this article was actually the inspiration for the modern telecommunications satellite. According to John R. Pierce, of Bell Labs, who was involved in the Echo satellite and Telstar projects, he gave a talk upon the subject in 1954 (published in 1955), using ideas that were "in the air", but was not aware of Clarke\'s article at the time.[102] In an interview given shortly before his death, Clarke was asked whether he had ever suspected that one day communications satellites would become so important; he replied: "I\'m often asked why I didn\'t try to patent the idea of a communications satellite. My answer is always, \'A patent is really a licence to be sued.\'"[103]', "Though different from Clarke's idea of telecom relay, the idea of communicating via satellites in geostationary orbit itself had been described earlier. For example, the concept of geostationary satellites was described in Hermann Oberth's 1923 book Die Rakete zu den Planetenräumen (The Rocket into Interplanetary Space), and then the idea of radio communication by means of those satellites in Herman Potočnik's (written under the pseudonym Hermann Noordung) 1928 book Das Problem der Befahrung des Weltraums\xa0– der Raketen-Motor (The Problem of Space Travel\xa0– The Rocket Motor), sections: Providing for Long Distance Communications and Safety,[d] and (possibly referring to the idea of relaying messages via satellite, but not that three would be optimal) Observing and Researching the Earth's Surface, published in Berlin.[104][e] Clarke acknowledged the earlier concept in his book Profiles of the Future.[f]", "Clarke was an avid scuba diver and a member of the Underwater Explorers Club. In addition to writing, Clarke set up several diving-related ventures with his business partner Mike Wilson. In 1956, while scuba diving, Wilson and Clarke uncovered ruined masonry, architecture, and idol images of the sunken original Koneswaram temple\xa0– including carved columns with flower insignia, and stones in the form of elephant heads\xa0– spread on the shallow surrounding seabed.[105][106] Other discoveries included Chola bronzes from the original shrine, and these discoveries were described in Clarke's 1957 book The Reefs of Taprobane.[107]", 'In 1961, while filming off Great Basses Reef, Wilson found a wreck and retrieved silver coins. Plans to dive on the wreck the following year were stopped when Clarke developed paralysis, ultimately diagnosed as polio. A year later, Clarke observed the salvage from the shore and the surface. The ship, ultimately identified as belonging to the Mughal Emperor, Aurangzeb, yielded fused bags of silver rupees, cannon, and other artefacts, carefully documented, became the basis for The Treasure of the Great Reef.[44][108] Living in Sri Lanka and learning its history also inspired the backdrop for his novel The Fountains of Paradise in which he described a space elevator. This, he believed, would make rocket-based access to space obsolete, and more than geostationary satellites, would ultimately be his scientific legacy.[109] In 2008, he said in an interview with IEEE Spectrum, "maybe in a generation or so the space elevator will be considered equally important" as the geostationary satellite, which was his most important technological contribution.[110]', 'Themes of religion and spirituality appear in much of Clarke\'s writing. He said: "Any path to knowledge is a path to God\xa0– or Reality, whichever word one prefers to use."[111] He described himself as "fascinated by the concept of God". J.\xa0B.\xa0S. Haldane, near the end of his life, suggested in a personal letter to Clarke that Clarke should receive a prize in theology for being one of the few people to write anything new on the subject, and went on to say that if Clarke\'s writings had not contained multiple contradictory theological views, he might have been a menace.[112] When he entered the Royal Air Force, Clarke insisted that his dog tags be marked "pantheist" rather than the default, Church of England,[44] and in a 1991 essay entitled "Credo", described himself as a logical positivist from the age of 10.[112] In 2000, Clarke told the Sri Lankan newspaper, The Island, "I don\'t believe in God or an afterlife,"[113] and he identified himself as an atheist.[114] He was honoured as a Humanist Laureate in the International Academy of Humanism.[115] He has also described himself as a "crypto-Buddhist", insisting Buddhism is not a religion.[116] He displayed little interest about religion early in his life, for example, only discovering a few months after marrying that his wife had strong Presbyterian beliefs[citation needed].', 'A famous quotation of Clarke\'s is often cited: "One of the great tragedies of mankind is that morality has been hijacked by religion."[116] He was quoted in Popular Science in 2004 as saying of religion: "Most malevolent and persistent of all mind viruses. We should get rid of it as quick as we can."[117] In a three-day "dialogue on man and his world" with Alan Watts, Clarke said he was biased against religion and could not forgive religions for what he perceived as their inability to prevent atrocities and wars over time.[118] In his introduction to the penultimate episode of Mysterious World, entitled "Strange Skies", Clarke said: "I sometimes think that the universe is a machine designed for the perpetual astonishment of astronomers," reflecting the dialogue of the episode, in which he stated this concept more broadly, referring to "mankind". Near the very end of that same episode, the last segment of which covered the Star of Bethlehem, he said his favourite theory[119] was that it might be a pulsar. Given that pulsars were discovered in the interval between his writing the short story, "The Star" (1955), and making Mysterious World (1980), and given the more recent discovery of pulsar PSR B1913+16, he said: "How romantic, if even now, we can hear the dying voice of a star, which heralded the Christian era."[119]', "Despite his atheism, themes of deism are a common feature within Clarke's work.[120][121]", 'Clarke left written instructions for a funeral: "Absolutely no religious rites of any kind, relating to any religious faith, should be associated with my funeral."[122]', 'Regarding freedom of information Clarke believed, "In the struggle for freedom of information, technology, not politics, will be the ultimate decider."[123]', 'Clarke also wrote, "It is not easy to see how the more extreme forms of nationalism can long survive when men have seen the Earth in its true perspective as a single small globe against the stars."[123] Clarke opposed claims of sovereignty over space stating "There is hopeful symbolism in the fact that flags do not wave in a vacuum."[123]', 'Clarke was an anti-capitalist, stating that he did not fear automation because, "the goal of the future is full unemployment, so we can play. That\'s why we have to destroy the present politico-economic system."[124]', 'Regarding human jobs being replaced by robots, Clarke said: "Any teacher that can be replaced by a machine should be!"[123]', 'Clarke supported the use of renewable energy, saying: "I would like to see us kick our current addiction to oil, and adopt clean energy sources\xa0... Climate change has now added a new sense of urgency. Our civilisation depends on energy, but we can\'t allow oil and coal to slowly bake our planet."[123]', 'Clarke believed: ', "The best proof that there's intelligent life in outer space is the fact that it hasn't come here\xa0... the fact that we have not yet found the slightest evidence for life—much less intelligence—beyond this Earth does not surprise or disappoint me in the least. Our technology must still be laughably primitive; we may well be like jungle savages listening for the throbbing of tom-toms, while the ether around them carries more words per second than they could utter in a lifetime.[123] Two possibilities exist: either we are alone in the Universe or we are not... Both are equally terrifying.[123]", 'Early in his career, Clarke had a fascination with the paranormal and said it was part of the inspiration for his novel Childhood\'s End. Citing the numerous promising paranormal claims that were later shown to be fraudulent, Clarke described his earlier openness to the paranormal having turned to being "an almost total sceptic" by the time of his 1992 biography.[44] Similarly, in the prologue to the 1990 Del Rey edition of Childhood\'s End, he writes "...after ... researching my Mysterious World and Strange Powers programmes, I am an almost total skeptic. I have seen far too many claims dissolve into thin air, far too many demonstrations exposed as fakes. It has been a long, and sometimes embarrassing, learning process."[125] During interviews, both in 1993 and 2004–2005, he stated that he did not believe in reincarnation, saying there was no mechanism to make it possible, though "I\'m always paraphrasing J. B. S. Haldane: \'The universe is not only stranger than we imagine, it\'s stranger than we can imagine.\'"[126][127] He described the idea of reincarnation as fascinating, but favoured a finite existence.[128]', "Clarke was known for hosting several television series investigating the unusual: Arthur C. Clarke's Mysterious World (1980), Arthur C. Clarke's World of Strange Power (1985), and Arthur C. Clarke's Mysterious Universe (1994). Topics examined ranged from ancient, man-made artifacts with obscure origins (e.g., the Nazca lines or Stonehenge), to cryptids (purported animals unknown to science), or obsolete scientific theories that came to have alternate explanations (e.g., Martian canals).", 'In Arthur C. Clarke\'s Mysterious World, he describes three kinds of "mysteries":', "Clarke's programmes on unusual phenomena were parodied in a 1982 episode of the comedy series The Goodies, in which his show is cancelled after it is claimed that he does not exist.", "Clarke's work is marked by an optimistic view of science empowering mankind's exploration of the Solar System and the world's oceans. His images of the future often feature a Utopian setting with highly developed technology, ecology, and society, based on the author's ideals.[130] His early published stories usually featured the extrapolation of a technological innovation or scientific breakthrough into the underlying decadence of his own society.", 'A recurring theme in Clarke\'s works is the notion that the evolution of an intelligent species would eventually make them something close to gods. This was explored in his 1953 novel Childhood\'s End and briefly touched upon in his novel Imperial Earth. This idea of transcendence through evolution seems to have been influenced by Olaf Stapledon, who wrote a number of books dealing with this theme. Clarke has said of Stapledon\'s 1930 book Last and First Men that "No other book had a greater influence on my life\xa0... [It] and its successor Star Maker (1937) are the twin summits of [Stapledon\'s] literary career."[131]', 'Clarke was also well known as an admirer of Irish fantasy writer Lord Dunsany, also having corresponded with him until Dunsany\'s death in 1957. He described Dunsany as "one of the greatest writers of the century".[132]', 'He also listed H. G. Wells, Jules Verne, and Edgar Rice Burroughs as influences.[32]', 'Clarke won the 1963 Stuart Ballantine Medal from the Franklin Institute for the concept of satellite communications,[133][134] and other honours.[135] He won more than a dozen annual literary awards for particular works of science fiction.[38]', 'In 1986, Clarke provided a grant to fund the prize money (initially £1,000) for the Arthur C. Clarke Award for the best science fiction novel published in the United Kingdom in the previous year. In 2001 the prize was increased to £2001, and its value now matches the year (e.g., £2005 in 2005).', 'In 2005 he lent his name to the inaugural Sir Arthur Clarke Awards\xa0– dubbed the "Space Oscars". His brother attended the awards ceremony, and presented an award specially chosen by Arthur (and not by the panel of judges who chose the other awards) to the British Interplanetary Society.']}, {'headings': ['Discovery One'], 'subheadings': ['Contents', 'Development', 'Fictional history', 'Notes', 'References', 'Navigation menu'], 'paras': ['The United States Spacecraft Discovery One is a fictional spaceship featured in the first two novels of the Space Odyssey series by Arthur C. Clarke and in the films 2001: A Space Odyssey (1968) directed by Stanley Kubrick and 2010: The Year We Make Contact (1984) directed by Peter Hyams. The ship is a nuclear-powered interplanetary spaceship, crewed by two men and controlled by the AI on-board computer HAL 9000.  The ship is destroyed in the second novel and makes no further appearances.', "Clarke and Kubrick developed the first novel and film in parallel, but there were some differences to suit the different media.  Kubrick dropped the cooling fins of the ship, fearing they would be interpreted as wings.  The itinerary of Discovery One in the book is from Earth orbit via gravitational slingshot around Jupiter to Saturn and parking orbit around the moon Iapetus.  Kubrick changed this to the simpler route from Earth to Jupiter's moon Europa.", 'For the film, Kubrick built an exceptionally large model of the ship so that focus changes did not give away the true small size to the audience.  He also built a large, expensive, rotating carousel for the artificial gravity scenes.', 'The spaceship first appears in the novel 2001: A Space Odyssey by science fiction author Arthur C. Clarke and the film of the same name produced and directed by Stanley Kubrick. The book and the film were developed in parallel in a collaboration between Clarke and Kubrick.  Despite this, the novelized and filmed appearances of the craft differ.  Clarke based the design on ideas that were, or he believed were, scientifically feasible.  He gave the ship a hypothetical thermonuclear propulsion system and added huge cooling fins to radiate away the excess heat produced. In the book, Clarke says the fins "looked like the wings of some vast dragonfly" and that they gave the ship a "fleeting resemblance to an old-time sailing-ship". In the film, Kubrick removed the fins because he thought that the audience might interpret them as wings giving the spacecraft the ability to fly through an atmosphere.[citation needed]', 'Early in the development of the movie, Clarke and Kubrick considered having Discovery powered by an Orion type nuclear pulse propulsion system. Kubrick quickly decided against it, both because showing the ship accelerate by a \'putt-putt\' method might be "too comic" for film, and because it might be seen as him having embraced nuclear weapons after his previous film, Dr. Strangelove.[1]', 'Kubrick spent $750,000, a large portion of his $6 million budget,[2] on the set for the artificial gravity scenes in the carousel. The set was a vertically-mounted  30-short-ton (27\xa0t) circular set 38 feet (12\xa0m) in diameter and 10 feet (3.0\xa0m) wide.[3] The entire set could rotate around its axis at up to 3 miles per hour (4.8\xa0km/h).[4] The rim of the carousel would move slowly enough to allow the actors to walk around with it as if they were in a hamster wheel. This created the impression that the actors were walking up the walls of the set, while in fact, the actors remained at the bottom. The same technique was used for the Aries Moon shuttle scenes. This was not an entirely new idea; in the 1951 Royal Wedding a similar arrangement allowed Fred Astaire to apparently dance up the walls and along the ceiling of his hotel room.[5]', 'Clarke believed that the ability to transfer between zero-g and artificial gravity areas of a spaceship would be easily learnt by astronauts, and this is how Kubrick portrayed it in the film. However, expert opinion is that this would be somewhat more difficult to achieve, particularly due to the Coriolis force.[6] Long-radius centrifuge experiments by the Naval Medical Research Laboratory starting in 1958 kept subjects in a 30-foot (9.1\xa0m) diameter centrifuge complete with living quarters for up to three weeks. The experiments found that the subjects took three to four days to overcome motion sickness and balance issues.[7]', 'Two models for filming were made, one 15-foot (4.6\xa0m) long and one 54-foot (16\xa0m) long. The scale of the models, compared to many other productions, was unusually large. This was due to the need to keep the whole ship in focus for the shots, something which could not be done on smaller or tabletop models. With a smaller model, the camera needs to be brought in closer, and the change in focus across the model would betray the true size of the object.[8]', 'Following the completion of the film, Kubrick ordered both the models and the plans for constructing them destroyed, so that they could not be used on future productions. This presented a problem during preproduction for its 1984 sequel, 2010: The Year We Make Contact. The filmmakers were forced to refer to frame blowups of the Discovery from different angles taken from a 70mm print of 2001 in order to construct a new model.[citation needed]', "In addition, a model of the ship's head and a few body segments were used for close-up shots of Discovery docked with the Leonov.", 'A 12-foot model was used for the primary motion control shots, while a smaller one was used to depict the Discovery tumbling end over end over Io.', 'Because of the lack of aerodynamic design and its immense size, Discovery One was assembled in and launched from orbit. As described in the novel, it was originally intended to survey the Jovian system, but its mission was changed to go to Saturn and investigate the destination of the signal from the black monolith found at the crater Tycho. As a result, the mission became a one-way trip to Saturn and its moon Japetus. (In the filmed telling, the destination remains Jupiter.) After investigating alien artifacts at Saturn and Iapetus, the preliminary plan is for all five members of the crew to enter suspended animation for an indefinite period of time. Eventually, it was intended that the much larger and more powerful Discovery Two (not yet completed) would travel to Japetus and return with everyone in hibernation.', 'In the novel 2001: A Space Odyssey, Discovery One is described as being "almost 400\xa0feet long with a sphere 40\xa0feet dia." (122\xa0meters and 12.2\xa0meters respectively; the 2010 film mentions 800 feet (240\xa0m)) and powered by a nuclear plasma drive, separated by 275 feet (84\xa0m) of tankage and structure, from the spherical part of the spaceship where the crew quarters, the computer, flight controls, small auxiliary craft, and instrumentation are located.', "The ship's carousel is a spinning band of deck, mounted inside the crew compartment, using centrifugal force to simulate the effects of gravity and is the primary living and work area. The three hibernating astronauts are also located here. The carousel provides Moon-level gravity rotating at just over 5\xa0rpm.[9] The carousel can be stopped and the rotation stored in a flywheel.[10] There is an automated kitchen (developed with the assistance of General Mills); a ship-to-Earth communications center; and a complete medical section where the astronauts undergo regular automated checkups.", "Areas outside the carousel, are micro-g environments where the crew members use Velcro shoes to attach themselves to the floor. Piloting, navigation, and other tasks take place in these areas. There is also a pod bay, where three one-man repair and inspection craft are kept, and the spaceship's primary HAL 9000 mainframe computer.", 'Discovery is described as a very large ship that could be handled by only two astronauts (David Bowman and Frank Poole), working 12 hour alternating shifts, along with the HAL 9000. In the book, IBM predicted that computer development would be advanced to such an extent that the mission could be undertaken with all the astronauts placed in hibernation. It was said to be desired, however, that regular communications be maintained throughout the voyage between the pilot and copilot and mission control back on Earth. During communication, an account is taken of the elapsed time for electromagnetic waves crossing space between the spaceship and the Earth. For example, Poole is depicted watching a pre-recorded birthday message from his family, rather than interacting with them in real-time. Such a conversation is not possible because messages take anywhere from 30 to 52 minutes to transmit between Jupiter and Earth. Naturally, this time would depend on the relative positions of the bodies in the Solar System at any given moment.[11]', "After the malfunction of HAL, Bowman deactivated the computer, thus effectively isolating himself on Discovery. In the movie, when the spacecraft arrives at Jupiter, it encounters TMA-1's considerably larger 'Big Brother', 'TMA-2', at the Jupiter/Io L1 point. The novel is basically the same with Discovery in orbit around Saturn's moon Iapetus instead. In both versions, Bowman leaves Discovery to examine the monolith and is taken inside it. The novel and movie 2010: Odyssey Two follows the 2001: A Space Odyssey movie ending rather than the novel.", "After finding out that Discovery's orbit is failing, a joint Soviet-US mission (including Heywood Floyd) travels to Jupiter aboard the spacecraft Alexei Leonov to intercept and board Discovery believing that it harbours many of the answers to the mysteries surrounding the 2001 mission. Leonov docks with Discovery, reactivates the on-board systems, and stabilizes its orbit. Hal's creator, Dr. Chandra, is sent to reactivate the HAL 9000 computer and gather any data he can regarding the previous mission.", 'Later on, an apparition of Dave Bowman appears, warning Floyd that Leonov must leave Jupiter within two days. Floyd asks what will happen at that time, and Bowman replies, \'Something wonderful\'. Floyd has difficulty convincing the rest of the crew, at first, but a dark spot on Jupiter begins to form and starts growing. HAL\'s telescope reveals that the "Great Black Spot" is, in fact, a vast population of monoliths increasing at a geometric rate. (The film accelerates the pace from the novel, both shortening Bowman\'s deadline from fifteen days, and making the spot grow faster.)', "Initially, it was planned to inject Discovery on an Earth-bound trajectory (though it would not arrive for some years); however, when faced with Bowman's warning, the Leonov crew devises a plan to use Discovery as a 'booster rocket', enabling them to return to Earth ahead of schedule, but leaving Discovery in an elliptical orbit of Jupiter. The crew worries that Hal will have the same neuroses on discovering that he will be abandoned, and Chandra convinces HAL that the human crew is in danger and must leave.", 'After detaching itself from Discovery, Leonov makes a hasty exit from the Jupiter system, just in time to witness the Monoliths engulf Jupiter. Through a mechanism that the novel only partially explains, these monoliths increase Jupiter\'s density until the planet achieves nuclear fusion, becoming a small star. As Leonov leaves Jupiter, Bowman instructs HAL to repeatedly broadcast a message warning travellers not to land on Europa. The new star, which Earth eventually dubs "Lucifer", destroys Discovery. HAL is transformed into the same kind of entity as David Bowman and becomes Bowman\'s companion.']}, {'headings': ['Lists of science fiction films'], 'subheadings': ['Contents', 'Lists by decade', 'See also', 'References', 'External links', 'Navigation menu'], 'paras': ['Science fiction filmsThis is a list of science fiction films organized chronologically. These films have been released to a cinema audience by the commercial film industry and are widely distributed with reviews by reputablecritics. (The exception are the films on the made-for-TV list, which are normally not released to a cinema audience.) This includes silent film–era releases, serial films, and feature-length films. All of the films include core elements of science fiction, but can cross into other genres such as drama, mystery, action, horror, fantasy, and comedy.', 'Among the listed movies are films that have won motion-picture and science fiction awards as well as films that have been listed among the worst movies ever made, or have won one or more Golden Raspberry Awards. Critically distinguished films are indicated by footnotes in the listings.', 'Subgenre lists', 'Related films', 'Related lists', 'Film ratings']}, {'headings': ['List of science fiction films before 1920'], 'subheadings': ['See also', 'References', 'Navigation menu'], 'paras': ['', 'A list of science fiction films released before the 1920s. These films include core elements of science fiction and are widely available with reviews by reputablecritics or film historians.']}, {'headings': ['List of science fiction films of the 1920s'], 'subheadings': ['See also', 'References', 'Navigation menu'], 'paras': ['', 'A list of science fiction films released in the 1920s. These films include core elements of science fiction and are widely available with reviews by reputable critics or film historians.']}, {'headings': ['List of science fiction films of the 1930s'], 'subheadings': ['See also', 'Notes', 'Navigation menu'], 'paras': ['', 'This is a list of science fiction films that premiered between 1 January 1930 and 31 December 1939. In Phil Hardy\'s book Science Fiction (1983), the 1930s were described as a period where both science fiction literature and cinema were "in turmoil" and that by examining films of decade that "it is clear that Science Fiction, in no sense, can be seen as an ongoing genre in the thirties".[1]', "In the United States, films would use a science fiction plot device or character such as a mad scientist, but more closely resembled contemporary genres like horror, thriller and detective films.[1] The films enhanced other genres such as melodrama (Six Hours to Live), Westerns (The Phantom Empire) and most predominantly horror films such as Frankenstein or Dr. Jekyll and Mr. Hyde.[1][2] Towards the middle of the decade, science fiction was prominent in low budget Poverty Row films and film series.[1] European films such as End of the World and F.P.1 antwortet nicht and Things to Come continued the line of prophetic speculation of Fritz Lang's film Metropolis.[1] Towards the end of the 1930s as political climate was changing in Europe, films such as Bila Nemoc used science fiction elements to imagine the horrors of World War II.[1]", 'Few films from the era have been nominated or won awards, these include Fredric March winning an Academy Award for Best actor for Dr. Jekyll and Mr. Hyde while the film received nominations for best writing and cinematography.[3] Gilbert Kurland was nominated for Best Sound Recording for Bride of Frankenstein.[3]']}, {'headings': ['List of science fiction films of the 1940s'], 'subheadings': ['Contents', 'List', 'See also', 'References', 'Navigation menu'], 'paras': ['', 'A list of science fiction films released in the 1940s. These films include core elements of science fiction and are widely distributed with reviews by reputablecritics.']}, {'headings': ['List of science fiction films of the 1950s'], 'subheadings': ['Contents', 'List', 'See also', 'Notes', 'References', 'Navigation menu'], 'paras': ['', 'A list of science fiction films released in the 1950s. These films include core elements of science fiction, but can cross into other genres. They have been released to a cinema audience by the commercial film industry and are widely distributed with reviews by reputable critics.', "This period is sometimes described as the 'classic' era of science fiction theater. Much of the production was in a low-budget form, targeted at a teenage audience. Many were formulaic, gimmicky, comic-book-style films. They drew upon political themes or public concerns of the day, including depersonalization, infiltration, or fear of nuclear weapons. Invasion was a common theme, as were various threats to humanity.[1]", 'Approximately 192 science fiction films were made in the decade. Three of the films from this decade, Destination Moon (1950), The War of the Worlds (1953) and 20,000 Leagues Under the Sea (1954) won Academy Awards, while The Incredible Shrinking Man (1957) won a Hugo Award.', 'Russian title: Полёт на Луну, tr. Polyot na Lunu[2][3][4][5]']}, {'headings': ['List of science fiction films of the 1960s'], 'subheadings': ['Contents', 'List', 'See also', 'Notes', 'References', 'Navigation menu'], 'paras': ['', 'A list of science fiction films released in the 1960s. These films include core elements of science fiction, but can cross into other genres. They have been released to a cinema audience by the commercial film industry and are widely distributed with reviews by reputable critics. Collectively, the science fiction films from the 1960s received five Academy Awards, a Hugo Award and a BAFTA Award.']}, {'headings': ['List of science fiction films of the 1970s'], 'subheadings': ['Contents', 'List', 'See also', 'Notes', 'References', 'Navigation menu'], 'paras': ['', 'A list of science fiction films released in the 1970s. These films include core elements of science fiction, but can cross into other genres. They have been released to a cinema audience by the commercial film industry and are widely distributed with reviews by reputable critics.', 'During the 1970s, blockbuster science fiction films, which reached a much larger audience than previously, began to make their appearance. The financial success of these films resulted in heavy investment in special effects by the American film industry, leading to big-budget, heavily marketed science fiction film releases during the 1990s.[1] Collectively, the science fiction films from the 1970s received 11 Academy Awards, 10 Saturn Awards, six Hugo Awards, three Nebula Awards and two Grammy Awards. Two of these films, Star Wars (1977, currently known as Star Wars Episode IV: A New Hope) and Superman (1978), were the highest-grossing films of their respective years of release.']}, {'headings': ['List of science fiction films of the 1980s'], 'subheadings': ['Contents', 'List', 'See also', 'Notes', 'References', 'Navigation menu'], 'paras': ['', 'A list of science fiction films released in the 1980s. These films include core elements of science fiction, but can cross into other genres. They have been released to a cinema audience by the commercial film industry and are widely distributed with reviews by reputable critics.', 'Collectively, the science fiction films from the 1980s have received 14 Academy Awards, 11 Saturn Awards, six Hugo Awards, five BAFTA awards, four BSFA Awards, and one Golden Globe Award. Four of these movies were the highest-grossing films of their respective years of release. However, these films also received nine Golden Raspberry Awards.']}, {'headings': ['List of science fiction films of the 1990s'], 'subheadings': ['Contents', 'List', 'See also', 'Notes', 'References', 'Navigation menu'], 'paras': ['', 'A list of science fiction films released in the 1990s. These films include core elements of science fiction, but can cross into other genres. They have been released to a cinema audience by the commercial film industry and are widely distributed with reviews by reputable critics.', 'Collectively, the science fiction films from the 1990s have received 13 Academy Awards, 15 Saturn Awards, five Hugo Awards, one Nebula Award and one Golden Globe Award. Four of these movies were the highest-grossing films of their respective years of release. However, these films also received 10 Golden Raspberry Awards.', '1991', '1992', '1993', '1994', '1995', '[54]', '[56]', '1996', '1997', '1998', '1999']}, {'headings': ['List of science fiction films of the 2000s'], 'subheadings': ['Contents', 'List', 'See also', 'Notes', 'References', 'External links', 'Navigation menu'], 'paras': ['', 'This is a list of science fiction films released in the 2000s. These films include core elements of science fiction, but can cross into other genres. They have been released to a cinema audience by the commercial film industry and are widely distributed with reviews by reputable critics. Collectively, the science fiction films from the 2000s have received six Academy Awards, twenty Saturn Awards, two Hugo Awards, one Nebula Award, five BAFTA awards, and six Magritte Awards. However, these films also received 17 Golden Raspberry Awards.', 'United States']}, {'headings': ['List of science fiction films of the 2010s'], 'subheadings': ['Contents', 'Listing', 'See also', 'Notes', 'References', 'Navigation menu'], 'paras': ['', 'This is a list of science fiction films release in the 2010s. These films include core elements of science fiction, but can cross into other genres. They have been released to a cinema audience by the commercial film industry and are widely distributed with reviews by reputable critics.', 'Canada', 'Canada', 'Colombia', 'United StatesCanada', 'Canada']}, {'headings': ['List of science fiction films of the 2020s'], 'subheadings': ['Contents', 'Listing', 'Forthcoming', 'See also', 'Notes', 'References', 'Navigation menu'], 'paras': ['', 'This is a list of science fiction films released in the 2020s. These films include core elements of science fiction, but can cross into other genres. They have been released to a cinema audience by the commercial film industry and are widely distributed with reviews by reputable critics.']}, {'headings': ['List of science fiction television films'], 'subheadings': ['Contents', '1950s–1960s', '1970s', '1980s', '1990s', '2000s', '2010s', 'See also', 'References', 'Navigation menu'], 'paras': ['', 'This is a list of science fiction television films that did not have a theatrical release, including direct-to-video releases.']}, {'headings': ['Film genre'], 'subheadings': ['Contents', 'Overview', 'Examples of genres and subgenres', 'History', 'Pure and hybrid genres', 'Audience expectations', 'Categorization', 'Film in the context of history', 'See also', 'References', 'Further reading', 'External links', 'Navigation menu'], 'paras': ['A film genre is a stylistic or thematic category for motion pictures based on similarities either in the narrative elements, aesthetic approach, or the emotional response to the film.[2]', 'Drawing heavily from the theories of literary-genre criticism, film genres are usually delineated by "conventions, iconography, settings, narratives, characters and actors."[3] One can also classify films by the tone, theme/topic, mood, format, target audience, or budget.[4] These characteristics are most evident in genre films, which are "commercial feature films [that], through repetition and variation, tell familiar stories with familiar characters and familiar situations" in a given genre.[5]', "A film's genre will influence the use of filmmaking styles and techniques, such as the use of flashbacks and low-key lighting in film noir; tight framing in horror films; or fonts that look like rough-hewn logs for the titles of Western films.[6] In addition, genres have associated film-scoring conventions, such as lush string orchestras for romantic melodramas or electronic music for science-fiction films.[6] Genre also affects how films are broadcast on television, advertised, and organized in video-rental stores.[5]", 'Alan Williams distinguishes three main genre categories: narrative, avant-garde, and documentary.[7]', 'With the proliferation of particular genres, film subgenres can also emerge: the legal drama, for example, is a sub-genre of drama that includes courtroom- and trial-focused films. Subgenres are often a mixture of two separate genres; genres can also merge with seemingly unrelated ones to form hybrid genres, where popular combinations include the romantic comedy and the action comedy film. Broader examples include the docufiction and docudrama, which merge the basic categories of fiction and non-fiction (documentary).[8]', 'Genres are not fixed; they change and evolve over time, and some genres may largely disappear (for example, the melodrama).[4] Not only does genre refer to a type of film or its category, a key role is also played by the expectations of an audience about a film, as well as institutional discourses that create generic structures.[4]', 'Characteristics of particular genres are most evident in genre films, which are "commercial feature films [that], through repetition and variation, tell familiar stories with familiar characters and familiar situations" in a given genre.[5]', 'Drawing heavily from the theories of literary-genre criticism, film genres are usually delineated by conventions, iconography, narratives, formats, characters, and actors, all of which can vary according to the genre.[3] In terms of standard or "stock" characters, those in film noir, for example, include the femme fatale[9] and the "hardboiled" detective; while those in Westerns, stock characters include the schoolmarm and the gunslinger. Regarding actors, some may acquire a reputation linked to a single genre, such as John Wayne (the Western) or Fred Astaire (the musical).[10] Some genres have been characterized or known to use particular formats, which refers to the way in which films are shot (e.g., 35\xa0mm, 16 mm or 8\xa0mm) or the manner of presentation (e.g., anamorphic widescreen).[4]', 'Genres can also be classified by more inherent characteristics (usually implied in their names), such as settings, theme/topic, mood, target audience, or budget/type of production.[4]', "Screenwriters, in particular, often organize their stories by genre, focusing their attention on three specific aspects: atmosphere, character, and story.[11] A film's atmosphere includes costumes, props, locations, and the visceral experiences created for the audience.[12] Aspects of character include archetypes, stock characters, and the goals and motivations of the central characters.[13] Some story considerations for screenwriters, as they relate to genre, include theme, tent-pole scenes, and how the rhythm of characters' perspective shift from scene to scene.[14]", 'From the earliest days of cinema in the 19th century the term "genre" (already in use in English with reference to works of art or literary production from at least 1770[20])was used[by whom?] to organize films according to type.[21]By the 1950s André Bazin was discussing the concept of "genre" by using the Western film as an example; during this era, there was a debate over auteur theory versus genre.[4] In the late 1960s the concept of genre became a significant part of film theory.[4]', 'Film genres draw on genres from other forms; Western novels existed before the Western film, and musical theatre pre-dated film musicals.[22] The perceived genre of a film can change over time; for example, in the 21st century The Great Train Robbery (1903) classes as a key early Western film, but when released, marketing promoted it "for its relation to the then-popular genres of the chase film, the railroad film and the crime film".[23]A key reason that the early Hollywood industrial system from the 1920s to the 1950s favoured genre films is that in "Hollywood\'s industrial mode of production, genre movies are dependable products" to market to audiences - they were easy to produce and it was easy for audiences to understand a genre film.[24] In the 1920s to 1950s, genre films had clear conventions and iconography, such as the heavy coats worn by gangsters in films like Little Caesar (1931).[25]The conventions in genre films enable filmmakers to generate them in an industrial, assembly-line fashion, an approach which can be seen in the James Bond spy-films, which all use a formula of "lots of action, fancy gadgets, beautiful woman and colourful villains", even though the actors, directors and screenwriters change.[25]', 'Films are rarely purely from one genre, which is in keeping with the cinema\'s diverse and derivative origins, it being a blend of "vaudeville, music-hall, theatre, photography" and novels.[4] American film historian Janet Staiger states that the genre of a film can be defined in four ways. The "idealist method" judges films by predetermined standards. The "empirical method" identifies the genre of a film by comparing it to a list of films already deemed to fall within a certain genre. The apriori method uses common generic elements which are identified in advance. The "social conventions" method of identifying the genre of a film is based on the accepted cultural consensus within society.[26] Martin Loop contends that Hollywood films are not pure genres because most Hollywood movies blend the love-oriented plot of the romance genre with other genres.[26] Jim Colins claims that since the 1980s, Hollywood films have been influenced by the trend towards "ironic hybridization", in which directors combine elements from different genres, as with the Western/science fiction mix in Back to the Future Part III.[26]', 'Many films cross into multiple genres. Susan Hayward states that spy films often cross genre boundaries with thriller films.[4] Some genre films take genre elements from one genre and place them into the conventions of a second genre, such as with The Band Wagon (1953), which adds film noir and detective film elements into "The Girl Hunt" ballet.[25] In the 1970s New Hollywood era, there was so much parodying of genres that it can be hard to assign genres to some films from this era, such as Mel Brooks\' comedy-Western Blazing Saddles (1974) or the private eye parody The Long Goodbye (1973).[4] Other films from this era bend genres so much that it is challenging to put them in a genre category, such as Roman Polanski\'s Chinatown (1974) and William Friedkin\'s The French Connection (1971).[4]', 'Film theorist Robert Stam challenged whether genres really exist, or whether they are merely made up by critics. Stam has questioned whether "genres [are] really \'out there\' in the world or are they really the construction of analysts?". As well, he has asked whether there is a "... finite taxonomy of genres or are they in principle infinite?" and whether genres are "...timeless essences ephemeral, time-bound entities? Are genres culture-bound or trans-cultural?" Stam has also asked whether genre analysis should aim at being descriptive or prescriptive. While some genres are based on story content (the war film), other are borrowed from literature (comedy, melodrama) or from other media (the musical). Some are performer-based (Fred Astaire and Ginger Rogers films) or budget-based (blockbusters, low budget film), while others are based on artistic status (the art film), racial identity (Race films), location (the Western), or sexual orientation (Queer cinema).[27]', 'Many genres have built-in audiences and corresponding publications that support them, such as magazines and websites. For example, horror films have a well-established fanbase that reads horror magazines such as Fangoria. Films that are difficult to categorize into a genre are often less successful. As such, film genres are also useful in the areas of marketing, film criticism and the analysis of consumption. Hollywood story consultant John Truby states that "...you have to know how to transcend the forms [genres] so you can give the audience a sense of originality and surprise."[28]', 'Some screenwriters use genre as a means of determining what kind of plot or content to put into a screenplay. They may study films of specific genres to find examples. This is a way that some screenwriters are able to copy elements of successful movies and pass them off in a new screenplay. It is likely that such screenplays fall short in originality. As Truby says, "Writers know enough to write a genre script but they haven\'t twisted the story beats of that genre in such a way that it gives an original face to it".[29]', 'Cinema technologies are associated with genres. Huge widescreens helped Western films to create an expansive setting of the open plains and desert. Science fiction and fantasy films are associated with special effects, notably computer generated imagery (e.g., the Harry Potter films).[4]', 'In 2017, screenwriter Eric R. Williams published a system for screenwriters to conceptualize narrative film genres based on audience expectations.[30] The system was based upon the structure biologists use to analyze living beings. Williams wrote a companion book detailing his taxonomy, which claims to be able to identify all feature length narrative films with seven categorizations: film type, super genre, macro-genre, micro-genre, voice, and pathway.[31]', 'Because genres are easier to recognize than to define, academics agree they cannot be identified in a rigid way.[32] Furthermore, different countries and cultures define genres in different ways. A typical example are war movies. In US, they are mostly related to ones with large U.S involvement such as World wars and Vietnam, whereas in other countries, movies related to wars in other historical periods are considered war movies.', 'Film genres may appear to be readily categorizable from the setting of the film. Nevertheless, films with the same settings can be very different, due to the use of different themes or moods. For example, while both The Battle of Midway and All Quiet on the Western Front are set in a wartime context and might be classified as belonging to the war film genre, the first examines the themes of honor, sacrifice, and valour, and the second is an anti-war film which emphasizes the pain and horror of war. While there is an argument that film noir movies could be deemed to be set in an urban setting, in cheap hotels and underworld bars, many classic noirs take place mainly in small towns, suburbia, rural areas, or on the open road.[33]', 'The editors of filmsite.org argue that animation, pornographic film, documentary film, silent film and so on are non-genre-based film categories.[34]', 'Linda Williams argues that horror, melodrama, and pornography all fall into the category of "body genres" since they are each designed to elicit physical reactions on the part of viewers. Horror is designed to elicit spine-chilling, white-knuckled, eye-bulging terror; melodramas are designed to make viewers cry after seeing the misfortunes of the onscreen characters; and pornography is designed to elicit sexual arousal.[35] This approach can be extended: comedies make people laugh, tear-jerkers make people cry, feel-good films lift people\'s spirits and inspiration films provide hope for viewers.', 'Eric R. Williams (no relation to Linda Williams) argues that all narrative feature length films can be categorized as one of eleven "super genres" (Action, Crime, Fantasy, Horror, Romance, Science Fiction, Slice of Life, Sports, Thriller, War and Western).[11] Williams contends that labels such as comedy or drama are more broad than the category of super genre, and therefore fall into a category he calls "film type".[30] Similarly, Williams explains that labels such as animation and musical are more specific to storytelling technique and therefore fall into his category of "voice".[36] For example, according to Williams, a film like Blazing Saddles could be categorized as a Comedy (type) Western (super-genre) Musical (voice) while Anomalisa is a Drama (type) Slice of Life (super-genre) Animation (voice).  Williams has created a seven-tiered categorization for narrative feature films called the Screenwriters Taxonomy.[31]', 'A genre movie is a film that follows some or all of the conventions of a particular genre, whether or not it was intentional when the movie was produced.[37]', 'In order to understand the creation and context of each film genre, we must look at its popularity in the context of its place in history. For example, the 1970s Blaxploitation films have been called an attempt to "undermine the rise of Afro-American\'s Black consciousness movement" of that era.[4]  In William Park\'s analysis of film noir, he states that we must view and interpret film for its message with the context of history within our minds; he states that this is how film can truly be understood by its audience.[38] Film genres such as film noir and Western film reflect values of the time period. While film noir combines German expressionist filming strategies with post World War II ideals; Western films focused on the ideal of the early 20th century. Films such as the musical were created as a form of entertainment during the Great Depression allowing its viewers an escape during tough times. So when watching and analyzing film genres we must remember to remember its true intentions aside from its entertainment value.', "Over time, a genre can change through stages: the classic genre era; the parody of the classics; the period where filmmakers deny that their films are part of a certain genre; and finally a critique of the entire genre.[4] This pattern can be seen with the Western film. In the earliest, classic Westerns, there was a clear hero who protected society from lawless villains who lived in the wilderness and came into civilization to commit crimes.[4] However, in revisionist Westerns of the 1970s, the protagonist becomes an anti-hero who lives in the wilderness to get away from a civilization that is depicted as corrupt, with the villains now integrated into society. Another example of a genre changing over time is the popularity of the neo-noir films in the early 2000s (Mulholland Drive (2001), The Man Who Wasn't There (2001) and Far From Heaven (2002); are these film noir parodies, a repetition of noir genre tropes, or a re-examination of the noir genre?[4]", "This is also important to remember when looking at films in the future. As viewers watch a film they are conscious of societal influence with the film itself. In order to understand it's true intentions, we must identify its intended audience and what narrative of our current society, as well as it comments to the past in relation with today's society. This enables viewers to understand the evolution of film genres as time and history morphs or views and ideals of the entertainment industry."]}, {'headings': ['Speculative fiction'], 'subheadings': ['Contents', 'History', 'Distinguishing science fiction from other speculative fiction', 'Genres', 'See also', 'References', 'External links', 'Navigation menu'], 'paras': ['', 'Speculative fiction is a term that has been used with a variety of (sometimes contradictory) meanings.[1]  The broadest interpretation is as a category of fiction encompassing genres with elements that do not exist in reality, recorded history, nature, or the present universe. Such fiction covers various themes in the context of supernatural, futuristic, and other imaginative realms.[2] The genres under this umbrella category include, but are not limited to, science fiction, fantasy, horror, superhero fiction, alternate history, utopian and dystopian fiction, and supernatural fiction, as well as combinations thereof (for example, science fantasy).[3]', "Speculative fiction as a category ranges from ancient works to paradigm-changing and neotraditional works of the 21st century.[4][5] Characteristics of speculative fiction have been recognized in older works whose authors' intentions, or in the social contexts of the stories they portray, are now known. For example, the ancient Greek dramatist, Euripides, (c. 480–406 BCE) whose play Medea seems to have offended Athenian audiences when he speculated that the titular shamaness Medea killed her own children, as opposed to their being killed by other Corinthians after her departure.[6] Additionally, Euripides' play, Hippolytus, narratively introduced by Aphrodite, Goddess of Love in person, is suspected to have displeased his contemporary audiences, as his portrayal of Phaedra was seen as too lusty.[7]", 'In historiography, what is now called "speculative fiction" has previously been termed "historical invention",[8] "historical fiction", and other similar names. These terms have been extensively noted in literary criticism of the works of William Shakespeare,[9] such as when he co-locates Athenian Duke Theseus, Amazonian Queen Hippolyta, English fairy Puck, and Roman god Cupid across time and space in the Fairyland of the fictional Merovingian Germanic sovereign Oberon, in A Midsummer Night\'s Dream.[10]', 'In mythography the concept of speculative fiction has been termed "mythopoesis", or mythopoeia. This practice involves the creative design and generation of lore and mythology for works of fiction. The term\'s definition comes from its use by J. R. R. Tolkien, whose novel, The Lord of the Rings,[11] demonstrates a clear application of this process. Themes common in mythopoeia, such as the supernatural, alternate history and sexuality, continue to be explored in works produced within the modern speculative fiction genre.[12]', 'The creation of speculative fiction in its general sense of hypothetical history, explanation, or ahistorical storytelling, has also been attributed to authors in ostensibly non-fiction modes since as early as Herodotus of Halicarnassus (fl. 5th century BCE), for his Histories,[13][14][15] and was already both practiced and edited out by early encyclopedic writers like Sima Qian (c. 145 or 135 BCE–86 BCE), author of Shiji.[16][17]', 'These examples highlight the caveat that many works, now regarded as intentional or unintentional speculative fiction, long predated the coining of the genre term; its concept, in its broadest sense, captures both a conscious and unconscious aspect of human psychology in making sense of the world, and responds to it by creating imaginative, inventive, and artistic expressions. Such expressions can contribute to practical societal progress through interpersonal influences, social and cultural movements, scientific research and advances, and the philosophy of science.[18][19][20]', 'In its English-language usage in arts and literature since the mid 20th century, "speculative fiction" as a genre term has often been attributed to Robert A. Heinlein, who first used the term in an editorial in The Saturday Evening Post, 8\xa0February 1947. In the article, Heinlein used "Speculative Fiction" as a synonym for "science fiction"; in a later piece, he explicitly stated that his use of the term did not include fantasy. However, though Heinlein may have come up with the term on his own, there are earlier citations: a piece in Lippincott\'s Monthly Magazine in 1889 used the term in reference to Edward Bellamy\'s Looking Backward: 2000–1887 and other works; and one in the May 1900 issue of The Bookman said that John Uri Lloyd\'s Etidorhpa, The End of the Earth had "created a great deal of discussion among people interested in speculative fiction".[21] A variation on this term is "speculative literature".[22]', 'The use of "speculative fiction" in the sense of expressing dissatisfaction with traditional or establishment science fiction was popularized in the 1960s and early 1970s by Judith Merril, as well as other writers and editors in connection with the New Wave movement. However, this use of the term fell into disuse around the mid-1970s.[23]', 'In the 2000s, the term came into wider use as a convenient collective term for a set of genres. However, some writers, such as Margaret Atwood, continue to distinguish "speculative fiction" specifically as a "no Martians" type of science fiction, "about things that really could happen."[24]', 'The Internet Speculative Fiction Database contains a broad list of different subtypes.', 'According to publisher statistics, men outnumber women about two to one among English-language speculative fiction writers aiming for professional publication. However, the percentages vary considerably by genre, with women outnumbering men in the fields of urban fantasy, paranormal romance and young adult fiction.[25]', 'Academic journals which publish essays on speculative fiction include Extrapolation, and Foundation.[26]', '"Speculative fiction" is sometimes abbreviated "spec-fic", "spec fic", "specfic",[27] "S-F", "SF" or "sf".[28] The last three abbreviations, however, are ambiguous as they have long been used to refer to science fiction (which lies within this general range of literature).[29]', 'The term has been used by some critics and writers dissatisfied with what they consider to be a limitation of science fiction: the need for the story to hold to scientific principles. They argue that "speculative fiction" better defines an expanded, open, imaginative type of fiction than does "genre fiction", and the categories of "fantasy", "mystery", "horror" and "science fiction".[30] Harlan Ellison used the term to avoid being pigeonholed as a writer. Ellison, a fervent proponent of writers embracing more literary and modernist directions,[31][32] broke out of genre conventions to push the boundaries of speculative fiction. ', 'The term "suppositional fiction" is sometimes used as a sub-category designating fiction in which characters and stories are constrained by an internally consistent world, but not necessarily one defined by any particular genre.[33][34][35]', 'Speculative fiction may include elements of one or more of the following genres:']}, {'headings': ['Science'], 'subheadings': ['Contents', 'Etymology', 'History', 'Branches', 'Scientific research', 'Philosophy of science', 'Scientific community', 'Society', 'See also', 'Notes', 'References', 'Bibliography', 'Navigation menu'], 'paras': ['', 'Science is a systematic endeavor that builds and organizes knowledge in the form of testable explanations and predictions about the universe.[1][2]', 'Science may be as old as the human species, and some of the earliest archeological evidence for scientific reasoning is tens of thousands of years old. The earliest written records in the history of science come from Ancient Egypt and Mesopotamia in around 3000 to 1200 BCE. Their contributions to mathematics, astronomy, and medicine entered and shaped Greek natural philosophy of classical antiquity, whereby formal attempts were made to provide explanations of events in the physical world based on natural causes.[3][4] After the fall of the Western Roman Empire, knowledge of Greek conceptions of the world deteriorated in Western Europe during the early centuries (400 to 1000 CE) of the Middle Ages, but was preserved in the Muslim world during the Islamic Golden Age[5] and later by the efforts of Byzantine Greek scholars who brought Greek manuscripts from the dying Byzantine Empire to Western Europe in the Renaissance.', 'The recovery and assimilation of Greek works and Islamic inquiries into Western Europe from the 10th to 13th century revived "natural philosophy",[6][7] which was later transformed by the Scientific Revolution that began in the 16th century[8] as new ideas and discoveries departed from previous Greek conceptions and traditions.[9][10] The scientific method soon played a greater role in knowledge creation and it was not until the 19th century that many of the institutional and professional features of science began to take shape;[11][12] along with the changing of "natural philosophy" to "natural science".[13]', 'Modern science is typically divided into three major branches:[14] natural sciences (e.g., biology, chemistry, and physics), which study the physical world; the social sciences (e.g., economics, psychology, and sociology), which study individuals and societies;[15][16] and the formal sciences (e.g., logic, mathematics, and theoretical computer science), which study formal systems, governed by axioms and rules.[17][18] There is disagreement whether the formal sciences are science disciplines,[19][20][21] because they do not rely on empirical evidence.[22][20] Applied sciences are disciplines that use scientific knowledge for practical purposes, such as in engineering and medicine.[23][24][25]', 'New knowledge in science is advanced by research from scientists who are motivated by curiosity about the world and a desire to solve problems.[26][27] Contemporary scientific research is highly collaborative and is usually done by teams in academic and research institutions,[28] government agencies, and companies.[29][30] The practical impact of their work has led to the emergence of science policies that seek to influence the scientific enterprise by prioritizing the ethical and moral development of commercial products, armaments, health care, public infrastructure, and environmental protection.', 'The word science has been used in Middle English since the 14th century in the sense of "the state of knowing". The word was borrowed from the Anglo-Norman language as the suffix -cience, which was borrowed from the Latin word scientia, meaning "knowledge, awareness, understanding". It is a noun derivative of the Latin sciens meaning "knowing", and undisputedly derived from the Latin sciō, the present participle scīre, meaning "to know".[31]', 'There are many hypotheses for science\'s ultimate word origin. According to Michiel de Vaan, Dutch linguist and Indo-Europeanist, sciō may have its origin in the Proto-Italic language as *skije- or *skijo- meaning "to know", which may originate from Proto-Indo-European language as *skh1-ie, *skh1-io, meaning "to incise". The Lexikon der indogermanischen Verben proposed sciō is a back-formation of nescīre, meaning "to not know, be unfamiliar with", which may derive from Proto-Indo-European *sekH- in Latin secāre, or *skh2-, from *sḱʰeh2(i)- meaning "to cut".[32]', 'In the past, science was a synonym for "knowledge" or "study", in keeping with its Latin origin. A person who conducted scientific research was called a "natural philosopher" or "man of science".[33] In 1833, William Whewell coined the term scientist and the term first appeared in literature one year later in Mary Somerville\'s On the Connexion of the Physical Sciences, published in the Quarterly Review.[34]', 'Science may be decribed to be as old as the human species,[35] Science has no single origin. Rather, scientific methods emerged gradually over the course of thousands of years, taking different forms around the world, and few details are known about the very earliest developments. Some of the earliest evidence for scientific reasoning is tens of thousands of years old,[36] and women likely played a central role in prehistoric science,[37] as did religious rituals.[38] Some Western authors have dismissed these efforts as "protoscientific".[39]', 'Direct evidence for scientific processes becomes clearer with the advent of writing systems in early civilizations like Ancient Egypt and Mesopotamia, creating the earliest written records in the history of science in around 3000 to 1200 BCE.[40][4] Although the words and concepts of "science" and "nature" were not part of the conceptual landscape at the time, the ancient Egyptians and Mesopotamians made contributions that would later find a place in Greek and medieval science: mathematics, astronomy, and medicine.[41][3] From the 3rd millennium BCE, the ancient Egyptians developed a decimal numbering system,[42] solved practical problems using geometry,[43] and developed a calendar.[44] Their healing therapies involved drug treatments and the supernatural, such as prayers, incantations, and rituals.[45]', 'The ancient Mesopotamians used knowledge about the properties of various natural chemicals for manufacturing pottery, faience, glass, soap, metals, lime plaster, and waterproofing.[46] They studied animal physiology, anatomy, behavior, and astrology for divinatory purposes.[47] The Mesopotamians had an intense interest in medicine[46] and the earliest medical prescriptions appeared in Sumerian during the Third Dynasty of Ur.[48] They seem to study scientific subjects which have practical or religious applications and have little interest of satisfying curiosity.[46]', 'In classical antiquity, there is no real ancient analog of a modern scientist. Instead, well-educated, usually upper-class, and almost universally male individuals performed various investigations into nature whenever they could afford the time.[49] Before the invention or discovery of the concept of phusis or nature by the pre-Socratic philosophers, the same words tend to be used to describe the natural "way" in which a plant grows,[50] and the "way" in which, for example, one tribe worships a particular god. For this reason, it is claimed that these men were the first philosophers in the strict sense and the first to clearly distinguish "nature" and "convention".[51]', 'The early Greek philosophers of the Milesian school, which was founded by Thales of Miletus and later continued by his successors Anaximander and Anaximenes, were the first to attempt to explain natural phenomena without relying on the supernatural.[52] The Pythagoreans developed a complex number philosophy[53]:\u200a467–68\u200a and contributed significantly to the development of mathematical science.[53]:\u200a465\u200a The theory of atoms was developed by the Greek philosopher Leucippus and his student Democritus.[54][55] The Greek doctor Hippocrates established the tradition of systematic medical science[56][57] and is known as "The Father of Medicine".[58]', "A turning point in the history of early philosophical science was Socrates' example of applying philosophy to the study of human matters, including human nature, the nature of political communities, and human knowledge itself. The Socratic method as documented by Plato's dialogues is a dialectic method of hypothesis elimination: better hypotheses are found by steadily identifying and eliminating those that lead to contradictions. The Socratic method searches for general commonly-held truths that shape beliefs and scrutinizes them for consistency.[59] Socrates criticized the older type of study of physics as too purely speculative and lacking in self-criticism.[60]", "Aristotle in the 4th century BCE created a systematic program of teleological philosophy.[61] In the 3rd century BCE, Greek astronomer Aristarchus of Samos was the first to propose a heliocentric model of the universe, with the Sun at the center and all the planets orbiting it.[62] Aristarchus's model was widely rejected because it was believed to violate the laws of physics,[62] while Ptolemy's Almagest, which contains a geocentric description of the Solar System, was accepted through the early Renaissance instead.[63][64] The inventor and mathematician Archimedes of Syracuse made major contributions to the beginnings of calculus.[65] Pliny the Elder was a Roman writer and polymath, who wrote the seminal encyclopedia Natural History.[66][67][68]", "Due to the collapse of the Western Roman Empire, the 5th century saw an intellectual decline and knowledge of Greek conceptions of the world deteriorated in  Western Europe.[69] During the period, Latin encyclopedists such as Isidore of Seville preserved the majority of general ancient knowledge.[70] In contrast, because the Byzantine Empire resisted attacks from invaders, they were able to preserve and improve prior learning.[71] John Philoponus, a Byzantine scholar in the 500s, started to question Aristotle's teaching of physics, introducing the theory of impetus.[72] His criticism served as an inspiration to medieval scholars and Galileo Galilei, who extensively cited his works ten centuries later.[73][74]", "During late antiquity and the early Middle Ages, natural phenomena were mainly examined via the Aristotelian approach. The approach includes Aristotle's four causes: material, formal, moving, and final cause.[75] Many Greek classical texts were preserved by the Byzantine empire and Arabic translations were done by groups such as the Nestorians and the Monophysites. Under the Caliphate, these Arabic translations were later improved and developed by Arabic scientists.[76] By the 6th and 7th centuries, the neighboring Sassanid Empire established the medical Academy of Gondeshapur, which is considered by Greek, Syriac, and Persian physicians as the most important medical center of the ancient world.[77]", "The House of Wisdom was established in Abbasid-era Baghdad, Iraq,[78] where the Islamic study of Aristotelianism flourished[79] until the Mongol invasions in the 13th century. Ibn al-Haytham, better known as Alhazen, began experimenting as a means to gain knowledge[80][81] and disproved Ptolemy's theory of vision[82]:\u200aBook I,\u200a[6.54]. p. 372\u200a Avicenna's compilation of the Canon of Medicine, a medical encyclopedia, is considered to be one of the most important publications in medicine and was used until the 18th century.[83]", 'By the eleventh century, most of Europe had become Christian,[84] and in 1088, the University of Bologna emerged as the first university in Europe.[85] As such, demand for Latin translation of ancient and scientific texts grew,[84] a major contributor to the Renaissance of the 12th century. Renaissance scholasticism in western Europe flourished, with experiments done by observing, describing, and classifying subjects in nature.[86] In the 13rd century, medical teachers and students at Bologna began opening human bodies, leading to the ﬁrst anatomy textbook based on human dissection by Mondino de Luzzi.[87]', "New developments in optics played a role in the inception of the Renaissance, both by challenging long-held metaphysical ideas on perception, as well as by contributing to the improvement and development of technology such as the camera obscura and the telescope. At the start of the Renaissance, Roger Bacon, Vitello, and John Peckham each built up a scholastic ontology upon a causal chain beginning with sensation, perception, and finally apperception of the individual and universal forms of Aristotle.[82]:\u200aBook I\u200a A model of vision later known as perspectivism was exploited and studied by the artists of the Renaissance. This theory uses only three of Aristotle's four causes: formal, material, and final.[88]", "In the sixteenth century, Nicolaus Copernicus formulated a heliocentric model of the Solar System, stating that the planets revolve around the Sun, instead of the geocentric model where the planets and the Sun revolve around the Earth. This was based on a theorem that the orbital periods of the planets are longer as their orbs are farther from the center of motion, which he found not to agree with Ptolemy's model.[89]", "Johannes Kepler and others challenged the notion that the only function of the eye is perception, and shifted the main focus in optics from the eye to the propagation of light.[88][90] Kepler is best known, however, for improving Copernicus' heliocentric model through the discovery of Kepler's laws of planetary motion. Kepler did not reject Aristotelian metaphysics and described his work as a search for the Harmony of the Spheres.[91] Galileo had made significant contributions to astronomy, physics and engineering. However, he became persecuted after Pope Urban VIII sentenced him for writing about the heliocentric model.[92]", 'The printing press was widely used to publish scholarly arguments, including some that disagreed widely with contemporary ideas of nature.[93] Francis Bacon and René Descartes published philosophical arguments in favor of a new type of non-Aristotelian science. Bacon emphasized the importance of experiment over contemplation, questioned the Aristotelian concepts of formal and final cause, promoted the idea that science should study the laws of nature and the improvement of all human life.[94] Descartes emphasized individual thought and argued that mathematics rather than geometry should be used to study nature.[95]', 'At the start of the Age of Enlightenment, Isaac Newton formed the foundation of classical mechanics by his Philosophiæ Naturalis Principia Mathematica, greatly influencing future physicists.[96] Gottfried Wilhelm Leibniz incorporated terms from Aristotelian physics, now used in a new non-teleological way. This implied a shift in the view of objects: objects were now considered as having no innate goals. Leibniz assumed that different types of things all work according to the same general laws of nature, with no special formal or final causes.[97]', 'During this time, the declared purpose and value of science became producing wealth and inventions that would improve human lives, in the materialistic sense of having more food, clothing, and other things. In Bacon\'s words, "the real and legitimate goal of sciences is the endowment of human life with new inventions and riches", and he discouraged scientists from pursuing intangible philosophical or spiritual ideas, which he believed contributed little to human happiness beyond "the fume of subtle, sublime or pleasing [speculation]".[98]', 'Science during the Enlightenment was dominated by scientific societies[99] and academies, which had largely replaced universities as centers of scientific research and development. Societies and academies were the backbones of the maturation of the scientific profession. Another important development was the popularization of science among an increasingly literate population.[100] Enlightenment philosophers chose a short history of scientific predecessors\xa0– Galileo, Boyle, and Newton principally\xa0– as the guides to every physical and social field of the day.[101]', 'The 18th century saw significant advancements in the practice of medicine[102] and physics;[103] the development of biological taxonomy by Carl Linnaeus;[104] a new understanding of magnetism and electricity;[105] and the maturation of chemistry as a discipline.[106] Ideas on human nature, society, and economics evolved during the Enlightenment. Hume and other Scottish Enlightenment thinkers developed A Treatise of Human Nature, which was expressed historically in works by authors including James Burnett, Adam Ferguson, John Millar and William Robertson, all of whom merged a scientific study of how humans behaved in ancient and primitive cultures with a strong awareness of the determining forces of modernity.[107] Modern sociology largely originated from this movement.[108] In 1776, Adam Smith published The Wealth of Nations, which is often considered the first work on modern economics.[109]', 'During the nineteenth century, many distinguishing characteristics of contemporary modern science began to take shape. These included the transformation of the life and physical sciences, frequent use of precision instruments, emergence of terms such as "biologist", "physicist", "scientist", increased professionalization of those studying nature, scientists gained cultural authority over many dimensions of society, industrialization of numerous countries, thriving of popular science writings and emergence of science journals.[110] During the late 19th century, psychology emerged as a separate discipline from philosophy when Wilhelm Wundt founded the first laboratory for psychological research in 1879.[111]', 'During the mid-19th century, Charles Darwin and Alfred Russel Wallace independently proposed the theory of evolution by natural selection in 1858, which explained how different plants and animals originated and evolved. Their theory was set out in detail in Darwin\'s book On the Origin of Species, published in 1859.[112] Separately, Gregor Mendel presented his paper, "Experiments on Plant Hybridization" in 1865,[113] which outlined the principles of biological inheritance, serving as the basis for modern genetics.[114]', "Early in the 19th century, John Dalton suggested the modern atomic theory, based on Democritus's original idea of indivisible particles called atoms.[115] The laws of conservation of energy, conservation of momentum and conservation of mass suggested a highly stable universe where there could be little loss of resources. However, with the advent of the steam engine and the industrial revolution there was an increased understanding that not all forms of energy have the same energy qualities, the ease of conversion to useful work or to another form of energy.[116] This realization led to the development of the laws of thermodynamics, in which the free energy of the universe is seen as constantly declining: the entropy of a closed universe increases over time.[a]", "The electromagnetic theory was established in the 19th century by the works of Hans Christian Ørsted, André-Marie Ampère, Michael Faraday, James Clerk Maxwell, Oliver Heaviside, and Heinrich Hertz. The new theory raised questions that could not easily be answered using Newton's framework. The discovery of X-rays inspired the discovery of radioactivity by Henri Becquerel and Marie Curie in 1896,[119] Marie Curie then became the first person to win two Nobel prizes.[120] In the next year came the discovery of the first subatomic particle, the electron.[121]", "In the first half of the century, the development of antibiotics and artificial fertilizers improved human living standards globally.[122][123] Harmful environmental issues such as ozone depletion, ocean acidification, eutrophication and climate change came to the public's attention and caused the onset of environmental studies.[124]", 'During this period, scientific experimentation became increasingly larger in scale and funding.[125] The extensive technological innovation stimulated by World War I, World War II, and the Cold War led to competitions between global powers, such as the Space Race[126] and nuclear arms race.[127] Substantial international collaborations were also made, despite armed conflicts.[128]', 'In the late 20th century, active recruitment of women and elimination of sex discrimination greatly increased the number of women scientists, but large gender disparities remained in some fields.[129] The discovery of the cosmic microwave background in 1964[130] led to a rejection of the steady-state model of the universe in favor of the Big Bang theory of Georges Lemaître.[131]', "The century saw fundamental changes within science disciplines. Evolution became a unified theory in the early 20th-century when the modern synthesis reconciled Darwinian evolution with classical genetics.[132] Albert Einstein's theory of relativity and the development of quantum mechanics complement classical mechanics to describe physics in extreme length, time and gravity.[133][134] Widespread use of integrated circuits in the last quarter of the 20th century combined with communications satellites led to a revolution in information technology and the rise of the global internet and mobile computing, including smartphones. The need for mass systematization of long, intertwined causal chains and large amounts of data led to the rise of the fields of systems theory and computer-assisted scientific modeling.[135]", "The Human Genome Project was completed in 2003 by identifying and mapping all of the genes of the human genome.[136] The first induced pluripotent human stem cells were made in 2006, allowing adult cells to be transformed into stem cells and turn to any cell type found in the body.[137] With the affirmation of the Higgs boson discovery in 2013, the last particle predicted by the Standard Model of particle physics was found.[138] In 2015, gravitational waves, predicted by general relativity a century before, were first observed.[139][140] In 2019, the international collaboration Event Horizon Telescope presented the first direct image of a black hole's accretion disk.[141]", 'Modern science is commonly divided into three major branches: natural science, social science, and formal science.[14] Each of these branches comprises various specialized yet overlapping scientific disciplines that often possess their own nomenclature and expertise.[142] Both natural and social sciences are empirical sciences,[143] as their knowledge is based on empirical observations and is capable of being tested for its validity by other researchers working under the same conditions.[144]', 'Natural science is the study of the physical world. It can be divided into two main branches: life science and physical science. These two branches may be further divided into more specialized disciplines. For example, physical science can be subdivided into physics, chemistry, astronomy, and earth science. Modern natural science is the successor to the natural philosophy that began in Ancient Greece. Galileo, Descartes, Bacon, and Newton debated the benefits of using approaches which were more mathematical and more experimental in a methodical way. Still, philosophical perspectives, conjectures, and presuppositions, often overlooked, remain necessary in natural science.[145] Systematic data collection, including discovery science, succeeded natural history, which emerged in the 16th century by describing and classifying plants, animals, minerals, and so on.[146] Today, "natural history" suggests observational descriptions aimed at popular audiences.[147]', 'Social science is the study of human behavior and functioning of societies.[15][16] It has many disciplines that include, but are not limited to anthropology, economics, history, human geography, political science, psychology, and sociology.[15] In the social sciences, there are many competing theoretical perspectives, many of which are extended through competing research programs such as the functionalists, conflict theorists, and interactionists in sociology.[15] Due to the limitations of conducting controlled experiments involving large groups of individuals or complex situations, social scientists may adopt other research methods such as the historical method, case studies, and cross-cultural studies. Moreover, if quantitative information is available, social scientists may rely on statistical approaches to better understand social relationships and processes.[15]', 'Formal science is an area of study that generates knowledge using formal systems.[148][17][18] A formal system is an abstract structure used for inferring theorems from axioms according to a set of rules.[149] It includes mathematics,[150][151] systems theory, and theoretical computer science. The formal sciences share similarities with the other two branches by relying on objective, careful, and systematic study of an area of knowledge. They are, however, different from the empirical sciences as they rely exclusively on deductive reasoning, without the need for empirical evidence, to verify their abstract concepts.[22][152][144] The formal sciences are therefore a priori disciplines and because of this, there is disagreement on whether they constitute a science.[19][153] Nevertheless, the formal sciences play an important role in the empirical sciences. Calculus, for example, was initially invented to understand motion in physics.[154] Natural and social sciences that rely heavily on mathematical applications include mathematical physics,[155] chemistry,[156] biology,[157] finance,[158] and economics.[159]', 'Applied science is the use of the scientific method and knowledge to attain practical goals and includes a broad range of disciplines such as engineering and medicine.[160][25] Engineering is the use of scientific principles to invent, design and build machines, structures and technologies.[161] Science may contribute to the development of new technologies.[162] Medicine is the practice of caring for patients by maintaining and restoring health through the prevention, diagnosis, and treatment of injury or disease.[163][164] The applied sciences are often contrasted with the basic sciences, which are focused on advancing scientific theories and laws that explain and predict events in the natural world.[165][166]', 'Computational science applies computing power to simulate real-world situations, enabling a better understanding of scientific problems than formal mathematics alone can achieve. The use of machine learning and artificial intelligence is becoming a central feature of computational contributions to science for example in agent-based computational economics, random forests, topic modeling and various forms of prediction. However, machines alone rarely advance knowledge as they require human guidance and capacity to reason; and they can introduce bias against certain social groups or sometimes underperform against humans.[167][168]', 'Interdisciplinary science involves the combination of two or more disciplines into one,[169] such as bioinformatics, a combination of biology and computer science.[170] The concept has existed since the ancient Greek and it became popular again in the 20th century.[171]', 'Scientific research can be labeled as either basic or applied research. Basic research is the search for knowledge and applied research is the search for solutions to practical problems using this knowledge. Most understanding comes from basic research, though sometimes applied research targets specific practical problems. This leads to technological advances that were not previously imaginable.[172]', 'Scientific research involves using the scientific method, which seeks to objectively explain the events of nature in a reproducible way.[173] Scientists usually take for granted a set of basic assumptions that are needed to justify the scientific method: there is an objective reality shared by all rational observers; this objective reality is governed by natural laws; these laws were discovered by means of systematic observation and experimentation.[174] Mathematics is essential in the formation of hypotheses, theories, and laws, because it is used extensively in quantitative modeling, observing, and collecting measurements.[175] Statistics is used to summarize and analyze data, which allows scientists to assess the reliability of experimental results.[176]', 'In the scientific method, an explanatory thought experiment or hypothesis is put forward as an explanation using parsimony principles and is expected to seek consilience\xa0– fitting with other accepted facts related to an observation or scientific question.[177] This tentative explanation is used to make falsifiable predictions, which are typically posted before being tested by experimentation. Disproof of a prediction is evidence of progress.[173]:\u200a4–5\u200a[178] Experimentation is especially important in science to help establish causal relationships to avoid the correlation fallacy, though in some sciences such as astronomy or geology, a predicted observation might be more appropriate.[179]', 'When a hypothesis proves unsatisfactory, it is modified or discarded.[180] If the hypothesis survived testing, it may become adopted into the framework of a scientific theory, a logically reasoned, self-consistent model or framework for describing the behavior of certain natural events. A theory typically describes the behavior of much broader sets of observations than a hypothesis; commonly, a large number of hypotheses can be logically bound together by a single theory. Thus a theory is a hypothesis explaining various other hypotheses. In that vein, theories are formulated according to most of the same scientific principles as hypotheses. Scientists may generate a model, an attempt to describe or depict an observation in terms of a logical, physical or mathematical representation and to generate new hypotheses that can be tested by experimentation.[181]', 'While performing experiments to test hypotheses, scientists may have a preference for one outcome over another.[182][183] Eliminating the bias can be achieved by transparency, careful experimental design, and a thorough peer review process of the experimental results and conclusions.[184][185] After the results of an experiment are announced or published, it is normal practice for independent researchers to double-check how the research was performed, and to follow up by performing similar experiments to determine how dependable the results might be.[186] Taken in its entirety, the scientific method allows for highly creative problem solving while minimizing the effects of subjective and confirmation bias.[187] Intersubjective verifiability, the ability to reach a consensus and reproduce results, is fundamental to the creation of all scientific knowledge.[188]', 'Scientific research is published in a range of literature.[189] Scientific journals communicate and document the results of research carried out in universities and various other research institutions, serving as an archival record of science. The first scientific journal, Journal des sçavans by Philosophical Transactions, began publication in 1665. Since that time the total number of active periodicals has steadily increased. In 1981, one estimate for the number of scientific and technical journals in publication was 11,500.[190]', 'Most scientific journals cover a single scientific field and publish the research within that field; the research is normally expressed in the form of a scientific paper. Science has become so pervasive in modern societies that it is considered necessary to communicate the achievements, news, and ambitions of scientists to a wider population.[191]', 'The replication crisis is an ongoing methodological crisis that affects parts of the social and life sciences. In subsequent investigations, the results of many scientific studies are proven to be unrepeatable.[192] The crisis has long-standing roots; the phrase was coined in the early 2010s[193] as part of a growing awareness of the problem. The replication crisis represents an important body of research in metascience, which aims to improve the quality of all scientific research while reducing waste.[194]', 'An area of study or speculation that masquerades as science in an attempt to claim a legitimacy that it would not otherwise be able to achieve is sometimes referred to as pseudoscience, fringe science, or junk science.[195][196] Physicist Richard Feynman coined the term "cargo cult science" for cases in which researchers believe and at a glance looks like they are doing science, but lack the honesty allowing their results to be rigorously evaluated.[197] Various types of commercial advertising, ranging from hype to fraud, may fall into these categories. Science has been described as "the most important tool" for separating valid claims from invalid ones.[198]', 'There can also be an element of political or ideological bias on all sides of scientific debates. Sometimes, research may be characterized as "bad science," research that may be well-intended but is incorrect, obsolete, incomplete, or over-simplified expositions of scientific ideas. The term "scientific misconduct" refers to situations such as where researchers have intentionally misrepresented their published data or have purposely given credit for a discovery to the wrong person.[199]', '', 'There are different schools of thought in the philosophy of science. The most popular position is empiricism, which holds that knowledge is created by a process involving observation; scientific theories generalize observations.[200] Empiricism generally encompasses inductivism, a position that explains how general theories can be made from the finite amount of empirical evidence available. Many versions of empiricism exist, with the predominant ones being Bayesianism[201] and the hypothetico-deductive method.[200]', 'Empiricism has stood in contrast to rationalism, the position originally associated with Descartes, which holds that knowledge is created by the human intellect, not by observation.[202] Critical rationalism is a contrasting 20th-century approach to science, first defined by Austrian-British philosopher Karl Popper. Popper rejected the way that empiricism describes the connection between theory and observation. He claimed that theories are not generated by observation, but that observation is made in the light of theories:  that the only way theory A can be affected by observation is after theory A were to conflict with observation, but theory B were to survive the observation.[203]Popper proposed replacing verifiability with falsifiability as the landmark of scientific theories,  replacing induction with falsification as the empirical method.[203] Popper further claimed that there is actually only one universal method, not specific to science: the negative method of criticism, trial and error,[204] covering all products of the human mind, including science, mathematics, philosophy, and art.[205]', 'Another approach, instrumentalism, emphasizes the utility of theories as instruments for explaining and predicting phenomena. It views scientific theories as black boxes with only their input (initial conditions) and output (predictions) being relevant. Consequences, theoretical entities, and logical structure are claimed to be something that should be ignored.[206] Close to instrumentalism is constructive empiricism, according to which the main criterion for the success of a scientific theory is whether what it says about observable entities is true.[207]', 'Thomas Kuhn argued that the process of observation and evaluation takes place within a paradigm, a logically consistent "portrait" of the world that is consistent with observations made from its framing. He characterized normal science as the process of observation and "puzzle solving" which takes place within a paradigm, whereas revolutionary science occurs when one paradigm overtakes another in a paradigm shift.[208] Each paradigm has its own distinct questions, aims, and interpretations. The choice between paradigms involves setting two or more "portraits" against the world and deciding which likeness is most promising. A paradigm shift occurs when a significant number of observational anomalies arise in the old paradigm and a new paradigm makes sense of them. That is, the choice of a new paradigm is based on observations, even though those observations are made against the background of the old paradigm. For Kuhn, acceptance or rejection of a paradigm is a social process as much as a logical process. Kuhn\'s position, however, is not one of relativism.[209]', 'Finally, another approach often cited in debates of scientific skepticism against controversial movements like "creation science" is methodological naturalism. Naturalists maintain that a difference should be made between natural and supernatural, and science should be restricted to natural explanations.[210] Methodological naturalism maintains that science requires strict adherence to empirical study and independent verification.[211]', 'The scientific community is a network of interacting scientists who conducts scientific research. The community consists of smaller groups working in scientific fields. By having peer review, through discussion and debate within journals and conferences, scientists maintain the quality of research methodology and objectivity when interpreting results.[212]', 'Scientists are individuals who conduct scientific research to advance knowledge in an area of interest.[213][214] In modern times, many professional scientists are trained in an academic setting and upon completion, attain an academic degree, with the highest degree being a doctorate such as a Doctor of Philosophy or PhD.[215] Many scientists pursue careers in various sectors of the economy such as academia, industry, government, and nonprofit organizations.[216][217][218]', 'Scientists exhibit a strong curiosity about reality and a desire to apply scientific knowledge for the benefit of health, nations, the environment, or industries. Other motivations include recognition by their peers and prestige. In modern times, many scientists have advanced degrees[219] in an area of science and pursue careers in various sectors of the economy such as academia, industry, government, and nonprofit environments.[220][221]', "Science has historically been a male-dominated field, with notable exceptions. Women in science faced considerable discrimination in science, much as they did in other areas of male-dominated societies. For example, women were frequently being passed over for job opportunities and denied credit for their work.[222] The achievements of women in science have been attributed to the defiance of their traditional role as laborers within the domestic sphere.[223] Lifestyle choice plays a major role in female engagement in science; female graduate students' interest in careers in research declines dramatically throughout graduate school, whereas that of their male colleagues remains unchanged.[224]", 'Learned societies for the communication and promotion of scientific thought and experimentation have existed since the Renaissance.[225] Many scientists belong to a learned society that promotes their respective scientific discipline, profession, or group of related disciplines.[226] Membership may either be open to all, require possession of scientific credentials, or conferred by election.[227] Most scientific societies are non-profit organizations, and many are professional associations. Their activities typically include holding regular conferences for the presentation and discussion of new research results and publishing or sponsoring academic journals in their discipline. Some societies act as professional bodies, regulating the activities of their members in the public interest or the collective interest of the membership.', 'The professionalization of science, begun in the 19th century, was partly enabled by the creation of national distinguished academies of sciences such as the Italian Accademia dei Lincei in 1603,[228] the British Royal Society in 1660,[229] the French Academy of Sciences in 1666,[230] the American National Academy of Sciences in 1863,[231] the German Kaiser Wilhelm Society in 1911,[232] and the Chinese Academy of Sciences in 1949.[233] International scientific organizations, such as the International Science Council, are devoted to international cooperation for science advancement.[234]', 'Science awards are usually given to individuals or organizations that have made significant contributions to a discipline. They are often given by prestigious institutions, thus it is considered a great honor for a scientist receiving them. Since the early Renaissance, scientists are often awarded medals, money, and titles. The Nobel Prize, a widely regarded prestigious award, is awarded annually to those who have achieved scientific advances in the fields of medicine, physics, and chemistry.[235]', 'Scientific research is often funded through a competitive process in which potential research projects are evaluated and only the most promising receive funding. Such processes, which are run by government, corporations, or foundations, allocate scarce funds. Total research funding in most developed countries is between 1.5% and 3% of GDP.[236] In the OECD, around two-thirds of research and development in scientific and technical fields is carried out by industry, and 20% and 10% respectively by universities and government. The government funding proportion in certain fields is higher, and it dominates research in social science and humanities. In the lesser-developed nations, government provides the bulk of the funds for their basic scientific research.[237]', 'Many governments have dedicated agencies to support scientific research, such as the National Science Foundation in the United States,[238] the National Scientific and Technical Research Council in Argentina,[239] Commonwealth Scientific and Industrial Research Organization in Australia,[240] National Centre for Scientific Research in France,[241] the Max Planck Society in Germany,[242] and National Research Council in Spain.[243] In commercial research and development, all but the most research-oriented corporations focus more heavily on near-term commercialization possibilities rather than research driven by curiosity.[244]', "Science policy is concerned with policies that affect the conduct of the scientific enterprise, including research funding, often in pursuance of other national policy goals such as technological innovation to promote commercial product development, weapons development, health care, and environmental monitoring. Science policy sometimes refers to the act of applying scientific knowledge and consensus to the development of public policies. In accordance with public policy being concerned about the well-being of its citizens, science policy's goal is to consider how science and technology can best serve the public.[245] Public policy can directly affect the funding of capital equipment and intellectual infrastructure for industrial research by providing tax incentives to those organizations that fund research.[191]", 'Science education for the general public is embedded in the school curriculum, and is supplemented by online pedagogical content (for example, YouTube and Khan Academy), museums, and science magazines and blogs. Scientific literacy is chiefly concerned with an understanding of the scientific method, units and methods of measurement, empiricism, a basic understanding of statistics (correlations, qualitative versus quantitative observations, aggregate statistics), as well as a basic understanding of core scientific fields, such as physics, chemistry, biology, ecology, geology and computation. As a student advances into higher stages of formal education, the curriculum becomes more in depth. Traditional subjects usually included in the curriculum are natural and formal sciences, although recent movements include social and applied science as well.[246]', 'The mass media face pressures that can prevent them from accurately depicting competing scientific claims in terms of their credibility within the scientific community as a whole. Determining how much weight to give different sides in a scientific debate may require considerable expertise regarding the matter.[247] Few journalists have real scientific knowledge, and even beat reporters who are knowledgeable about certain scientific issues may be ignorant about other scientific issues that they are suddenly asked to cover.[248][249]', 'Science magazines such as New Scientist, Science & Vie, and Scientific American cater to the needs of a much wider readership and provide a non-technical summary of popular areas of research, including notable discoveries and advances in certain fields of research.[250] Science fiction genre, primarily speculative fiction, can transmit the ideas and methods of science to the general public.[251] Recent efforts to intensify or develop links between science and non-scientific disciplines, such as literature or poetry, include the Creative Writing Science resource developed through the Royal Literary Fund.[252]', 'While the scientific method is broadly accepted in the scientific community, some fractions of society reject certain scientific positions or are skeptical about science. Examples are the common notion that COVID-19 is not a major health threat to the US (held by 39% of Americans in August 2021)[253] or the belief that climate change is not a major threat to the US (also held by 40% of Americans, in late 2019 and early 2020).[254] Psychologists have pointed to four factors driving rejection of scientific results:[255]', 'Anti-science attitudes seem to be often caused by fear of rejection in social groups. For instance, climate change is perceived as a threat by only 22% of Americans on the right side of the political spectrum, but by 85% on the left.[257] That is, if someone on the left would not consider climate change as a threat, this person may face contempt and be rejected in that social group. In fact, people may rather deny a scientifically accepted fact than lose or jeopardize their social status.[258]', 'Attitudes towards science are often determined by political opinions and goals. Government, business and advocacy groups have been known to use legal and economic pressure to influence scientific researchers. Many factors can act as facets of the politicization of science such as anti-intellectualism, perceived threats to religious beliefs, and fear for business interests.[260] Politicization of science is usually accomplished when scientific information is presented in a way that emphasizes the uncertainty associated with the scientific evidence.[261] Tactics such as shifting conversation, failing to acknowledge facts, and capitalizing on doubt of scientific consensus have been used to gain more attention for views that have been undermined by scientific evidence.[262] Examples of issues that have involved the politicization of science include the global warming controversy, health effects of pesticides, and health effects of tobacco.[262][263]']}, {'headings': ['Extraterrestrials in fiction'], 'subheadings': ['Contents', 'History', 'Modern', 'See also', 'References', 'Further reading', 'External links', 'Navigation menu'], 'paras': ['An extraterrestrial or alien is any extraterrestrial lifeform; a lifeform that did not originate on Earth. The word extraterrestrial means "outside Earth". The first published use of extraterrestrial as a noun occurred in 1956, during the Golden Age of Science Fiction.[1]', 'Extraterrestrials are a common theme in modern science-fiction, and also appeared in much earlier works such as the second-century parody True History by  Lucian of Samosata.', 'Gary Westfahl writes:', 'Science fiction aliens are both metaphors and real possibilities. One can probe the nature of humanity with aliens that by contrast illustrate and comment upon human nature. Still, as evidenced by widespread belief in alien visitors (see UFOs) and efforts to detect extraterrestrial radio signals, humans also crave companionship in a vast, cold universe and aliens may represent hopeful, compensatory images of the strange friends we have been unable to find. Thus, aliens will likely remain a central theme in science fiction until we actually encounter them.[2]', 'Cosmic pluralism, the assumption that there are many inhabited worlds beyond the human sphere predates modernity and the development of the heliocentric model and is common in mythologies worldwide. The 2nd century writer of satires, Lucian, in his True History claims to have visited the moon when his ship was sent up by a fountain, which was peopled and at war with the people of the Sun over colonisation of the Morning Star.[3] Other worlds are depicted in such early works as the 10th-century Japanese narrative, The Tale of the Bamboo Cutter, and the medieval Arabic The Adventures of Bulukiya (from the One Thousand and One Nights).[4]', 'The assumption of extraterrestrial life in the narrow sense (as opposed to generic cosmic pluralism) becomes possible with the development of the heliocentric understanding of the Solar System, and later the understanding of interstellar space, during the Early Modern period, and the topic was popular in the literature of the 17th and 18th centuries.', "In Johannes Kepler's Somnium, published in 1634, the character Duracotus is transported to the moon by demons. Even if much of the story is fantasy, the scientific facts about the moon and how the lunar environment has shaped its non-human inhabitants are science fiction.", 'The didactic poet Henry More took up the classical theme of Cosmic pluralism of the Greek Democritus in "Democritus Platonissans, or an Essay Upon the Infinity of Worlds" (1647).[5] With the new relative viewpoint that understood "our world\'s sunne / Becomes a starre elsewhere", More made the speculative leap to extrasolar planets,', 'The possibility of extraterrestrial life was a commonplace of educated discourse in the 17th century, though in Paradise Lost (1667)[6] John Milton cautiously employed the conditional when the angel suggests to Adam the possibility of life on the Moon:', 'Fontanelle\'s "Conversations on the Plurality of Worlds" with its similar excursions on the possibility of extraterrestrial life, expanding rather than denying the creative sphere of a Maker, was translated into English in 1686.[7] In "The Excursion" (1728) David Mallet exclaimed, "Ten thousand worlds blaze forth; each with his train / Of peopled worlds."[8] In 1752, Voltaire published the novella Micromégas, telling the story of a giant that visits earth to impart knowledge. Washington Irving in his novel, A History of New York from the Beginning of the World to the End of the Dutch Dynasty, spoke of earth being visited by Lunarians.[9]', "Camille Flammarion (1842-1925) who lived in a time where biological science had made further progress, made speculation about how life could have evolved on other planets in works such as La pluralité des mondes habités (The Plurality of Inhabited Worlds) (1862) and Recits de L'Infini (1872), translated as Stories of Infinity in 1873. Stories written before the genre of science fiction had found its form.", 'Closer to the modern age is J.-H. Rosny, who wrote the short story Les Xipéhuz (1887), about a human encounter with extraterrestrials who turn out to be a mineral life form impossible to communicate with.', 'Authors such as H. G. Wells, Olaf Stapledon and Edgar Rice Burroughs wrote both monitory and celebratory stories of encounting aliens in their science fiction and fantasies. Westfahl sums up: "To survey science fiction aliens, one can classify them by their physiology, character, and eventual relationships with humanity":', 'Early works posited that aliens would be identical or similar to humans, as is true of Edgar Rice Burroughs\'s Martians (see Mars; A Princess of Mars), with variations in skin color, size, and number of arms. ... Later writers realized that such humanoid aliens would not arise through parallel evolution and hence either avoided them or introduced the explanation of ancient races that populated the cosmos with similar beings. The notion surfaces in Ursula K. Le Guin\'s Hainish novels (see The Left Hand of Darkness; The Dispossessed) and was introduced to justify the humanoid aliens of Star Trek (who even intermarry and have children) in the Star Trek: The Next Generation episode "The Chase" (1993).Another common idea is aliens who closely resemble animals.[2]', "Among the many fictional aliens who resemble Earth's animals, Westfahl lists:", 'Westfahl continues, "However, Stanley G. Weinbaum\'s A Martian Odyssey (1934) encouraged writers to create genuinely unusual aliens, not merely humans or animals in disguise. Olaf Stapledon also populated the universe with disparate aliens, including sentient stars, in Star Maker. Later, Hal Clement, a hard science fiction writer famed for strange but plausible worlds, also developed bizarre aliens in works like Cycle of Fire (1957)."[2]', 'Articles related to the phenomenon of extraterrestrials in fiction and popular culture:', 'Articles related to the purported or theorized existence of extraterrestrials:']}, {'headings': ['List of fictional spacecraft'], 'subheadings': ['Contents', 'Space stations', 'Shuttles', 'Lunar', 'Interplanetary', 'Interstellar', 'Intergalactic', 'Personal spacecraft', 'Lists of fictional spacecraft', 'See also', 'References', 'External links', 'Navigation menu'], 'paras': ['This is a list of fictional spacecraft, starships and exo-atmospheric vessels that have been identified by name in notable published works of fiction. The term "spacecraft" is mainly used to refer to spacecraft that are real or conceived using present technology. The terms "spaceship" and "starship" are generally applied only to fictional space vehicles, usually those capable of transporting people.', 'Spaceships are often one of the key plot devices in science fiction. Numerous short stories and novels are built up around various ideas for spacecraft, and spacecraft have featured in many films and television series. Some hard science fiction books focus on the technical details of the craft. Some fictional spaceships have been referenced in the real world, notably Starship Enterprise from Star Trek which gave its name to Space Shuttle Enterprise and to the VSS Enterprise.[1]For other ships from Star Wars, Star Trek, Robotech, and other major franchises, see the separate lists linked below.', '(Planetary surface to orbit)', '"Space fighters" are fictional spacecraft analogous to fighter aircraft. They are popular as the subjects of flight simulators, movies and books. The following are some examples of notable space fighters from various media franchises:', 'The Earth Alliance (Starfury fighters)', 'The Minbari Federation', 'The Narn Regime', 'The Centauri Republic', 'The Shadows', 'The Vorlons', '', 'The Twelve Colonies', 'The Cylons', '', 'The Earth Defense Directorate', 'The Draconian Empire', '', 'The United States Marine Corps', 'The Chigs', '', 'Terrestrial', 'Extraterrestrial', '', "Tau'ri/Earth (USAF fighters)", "The Goa'uld", 'The Wraith', 'The Ori', '', 'In the Star Wars universe, a "starfighter" is a blanket term for all small combat space craft, regardless of shields, hyperspace capability, weaponry (unless it carries none), armor, maneuverability and crew. "Snubfighter" (a term first used in Star Wars), though no concise definition has been given, often refers to a fighter carrying shielding, secondary weapons systems such as proton torpedoes or concussion missiles, and being hyperspace capable. Starfighters sometimes bear mission designations similar to modern fighter aircraft, such as "strike fighter" and "space superiority fighter".', 'Galactic Republic', 'Rebel Alliance and New Republic', 'Galactic Empire and First Order', 'Confederacy of Independent Systems', 'Andromeda', 'Halo', 'Marvel Cinematic Universe']}]}
